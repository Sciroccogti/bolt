{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_bits = 64\n",
    "ncodebooks=64\n",
    "ncentroids=256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = socket.gethostname()\n",
    "if host_name == 'DESKTOP-PLRL7TK':\n",
    "    dir_train = ''\n",
    "    dir_result = ''\n",
    "elif host_name == 'DESKTOP-6FOH47P':\n",
    "    dir_train = ''\n",
    "    dir_result = ''\n",
    "    data_to_fcpath_train= ''\n",
    "    featurepath_train= ''\n",
    "    data_to_fcpath_test = ''\n",
    "    featurepath_test = ''\n",
    "elif host_name == 'jm-System-Product-Name':\n",
    "    dir_train = '/home/hdr/transformer_data/joined'\n",
    "    dir_result = '/home/hdr/pq/res'\n",
    "    data_to_fcpath_train= 'data_to_fc_f%i_train.npy' % feedback_bits\n",
    "    y_train = 'y_f%i_train.npy' % feedback_bits\n",
    "    featurepath_train= 'feature_f%i_train.npy' % feedback_bits\n",
    "    data_to_fcpath_test = 'data_to_fc_f%i_test.npy' % feedback_bits\n",
    "    featurepath_test = 'feature_f%i_test.npy' % feedback_bits\n",
    "    y_test = 'y_f%i_test.npy' % feedback_bits\n",
    "else:\n",
    "    raise NameError(\"You are running the script in a new computer, please define dir_train\")\n",
    "\n",
    "\n",
    "weightpath = 'encoder_fcw_f%i.npy' % feedback_bits\n",
    "biaspath = 'encoder_fcb_f%i.npy' % feedback_bits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "================================\n",
      "learn_multisplits(): initial loss:    420.3125360567696\n",
      "learn_multisplits(): returning loss:  170.46519468958843\n",
      "================================\n",
      "learn_multisplits(): initial loss:    97.08919403999599\n",
      "learn_multisplits(): returning loss:  66.05978601994636\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1850.8585199694396\n",
      "learn_multisplits(): returning loss:  717.1428710779451\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2363.8082468123366\n",
      "learn_multisplits(): returning loss:  1243.6454652000634\n",
      "================================\n",
      "learn_multisplits(): initial loss:    635.323977701418\n",
      "learn_multisplits(): returning loss:  285.3327842578398\n",
      "================================\n",
      "learn_multisplits(): initial loss:    137.45779233511234\n",
      "learn_multisplits(): returning loss:  106.17936122722261\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2347.0740751496082\n",
      "learn_multisplits(): returning loss:  1050.9285663284763\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2685.082165520218\n",
      "learn_multisplits(): returning loss:  1513.0226764678955\n",
      "================================\n",
      "learn_multisplits(): initial loss:    356.51699749936967\n",
      "learn_multisplits(): returning loss:  185.34991595569142\n",
      "================================\n",
      "learn_multisplits(): initial loss:    108.0244115663122\n",
      "learn_multisplits(): returning loss:  83.86692175518677\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1334.325701008411\n",
      "learn_multisplits(): returning loss:  746.8299246079288\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1701.6509638564771\n",
      "learn_multisplits(): returning loss:  1081.4094486236572\n",
      "================================\n",
      "learn_multisplits(): initial loss:    479.3706519125743\n",
      "learn_multisplits(): returning loss:  233.97859483467892\n",
      "================================\n",
      "learn_multisplits(): initial loss:    138.20758134323455\n",
      "learn_multisplits(): returning loss:  102.22008871281113\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1634.7309041922206\n",
      "learn_multisplits(): returning loss:  849.6763324626146\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2058.512453489735\n",
      "learn_multisplits(): returning loss:  1275.2217826843262\n",
      "================================\n",
      "learn_multisplits(): initial loss:    360.4489681261623\n",
      "learn_multisplits(): returning loss:  143.84362174664864\n",
      "================================\n",
      "learn_multisplits(): initial loss:    73.99082428312977\n",
      "learn_multisplits(): returning loss:  54.78182559981529\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1772.7381544168877\n",
      "learn_multisplits(): returning loss:  686.5270133928854\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2353.3163075962757\n",
      "learn_multisplits(): returning loss:  1236.2522444565063\n",
      "================================\n",
      "learn_multisplits(): initial loss:    618.8137138575967\n",
      "learn_multisplits(): returning loss:  291.2967313068864\n",
      "================================\n",
      "learn_multisplits(): initial loss:    138.94190747877184\n",
      "learn_multisplits(): returning loss:  105.75354173932908\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2288.4870056996333\n",
      "learn_multisplits(): returning loss:  1010.7777615674494\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2683.3503701340665\n",
      "learn_multisplits(): returning loss:  1537.6288318634033\n",
      "================================\n",
      "learn_multisplits(): initial loss:    365.78497242726723\n",
      "learn_multisplits(): returning loss:  197.8629163613752\n",
      "================================\n",
      "learn_multisplits(): initial loss:    99.41493195722177\n",
      "learn_multisplits(): returning loss:  80.17851172554519\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1346.9919499181524\n",
      "learn_multisplits(): returning loss:  743.1940313686154\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1743.7206144167262\n",
      "learn_multisplits(): returning loss:  1096.6942710876465\n",
      "================================\n",
      "learn_multisplits(): initial loss:    432.414480500041\n",
      "learn_multisplits(): returning loss:  213.61273356150008\n",
      "================================\n",
      "learn_multisplits(): initial loss:    119.32580653582222\n",
      "learn_multisplits(): returning loss:  93.40431414856417\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1652.0505592622728\n",
      "learn_multisplits(): returning loss:  858.7930545806885\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2079.4617547581893\n",
      "learn_multisplits(): returning loss:  1285.5585174560547\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4475.9491684468485\n",
      "learn_multisplits(): returning loss:  1404.9123072812185\n",
      "================================\n",
      "learn_multisplits(): initial loss:    891.0198237416046\n",
      "learn_multisplits(): returning loss:  587.515409564681\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8400.899150238874\n",
      "learn_multisplits(): returning loss:  3249.2878768424357\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14499.361088616057\n",
      "learn_multisplits(): returning loss:  7029.773198557087\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5891.22846385808\n",
      "learn_multisplits(): returning loss:  2292.5783052135193\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1169.500299305924\n",
      "learn_multisplits(): returning loss:  862.6681689627875\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12338.576101655724\n",
      "learn_multisplits(): returning loss:  5086.363773314401\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16494.49225265204\n",
      "learn_multisplits(): returning loss:  8511.455982208252\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4326.069072344977\n",
      "learn_multisplits(): returning loss:  1752.488152843208\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1135.1122472085694\n",
      "learn_multisplits(): returning loss:  827.5534753799438\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8718.490614759901\n",
      "learn_multisplits(): returning loss:  4266.805263519287\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11452.648990204925\n",
      "learn_multisplits(): returning loss:  6830.743049621582\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4992.545785518938\n",
      "learn_multisplits(): returning loss:  1955.5601143987542\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1257.934449531635\n",
      "learn_multisplits(): returning loss:  926.0565837475147\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9722.503406597956\n",
      "learn_multisplits(): returning loss:  4680.622303009033\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13262.811857436083\n",
      "learn_multisplits(): returning loss:  7529.544395446777\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4219.052691500701\n",
      "learn_multisplits(): returning loss:  1363.1915981886882\n",
      "================================\n",
      "learn_multisplits(): initial loss:    730.3461553244042\n",
      "learn_multisplits(): returning loss:  512.877792942619\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8433.028822115008\n",
      "learn_multisplits(): returning loss:  3254.1089330223726\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14876.875746142345\n",
      "learn_multisplits(): returning loss:  7190.363704718381\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5939.353981244078\n",
      "learn_multisplits(): returning loss:  2310.1397579297536\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1189.038633934425\n",
      "learn_multisplits(): returning loss:  867.9254430164476\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12406.016666282481\n",
      "learn_multisplits(): returning loss:  5065.415864569967\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17122.30484755266\n",
      "learn_multisplits(): returning loss:  8868.213924407959\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4403.7828758756605\n",
      "learn_multisplits(): returning loss:  1754.8530143177873\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1076.8581081217724\n",
      "learn_multisplits(): returning loss:  804.2142643467835\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8674.758861233233\n",
      "learn_multisplits(): returning loss:  4242.533115386963\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11340.37710833685\n",
      "learn_multisplits(): returning loss:  6726.348793029785\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4783.8162884360645\n",
      "learn_multisplits(): returning loss:  1964.1569347496425\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1172.8409394886535\n",
      "learn_multisplits(): returning loss:  852.2175817557929\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9660.521030229409\n",
      "learn_multisplits(): returning loss:  4740.531580828792\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12971.31234934057\n",
      "learn_multisplits(): returning loss:  7309.547637939453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdr/pq/bolt/experiments/python/clusterize.py:1436: UserWarning: Persisting input arguments took 1.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  _learn_mithral_initialization(X, ncodebooks, pq_perm_algo='start')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_res mse / X mse:  0.48007688\n",
      "fitting dense lstsq to X_res\n",
      "X_res mse / X mse after lstsq:  0.30092287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdr/pq/bolt/experiments/python/vquantizers.py:610: UserWarning: Persisting input arguments took 1.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  self.splits_lists, self.centroids = clusterize.learn_mithral(\n"
     ]
    }
   ],
   "source": [
    "est3 = mm.estFactory(X_path=data_to_fcpath_train, W_path=weightpath, Y_path=y_train, dir= dir_train, ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(dir_train+'/'+data_to_fcpath_test)\n",
    "w_test = np.load(dir_train+'/'+weightpath)\n",
    "bias = np.load(dir_train+'/'+biaspath)\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11261847 -0.05474875  0.39857587 ... -0.07012872 -0.08791453\n",
      "  -0.42581087]\n",
      " [-0.26311538  0.82196355 -0.35289183 ...  0.30560514 -0.21315914\n",
      "  -0.17532165]\n",
      " [-0.01262615  0.19574049  0.77430975 ...  1.0570729   0.41306394\n",
      "   0.32565683]\n",
      " ...\n",
      " [-0.01262615  0.07049587 -0.22764722 ...  0.05511589  1.1645317\n",
      "  -0.17532165]\n",
      " [-0.38836     0.07049587 -0.1024026  ... -0.32061794  0.03733009\n",
      "  -0.05007704]\n",
      " [ 0.3631077   0.19574049 -0.22764722 ... -0.82159644  0.03733009\n",
      "   0.45090145]]\n",
      "(32000, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(y_out_last.shape)\n",
    "# np.save(\"LDPC_decoder_NET_testdata/\" + snr + \"nomul_matmul_yout_matmul\", y_out_matmul)\n",
    "# np.save(dir_result+'/'+method+'fc1_fb256_cb%i_ct%i.npy' % (ncodebooks, ncentroids), y_out_matmul)\n",
    "np.save(dir_result+'/'+method+'fc1_fb%i_cb%i_ct%i.npy' % (feedback_bits, ncodebooks, ncentroids), y_out_last)\n",
    "# io.savemat(dir_result+'\\\\fc1_256.mat', {\"NN_output_buffer\": y_out_last})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pqhdr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
