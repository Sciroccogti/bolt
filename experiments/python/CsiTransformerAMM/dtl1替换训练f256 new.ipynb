{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder transformer层的linear1层（dtl1）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/conda/envs/bolt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# 获取当前文件所在的文件夹路径\n",
    "if \"__file__\" in globals():\n",
    "    # 获取__file__变量的值\n",
    "    file_path = __file__\n",
    "    # 获取当前文件所在的文件夹路径\n",
    "    dir_now = os.path.dirname(file_path)\n",
    "else:\n",
    "    # 获取当前工作目录\n",
    "    dir_now = os.getcwd()\n",
    "sys.path.append(dir_now)\n",
    "sys.path.append(os.path.join(dir_now, '../'))\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # 防止jupyter爆内存\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_PLUTO\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "# for method in [METHOD_MITHRAL, METHOD_PQ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'dtl1'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"dx_linear1\"\n",
    "\n",
    "auto_train_change_nbits = False # 是否根据已运行的训练性能结果改变nbits自动训练，（train_sam_num取已训练的最大值）\n",
    "auto_train_change_upcast = False # 是否根据已运行的训练性能结果改变upcast自动训练，（train_sam_num取已训练的最大值）\n",
    "\n",
    "if auto_train_change_nbits:\n",
    "    nbits_trained = 8\n",
    "if auto_train_change_upcast:\n",
    "    upcast_trained = 16\n",
    "quantize_lut = False\n",
    "nbits_goal = 6\n",
    "upcast_goal = -1\n",
    "lut_work_const = -1\n",
    "if quantize_lut == False:\n",
    "    nbits_goal = 0\n",
    "nbits = nbits_goal # 要运行的量化比特数\n",
    "upcast_every = upcast_goal # 要运行的upcast\n",
    "\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "\n",
    "if not auto_train_change_nbits and not auto_train_change_upcast:\n",
    "    ncodebooks = 64 # max:64\n",
    "    ncentroids = 256\n",
    "    train_sam_num = 1000 # 训练集样本数\n",
    "elif auto_train_change_nbits:\n",
    "    param2change = \"nbits\"\n",
    "    param_trained = nbits_trained\n",
    "    param_goal = nbits_goal\n",
    "    cb_ct_ntr_combinations_unique = change_param_auto_run_list(linear_name, method, feedback_bits, param2change, param_trained, param_goal, \"upcast_every\", 16)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "elif auto_train_change_upcast:\n",
    "    param2change = \"upcast_every\"\n",
    "    param_trained = upcast_trained\n",
    "    param_goal = upcast_goal\n",
    "    cb_ct_ntr_combinations_unique = change_param_auto_run_list(linear_name, method, feedback_bits, param2change, param_trained, param_goal, \"nbits\", 8)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMM_train_dirs = get_AMM_train_dirs(linear_name, linear_name_full, method, feedback_bits, train_sam_num, test_sam_num)\n",
    "create_dir(AMM_train_dirs[\"dir_result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查找以此为前缀的数据集： dx_linear1in_train_f256\n",
      "没有比输入样本数更大的数据集，从样本合成新数据集\n",
      "生成的数据集前缀:  dx_linear1in train\n",
      "样本合并第一维前大小： (32, 32, 64)\n",
      "样本合并第一维后大小： (1024, 64)\n",
      "合并后数据集大小:  (3072000, 64)\n",
      "intermediate_name[-3:] == out: False\n",
      "查找以此为前缀的数据集： dx_linear1in_test_f256\n",
      "没有比输入样本数更大的数据集，从样本合成新数据集\n",
      "生成的数据集前缀:  dx_linear1in test\n",
      "样本合并第一维前大小： (32, 32, 64)\n",
      "样本合并第一维后大小： (1024, 64)\n",
      "合并后数据集大小:  (1024000, 64)\n",
      "intermediate_name[-3:] == out: False\n",
      "查找以此为前缀的数据集： dx_linear1out_train_f256\n",
      "没有比输入样本数更大的数据集，从样本合成新数据集\n",
      "生成的数据集前缀:  dx_linear1out train\n",
      "样本合并第一维前大小： (32, 32, 128)\n",
      "样本合并第一维后大小： (1024, 128)\n",
      "合并后数据集大小:  (3072000, 128)\n",
      "intermediate_name[-3:] == out: True\n",
      "查找以此为前缀的数据集： dx_linear1out_test_f256\n",
      "没有比输入样本数更大的数据集，从样本合成新数据集\n",
      "生成的数据集前缀:  dx_linear1out test\n",
      "样本合并第一维前大小： (32, 32, 128)\n",
      "样本合并第一维后大小： (1024, 128)\n",
      "合并后数据集大小:  (1024000, 128)\n",
      "intermediate_name[-3:] == out: True\n"
     ]
    }
   ],
   "source": [
    "dataset_prepare(AMM_train_dirs[\"dir_joined\"], linear_name_full, feedback_bits, [train_sam_num, test_sam_num], \n",
    "                batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (3072000, 64)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16495.793483881633\n",
      "learn_multisplits(): returning loss:  1.2165498540271074\n",
      "================================\n",
      "learn_multisplits(): initial loss:    40391.02118037222\n",
      "learn_multisplits(): returning loss:  1.9325443105772138\n",
      "================================\n",
      "learn_multisplits(): initial loss:    43029.03895728454\n",
      "learn_multisplits(): returning loss:  2.087842100299895\n",
      "================================\n",
      "learn_multisplits(): initial loss:    345555.4423322543\n",
      "learn_multisplits(): returning loss:  16.394129342287535\n",
      "================================\n",
      "learn_multisplits(): initial loss:    80581.71919248573\n",
      "learn_multisplits(): returning loss:  5.541669798083603\n",
      "================================\n",
      "learn_multisplits(): initial loss:    63568.134343922626\n",
      "learn_multisplits(): returning loss:  2.82762308139354\n",
      "================================\n",
      "learn_multisplits(): initial loss:    55166.96078488032\n",
      "learn_multisplits(): returning loss:  3.2949237569700927\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18710.057500212188\n",
      "learn_multisplits(): returning loss:  1.3335342779755592\n",
      "================================\n",
      "learn_multisplits(): initial loss:    65583.23562304178\n",
      "learn_multisplits(): returning loss:  70.89488120292901\n",
      "================================\n",
      "learn_multisplits(): initial loss:    29212.864765052615\n",
      "learn_multisplits(): returning loss:  2.6344521942082793\n",
      "================================\n",
      "learn_multisplits(): initial loss:    29194.64615906566\n",
      "learn_multisplits(): returning loss:  2.0949451569467783\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23488.707263288416\n",
      "learn_multisplits(): returning loss:  2.400278473121044\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23164.04699469495\n",
      "learn_multisplits(): returning loss:  1.8814175406005234\n",
      "================================\n",
      "learn_multisplits(): initial loss:    38743.185520690604\n",
      "learn_multisplits(): returning loss:  5.452860596999482\n",
      "================================\n",
      "learn_multisplits(): initial loss:    73149.83403013408\n",
      "learn_multisplits(): returning loss:  6.307203819975257\n",
      "================================\n",
      "learn_multisplits(): initial loss:    84267.72411803404\n",
      "learn_multisplits(): returning loss:  64.00226515668191\n",
      "================================\n",
      "learn_multisplits(): initial loss:    36728.1596635561\n",
      "learn_multisplits(): returning loss:  3.9846465660658152\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19560.450798864662\n",
      "learn_multisplits(): returning loss:  1.3220855973565713\n",
      "================================\n",
      "learn_multisplits(): initial loss:    46274.36371221343\n",
      "learn_multisplits(): returning loss:  2.6947791648562998\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8235.156510986086\n",
      "learn_multisplits(): returning loss:  0.43734421773115173\n",
      "================================\n",
      "learn_multisplits(): initial loss:    20639.67863998826\n",
      "learn_multisplits(): returning loss:  1.0073105103801936\n",
      "================================\n",
      "learn_multisplits(): initial loss:    58146.94924416153\n",
      "learn_multisplits(): returning loss:  13.102706256984353\n",
      "================================\n",
      "learn_multisplits(): initial loss:    24888.762889580546\n",
      "learn_multisplits(): returning loss:  2.0167439725030922\n",
      "================================\n",
      "learn_multisplits(): initial loss:    76478.61131833112\n",
      "learn_multisplits(): returning loss:  3.218494208995253\n",
      "================================\n",
      "learn_multisplits(): initial loss:    94878.02937009688\n",
      "learn_multisplits(): returning loss:  56.737535646496056\n",
      "================================\n",
      "learn_multisplits(): initial loss:    20005.066641797046\n",
      "learn_multisplits(): returning loss:  1.1170565314823762\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28835.350527050974\n",
      "learn_multisplits(): returning loss:  1.5823762211948633\n",
      "================================\n",
      "learn_multisplits(): initial loss:    88492.78651609292\n",
      "learn_multisplits(): returning loss:  10.536909984836168\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17906.706318027413\n",
      "learn_multisplits(): returning loss:  1.0503073688596487\n",
      "================================\n",
      "learn_multisplits(): initial loss:    25788.6387069597\n",
      "learn_multisplits(): returning loss:  2.545614364406655\n",
      "================================\n",
      "learn_multisplits(): initial loss:    44465.576989822315\n",
      "learn_multisplits(): returning loss:  3.826996656797326\n",
      "================================\n",
      "learn_multisplits(): initial loss:    63843.45416539478\n",
      "learn_multisplits(): returning loss:  25.66090296726398\n",
      "================================\n",
      "learn_multisplits(): initial loss:    312681.7535064618\n",
      "learn_multisplits(): returning loss:  15.890736455097795\n",
      "================================\n",
      "learn_multisplits(): initial loss:    956417.834255188\n",
      "learn_multisplits(): returning loss:  1681.7688735096629\n",
      "================================\n",
      "learn_multisplits(): initial loss:    158376.71992950438\n",
      "learn_multisplits(): returning loss:  61.88545910609689\n",
      "================================\n",
      "learn_multisplits(): initial loss:    88667.8886672182\n",
      "learn_multisplits(): returning loss:  10.438113776281517\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7402.314199844282\n",
      "learn_multisplits(): returning loss:  0.8048450733176589\n",
      "================================\n",
      "learn_multisplits(): initial loss:    20570.973520374217\n",
      "learn_multisplits(): returning loss:  1.9726475015051563\n",
      "================================\n",
      "learn_multisplits(): initial loss:    35883.61740592448\n",
      "learn_multisplits(): returning loss:  2.867227613925934\n",
      "================================\n",
      "learn_multisplits(): initial loss:    32016.414691429913\n",
      "learn_multisplits(): returning loss:  3.022134749335237\n",
      "================================\n",
      "learn_multisplits(): initial loss:    31840.294859872738\n",
      "learn_multisplits(): returning loss:  5.393566496555588\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9650.392942733744\n",
      "learn_multisplits(): returning loss:  0.8913427946972661\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11474.136543031036\n",
      "learn_multisplits(): returning loss:  4.097401421975649\n",
      "================================\n",
      "learn_multisplits(): initial loss:    22442.73971641014\n",
      "learn_multisplits(): returning loss:  2.4817723505415397\n",
      "================================\n",
      "learn_multisplits(): initial loss:    46168.582990215655\n",
      "learn_multisplits(): returning loss:  3.8183717082666173\n",
      "================================\n",
      "learn_multisplits(): initial loss:    108050.40558598447\n",
      "learn_multisplits(): returning loss:  20.417549321287886\n",
      "================================\n",
      "learn_multisplits(): initial loss:    198177.89239517084\n",
      "learn_multisplits(): returning loss:  70.27601592650751\n",
      "================================\n",
      "learn_multisplits(): initial loss:    65547.50250242106\n",
      "learn_multisplits(): returning loss:  22.291731784373656\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28872.042595150942\n",
      "learn_multisplits(): returning loss:  2.8814695595314976\n",
      "================================\n",
      "learn_multisplits(): initial loss:    25236.40334947705\n",
      "learn_multisplits(): returning loss:  5.417056654454454\n",
      "================================\n",
      "learn_multisplits(): initial loss:    30634.08248141408\n",
      "learn_multisplits(): returning loss:  2.7296109807211906\n",
      "================================\n",
      "learn_multisplits(): initial loss:    34605.03458640663\n",
      "learn_multisplits(): returning loss:  5.8579561294390246\n",
      "================================\n",
      "learn_multisplits(): initial loss:    34396.151735234256\n",
      "learn_multisplits(): returning loss:  11.667375904925132\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19449.071070045156\n",
      "learn_multisplits(): returning loss:  2.3578114706029467\n",
      "================================\n",
      "learn_multisplits(): initial loss:    90889.27213811113\n",
      "learn_multisplits(): returning loss:  23.970592723670553\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17531.85879062263\n",
      "learn_multisplits(): returning loss:  1.0133599542314187\n",
      "================================\n",
      "learn_multisplits(): initial loss:    37057.45797966795\n",
      "learn_multisplits(): returning loss:  2.1828840197995305\n",
      "================================\n",
      "learn_multisplits(): initial loss:    134913.67417284648\n",
      "learn_multisplits(): returning loss:  5.788766198325902\n",
      "================================\n",
      "learn_multisplits(): initial loss:    26606.973330057783\n",
      "learn_multisplits(): returning loss:  2.824713838201618\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12861.418901623068\n",
      "learn_multisplits(): returning loss:  0.6136915478855371\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19385.156496284406\n",
      "learn_multisplits(): returning loss:  3.323147671928305\n",
      "================================\n",
      "learn_multisplits(): initial loss:    39552.11033881855\n",
      "learn_multisplits(): returning loss:  7.691503920553159\n",
      "================================\n",
      "learn_multisplits(): initial loss:    30510.30851271532\n",
      "learn_multisplits(): returning loss:  1.5659774497617036\n",
      "================================\n",
      "learn_multisplits(): initial loss:    75088.12114472191\n",
      "learn_multisplits(): returning loss:  4.141693210229278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:2007: UserWarning: Persisting input arguments took 1.38s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  _learn_mithral_initialization(X, ncodebooks, ncentroids=ncentroids, pq_perm_algo='start', **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_res mse / X mse:  0.00034963572\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(3072000, 64) Y:(3072000, 64)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 375. GiB for an array with shape (3072000, 16384) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m     est3 \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39mestFactory(X_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mlinearin_path_train\u001b[39m\u001b[39m\"\u001b[39m], W_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mweightpath\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[1;32m      3\u001b[0m                         Y_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39my_train\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39m AMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mdir_train\u001b[39m\u001b[39m\"\u001b[39m], ncodebooks\u001b[39m=\u001b[39mncodebooks, \n\u001b[1;32m      4\u001b[0m                         ncentroids\u001b[39m=\u001b[39mncentroids, methods\u001b[39m=\u001b[39m[method], nbits\u001b[39m=\u001b[39mnbits, quantize_lut \u001b[39m=\u001b[39m quantize_lut, \n\u001b[1;32m      5\u001b[0m                         upcast_every\u001b[39m=\u001b[39mupcast_every, bias_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mbiaspath\u001b[39m\u001b[39m\"\u001b[39m],lut_work_const\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m METHOD_MITHRAL:\n\u001b[0;32m----> 7\u001b[0m     est3 \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39;49mestFactory(X_path\u001b[39m=\u001b[39;49mAMM_train_dirs[\u001b[39m\"\u001b[39;49m\u001b[39mlinearin_path_train\u001b[39;49m\u001b[39m\"\u001b[39;49m], W_path\u001b[39m=\u001b[39;49mAMM_train_dirs[\u001b[39m\"\u001b[39;49m\u001b[39mweightpath\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[1;32m      8\u001b[0m                         Y_path\u001b[39m=\u001b[39;49mAMM_train_dirs[\u001b[39m\"\u001b[39;49m\u001b[39my_train\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49m AMM_train_dirs[\u001b[39m\"\u001b[39;49m\u001b[39mdir_train\u001b[39;49m\u001b[39m\"\u001b[39;49m], ncodebooks\u001b[39m=\u001b[39;49mncodebooks, \n\u001b[1;32m      9\u001b[0m                         ncentroids\u001b[39m=\u001b[39;49mncentroids, methods\u001b[39m=\u001b[39;49m[method], nbits\u001b[39m=\u001b[39;49mnbits, quantize_lut \u001b[39m=\u001b[39;49m quantize_lut,\n\u001b[1;32m     10\u001b[0m                         upcast_every\u001b[39m=\u001b[39;49mupcast_every, lut_work_const\u001b[39m=\u001b[39;49mlut_work_const)\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     est3 \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39mestFactory(X_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mlinearin_path_train\u001b[39m\u001b[39m\"\u001b[39m], W_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mweightpath\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[1;32m     13\u001b[0m                         Y_path\u001b[39m=\u001b[39mAMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39my_train\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39m AMM_train_dirs[\u001b[39m\"\u001b[39m\u001b[39mdir_train\u001b[39m\u001b[39m\"\u001b[39m], ncodebooks\u001b[39m=\u001b[39mncodebooks, \n\u001b[1;32m     14\u001b[0m                         ncentroids\u001b[39m=\u001b[39mncentroids, methods\u001b[39m=\u001b[39m[method], nbits\u001b[39m=\u001b[39mnbits, quantize_lut \u001b[39m=\u001b[39m quantize_lut,\n\u001b[1;32m     15\u001b[0m                         upcast_every\u001b[39m=\u001b[39mupcast_every)\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../matmul.py:330\u001b[0m, in \u001b[0;36mestFactory\u001b[0;34m(methods, ntasks, ncodebooks, ncentroids, verbose, limit_ntasks, tasks_all_same_shape, tasks, X_path, W_path, Y_path, bias_path, dir, nbits, quantize_lut, upcast_every, lut_work_const)\u001b[0m\n\u001b[1;32m    326\u001b[0m         est \u001b[39m=\u001b[39m _fitted_est_for_hparams(\n\u001b[1;32m    327\u001b[0m             method_id, hparams_dict,\n\u001b[1;32m    328\u001b[0m             task\u001b[39m.\u001b[39mX_train, task\u001b[39m.\u001b[39mW_train, task\u001b[39m.\u001b[39mY_train, bias\u001b[39m=\u001b[39mtask\u001b[39m.\u001b[39mbias)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         est \u001b[39m=\u001b[39m _fitted_est_for_hparams(\n\u001b[1;32m    331\u001b[0m             method_id, hparams_dict,\n\u001b[1;32m    332\u001b[0m             task\u001b[39m.\u001b[39;49mX_train, task\u001b[39m.\u001b[39;49mW_train, task\u001b[39m.\u001b[39;49mY_train)\n\u001b[1;32m    333\u001b[0m \u001b[39mexcept\u001b[39;00m amm\u001b[39m.\u001b[39mInvalidParametersException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    334\u001b[0m     \u001b[39m# hparams don't make sense for task (eg, D < d)\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhparams apparently invalid: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached_call(args, kwargs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/joblib/memory.py:537\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    534\u001b[0m         must_call \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m must_call:\n\u001b[0;32m--> 537\u001b[0m     out, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmmap_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         \u001b[39m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[39m# later calls\u001b[39;00m\n\u001b[1;32m    541\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/joblib/memory.py:779\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    778\u001b[0m     \u001b[39mprint\u001b[39m(format_call(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 779\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore_backend\u001b[39m.\u001b[39mdump_item(\n\u001b[1;32m    781\u001b[0m     [func_id, args_id], output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n\u001b[1;32m    783\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../matmul.py:247\u001b[0m, in \u001b[0;36m_fitted_est_for_hparams\u001b[0;34m(method_id, hparams_dict, X_train, W_train, Y_train, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m@_memory\u001b[39m\u001b[39m.\u001b[39mcache\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fitted_est_for_hparams\u001b[39m(method_id, hparams_dict, X_train, W_train,\n\u001b[1;32m    245\u001b[0m                             Y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m     est \u001b[39m=\u001b[39m _estimator_for_method_id(method_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhparams_dict)\n\u001b[0;32m--> 247\u001b[0m     est\u001b[39m.\u001b[39;49mfit(X_train, W_train, Y\u001b[39m=\u001b[39;49mY_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m est\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../vq_amm.py:52\u001b[0m, in \u001b[0;36mVQMatmul.fit\u001b[0;34m(self, A, B, Y)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mraise\u001b[39;00m amm\u001b[39m.\u001b[39mInvalidParametersException(\n\u001b[1;32m     50\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mD < C: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m < \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(D, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mncodebooks))\n\u001b[1;32m     51\u001b[0m \u001b[39m# A = X_train B = W_train Y = Y_train\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc\u001b[39m.\u001b[39;49mfit(A, B\u001b[39m.\u001b[39;49mT)\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../vquantizers.py:640\u001b[0m, in \u001b[0;36mMithralEncoder.fit\u001b[0;34m(self, X, Q)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, Q\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplits_lists, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentroids \u001b[39m=\u001b[39m clusterize\u001b[39m.\u001b[39;49mlearn_mithral(\n\u001b[1;32m    641\u001b[0m         X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncodebooks, ncentroids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncentroids, lut_work_const\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlut_work_const,\n\u001b[1;32m    642\u001b[0m         nonzeros_heuristic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnonzeros_heuristic, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached_call(args, kwargs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/joblib/memory.py:537\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    534\u001b[0m         must_call \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m must_call:\n\u001b[0;32m--> 537\u001b[0m     out, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmmap_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         \u001b[39m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[39m# later calls\u001b[39;00m\n\u001b[1;32m    541\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/joblib/memory.py:779\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    778\u001b[0m     \u001b[39mprint\u001b[39m(format_call(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 779\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore_backend\u001b[39m.\u001b[39mdump_item(\n\u001b[1;32m    781\u001b[0m     [func_id, args_id], output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n\u001b[1;32m    783\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:2064\u001b[0m, in \u001b[0;36mlearn_mithral\u001b[0;34m(X, ncodebooks, ncentroids, return_buckets, lut_work_const, **kwargs)\u001b[0m\n\u001b[1;32m   2062\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfitting dense lstsq to X_res\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2063\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  with X_enc:\u001b[39m\u001b[39m{\u001b[39;00mX_enc\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m Y:\u001b[39m\u001b[39m{\u001b[39;00mX_res\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2064\u001b[0m     W \u001b[39m=\u001b[39m encoded_lstsq(X_enc\u001b[39m=\u001b[39;49mX_enc, Y\u001b[39m=\u001b[39;49mX_res, K\u001b[39m=\u001b[39;49mncentroids)\n\u001b[1;32m   2065\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfitted dense lstsq with W:\u001b[39m\u001b[39m{\u001b[39;00mW\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2066\u001b[0m     \u001b[39m#exit(0)\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:1268\u001b[0m, in \u001b[0;36mencoded_lstsq\u001b[0;34m(X_enc, X_bin, Y, K, XtX, XtY, precondition, stable_ridge)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencoded_lstsq\u001b[39m(X_enc\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, X_bin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, Y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, K\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, XtX\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, XtY\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1265\u001b[0m                   precondition\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stable_ridge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m stable_ridge:\n\u001b[0;32m-> 1268\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_ridge_enc(X_enc\u001b[39m=\u001b[39;49mX_enc, Y\u001b[39m=\u001b[39;49mY, X_bin\u001b[39m=\u001b[39;49mX_bin, K\u001b[39m=\u001b[39;49mK, lamda\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,mode\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   1269\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnot doing sklearn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1270\u001b[0m     exit(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:1023\u001b[0m, in \u001b[0;36m_fit_ridge_enc\u001b[0;34m(X_enc, Y, K, lamda, X_bin, mode)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1022\u001b[0m     est \u001b[39m=\u001b[39m linear_model\u001b[39m.\u001b[39mRidge(fit_intercept\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, alpha\u001b[39m=\u001b[39mlamda, solver\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m     est\u001b[39m.\u001b[39;49mfit(X_bin, Y)\n\u001b[1;32m   1024\u001b[0m     \u001b[39m# return est.coef_.T\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     sk_result \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1126\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1125\u001b[0m _accept_sparse \u001b[39m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[39m.\u001b[39missparse(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver)\n\u001b[0;32m-> 1126\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1127\u001b[0m     X,\n\u001b[1;32m   1128\u001b[0m     y,\n\u001b[1;32m   1129\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m_accept_sparse,\n\u001b[1;32m   1130\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1131\u001b[0m     multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1132\u001b[0m     y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1133\u001b[0m )\n\u001b[1;32m   1134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/shared/conda/envs/bolt/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 375. GiB for an array with shape (3072000, 16384) and data type float64"
     ]
    }
   ],
   "source": [
    "if method == METHOD_PLUTO:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut, \n",
    "                        upcast_every=upcast_every, bias_path=AMM_train_dirs[\"biaspath\"],lut_work_const=-1)\n",
    "elif method == METHOD_MITHRAL:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut,\n",
    "                        upcast_every=upcast_every, lut_work_const=lut_work_const)\n",
    "else:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut,\n",
    "                        upcast_every=upcast_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(AMM_train_dirs[\"dir_test\"]+'/'+AMM_train_dirs[\"linearin_path_test\"])\n",
    "w_test = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"weightpath\"])\n",
    "bias = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"biaspath\"])\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "if method == METHOD_PLUTO:\n",
    "    y_out_last = y_out_matmul\n",
    "else:\n",
    "    y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42177775 -0.39216104 -0.48911437 ... -0.28617575  1.18143197\n",
      "  -0.36507135]\n",
      " [-0.46662221 -0.41168101 -0.39882974 ... -0.26375007  1.1454931\n",
      "  -0.3328296 ]\n",
      " [-0.39971023 -0.39388303 -0.47623784 ... -0.29193276  1.22485685\n",
      "  -0.36495896]\n",
      " ...\n",
      " [-0.25578558 -0.24067785 -0.35419173 ... -0.23181242  0.75545779\n",
      "  -0.27953781]\n",
      " [-0.32576602 -0.33141547 -0.37128933 ... -0.08775675  0.85471869\n",
      "  -0.31608656]\n",
      " [-0.20729054 -0.26354787 -0.13859468 ... -0.05452214  0.66584304\n",
      "  -0.15206914]]\n",
      "y_out_last.shape:  (1024000, 512)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 512)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), \n",
    "                                                        y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL or method == METHOD_PLUTO:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i_ql%i_nb%i_uc%i_lwc%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids, quantize_lut, nbits, upcast_every, lut_work_const)), y_out_last_re)\n",
    "elif method == METHOD_PQ or method == METHOD_MITHRALPQ:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i_ql%i_nb%i_uc%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids, quantize_lut, nbits, upcast_every)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids)), y_out_last_re)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3862f9e4f9adfe471d0cfb8974c7ced68c0a2c741b0095dbf74fd3651c38c3e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
