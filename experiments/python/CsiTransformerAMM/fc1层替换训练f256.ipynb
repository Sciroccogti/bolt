{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder末尾全连接层（fc1）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# 获取当前文件所在的文件夹路径\n",
    "if \"__file__\" in globals():\n",
    "    # 获取__file__变量的值\n",
    "    file_path = __file__\n",
    "    # 获取当前文件所在的文件夹路径\n",
    "    dir_now = os.path.dirname(file_path)\n",
    "else:\n",
    "    # 获取当前工作目录\n",
    "    dir_now = os.getcwd()\n",
    "sys.path.append(dir_now)\n",
    "sys.path.append(os.path.join(dir_now, '../'))\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # 防止jupyter爆内存\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_PLUTO\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "quantize_lut = True\n",
    "# for method in [METHOD_MITHRAL, METHOD_PQ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'fc1'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"fc1\"\n",
    "\n",
    "auto_train_change_nbits = False # 是否根据已运行的训练性能结果改变nbits自动训练，（train_sam_num取已训练的最大值）\n",
    "auto_train_change_upcast = False # 是否根据已运行的训练性能结果改变upcast自动训练，（train_sam_num取已训练的最大值）\n",
    "\n",
    "if auto_train_change_nbits:\n",
    "    nbits_trained = 8\n",
    "if auto_train_change_upcast:\n",
    "    upcast_trained = 16\n",
    "nbits_goal = 8\n",
    "upcast_goal = 16\n",
    "if quantize_lut == False:\n",
    "    nbits_goal = 0\n",
    "nbits = nbits_goal # 要运行的量化比特数\n",
    "upcast_every = upcast_goal # 要运行的upcast\n",
    "\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "\n",
    "if not auto_train_change_nbits and not auto_train_change_upcast:\n",
    "    ncodebooks = 256 # max:512\n",
    "    ncentroids = 256\n",
    "    train_sam_num = 650 # 训练集样本数\n",
    "elif auto_train_change_nbits:\n",
    "    param2change = \"nbits\"\n",
    "    param_trained = nbits_trained\n",
    "    param_goal = nbits_goal\n",
    "    cb_ct_ntr_combinations_unique = change_param_auto_run_list(linear_name, method, feedback_bits, param2change, param_trained, param_goal, \"upcast_every\", 16)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "elif auto_train_change_upcast:\n",
    "    param2change = \"upcast_every\"\n",
    "    param_trained = upcast_trained\n",
    "    param_goal = upcast_goal\n",
    "    cb_ct_ntr_combinations_unique = change_param_auto_run_list(linear_name, method, feedback_bits, param2change, param_trained, param_goal, \"nbits\", nbits_goal)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMM_train_dirs = get_AMM_train_dirs(linear_name, linear_name_full, method, feedback_bits, train_sam_num, test_sam_num)\n",
    "create_dir(AMM_train_dirs[\"dir_result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查找以此为前缀的数据集： fc1in_train_f256\n",
      "有比输入样本数更大的数据集，从中提取新数据集\n",
      "生成的数据集前缀:  fc1in train\n",
      "原数据集大小:  (224000, 2048)\n",
      "提取后数据集大小:  (20800, 2048)\n",
      "查找以此为前缀的数据集： fc1out_train_f256\n",
      "有比输入样本数更大的数据集，从中提取新数据集\n",
      "生成的数据集前缀:  fc1out train\n",
      "原数据集大小:  (224000, 256)\n",
      "提取后数据集大小:  (20800, 256)\n"
     ]
    }
   ],
   "source": [
    "dataset_prepare(AMM_train_dirs[\"dir_joined\"], linear_name_full, feedback_bits, [train_sam_num, test_sam_num], \n",
    "                batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (20800, 2048)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.2991740107536316\n",
      "learn_multisplits(): returning loss:  0.05627229084521446\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.359151840209961\n",
      "learn_multisplits(): returning loss:  0.75660998074585\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.4724603295326233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.51s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.08874702100411014\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.17834880948066711\n",
      "learn_multisplits(): returning loss:  0.03785044469930199\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.22982165217399597\n",
      "learn_multisplits(): returning loss:  0.03341400257207283\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.3045293986797333\n",
      "learn_multisplits(): returning loss:  0.06144609204915241\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.12116643786430359\n",
      "learn_multisplits(): returning loss:  0.026105780972230264\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.13698318600654602\n",
      "learn_multisplits(): returning loss:  0.030469580646922623\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.7667447328567505\n",
      "learn_multisplits(): returning loss:  0.12267451205627822\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6.954737663269043\n",
      "learn_multisplits(): returning loss:  1.7789880662767246\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.5875802040100098\n",
      "learn_multisplits(): returning loss:  0.14755313807228276\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.23734448850154877\n",
      "learn_multisplits(): returning loss:  0.06613761771731674\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.5886270999908447\n",
      "learn_multisplits(): returning loss:  0.16616389958102218\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18.104915618896484\n",
      "learn_multisplits(): returning loss:  2.807386174511521\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2.4865992069244385\n",
      "learn_multisplits(): returning loss:  0.5397686762103149\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.5166982412338257\n",
      "learn_multisplits(): returning loss:  0.1522521065647311\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.35721153020858765\n",
      "learn_multisplits(): returning loss:  0.08594871106149071\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.8572235107421875\n",
      "learn_multisplits(): returning loss:  1.4159518760829428\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.833387017250061\n",
      "learn_multisplits(): returning loss:  0.14091722723856037\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.19750083982944489\n",
      "learn_multisplits(): returning loss:  0.05909737113697133\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.21826660633087158\n",
      "learn_multisplits(): returning loss:  0.05271374722849487\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.5834887027740479\n",
      "learn_multisplits(): returning loss:  0.13143197491717729\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.13785496354103088\n",
      "learn_multisplits(): returning loss:  0.04555532511533045\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.1534973382949829\n",
      "learn_multisplits(): returning loss:  0.04839970244267988\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.2504972219467163\n",
      "learn_multisplits(): returning loss:  0.21719373656750918\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.072970390319824\n",
      "learn_multisplits(): returning loss:  3.15847719916178\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.004960536956787\n",
      "learn_multisplits(): returning loss:  0.22162707813049565\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.35179269313812256\n",
      "learn_multisplits(): returning loss:  0.10508228235448769\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.5082879066467285\n",
      "learn_multisplits(): returning loss:  0.205345546117087\n",
      "================================\n",
      "learn_multisplits(): initial loss:    21.151321411132812\n",
      "learn_multisplits(): returning loss:  3.883623077697848\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.582526206970215\n",
      "learn_multisplits(): returning loss:  0.9195459439229126\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.6996946334838867\n",
      "learn_multisplits(): returning loss:  0.2031431259592864\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.38198041915893555\n",
      "learn_multisplits(): returning loss:  0.08424449224102659\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.9563493728637695\n",
      "learn_multisplits(): returning loss:  1.1569227674368108\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8133432865142822\n",
      "learn_multisplits(): returning loss:  0.1410355295104182\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.2290925830602646\n",
      "learn_multisplits(): returning loss:  0.0606175383596792\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.25642701983451843\n",
      "learn_multisplits(): returning loss:  0.054302279673834164\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.4462326169013977\n",
      "learn_multisplits(): returning loss:  0.11968782745287854\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.16592824459075928\n",
      "learn_multisplits(): returning loss:  0.04826263137715614\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.19079551100730896\n",
      "learn_multisplits(): returning loss:  0.04972432684154171\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.7518794536590576\n",
      "learn_multisplits(): returning loss:  0.22546407376569277\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.95169448852539\n",
      "learn_multisplits(): returning loss:  3.285053223946136\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.322104811668396\n",
      "learn_multisplits(): returning loss:  0.27506192106909566\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.417434960603714\n",
      "learn_multisplits(): returning loss:  0.14522945605160942\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.0103108882904053\n",
      "learn_multisplits(): returning loss:  0.23551713988365464\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.28316879272461\n",
      "learn_multisplits(): returning loss:  4.250452657948756\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.986218214035034\n",
      "learn_multisplits(): returning loss:  0.8771963056246932\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8100517988204956\n",
      "learn_multisplits(): returning loss:  0.24346976150195587\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.4378644824028015\n",
      "learn_multisplits(): returning loss:  0.09448858632975998\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.543214797973633\n",
      "learn_multisplits(): returning loss:  1.293316230162418\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8827027678489685\n",
      "learn_multisplits(): returning loss:  0.15415705440648253\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.2646498680114746\n",
      "learn_multisplits(): returning loss:  0.06933856091006874\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.3138136565685272\n",
      "learn_multisplits(): returning loss:  0.061703022401673024\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.5161543488502502\n",
      "learn_multisplits(): returning loss:  0.1334580483566583\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.1842002421617508\n",
      "learn_multisplits(): returning loss:  0.05348027137080891\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.21784049272537231\n",
      "learn_multisplits(): returning loss:  0.056469523481680994\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8497673273086548\n",
      "learn_multisplits(): returning loss:  0.23869415174209507\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.345722198486328\n",
      "learn_multisplits(): returning loss:  3.8209969730642115\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.4727022647857666\n",
      "learn_multisplits(): returning loss:  0.3009887666138411\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.47554248571395874\n",
      "learn_multisplits(): returning loss:  0.15823025188808515\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.281144142150879\n",
      "learn_multisplits(): returning loss:  0.25851740673982704\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18.07526969909668\n",
      "learn_multisplits(): returning loss:  4.481735466846658\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.5566020011901855\n",
      "learn_multisplits(): returning loss:  0.9817911131262917\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.9390374422073364\n",
      "learn_multisplits(): returning loss:  0.2855119465733651\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.24384722113609314\n",
      "learn_multisplits(): returning loss:  0.05984387313625916\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.227017402648926\n",
      "learn_multisplits(): returning loss:  0.7418554545128302\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.3628678619861603\n",
      "learn_multisplits(): returning loss:  0.08163938553622074\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.11838802695274353\n",
      "learn_multisplits(): returning loss:  0.03602921538907822\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.12633085250854492\n",
      "learn_multisplits(): returning loss:  0.030408861665875975\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.24882248044013977\n",
      "learn_multisplits(): returning loss:  0.05906715445760824\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.07295975089073181\n",
      "learn_multisplits(): returning loss:  0.02446159975299622\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.08487950265407562\n",
      "learn_multisplits(): returning loss:  0.028104379430487825\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.7053419947624207\n",
      "learn_multisplits(): returning loss:  0.12392004484655672\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6.91652250289917\n",
      "learn_multisplits(): returning loss:  1.766669603254286\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.5706879496574402\n",
      "learn_multisplits(): returning loss:  0.14560870773652823\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.2408244013786316\n",
      "learn_multisplits(): returning loss:  0.06482134811534479\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.7154104709625244\n",
      "learn_multisplits(): returning loss:  0.17370542168308134\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17.83083724975586\n",
      "learn_multisplits(): returning loss:  2.7774215543556693\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2.4016683101654053\n",
      "learn_multisplits(): returning loss:  0.5400233779582777\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.5198782086372375\n",
      "learn_multisplits(): returning loss:  0.15360537620790637\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.37176305055618286\n",
      "learn_multisplits(): returning loss:  0.08785512568834974\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.73220157623291\n",
      "learn_multisplits(): returning loss:  1.4946074466226946\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8251869082450867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.73s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.14107202028935717\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.2007468044757843\n",
      "learn_multisplits(): returning loss:  0.060281659782674524\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.23140522837638855\n",
      "learn_multisplits(): returning loss:  0.054308675436672615\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.573783814907074\n",
      "learn_multisplits(): returning loss:  0.12497808021328183\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.13783694803714752\n",
      "learn_multisplits(): returning loss:  0.04570079526169479\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.15732510387897491\n",
      "learn_multisplits(): returning loss:  0.049729685836041426\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.2720448970794678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.21600441383282032\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.035934448242188\n",
      "learn_multisplits(): returning loss:  3.0742022448631587\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.9853655695915222\n",
      "learn_multisplits(): returning loss:  0.21914554868885716\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.3500140607357025\n",
      "learn_multisplits(): returning loss:  0.10369771966429653\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.440183162689209\n",
      "learn_multisplits(): returning loss:  0.20738325439845084\n",
      "================================\n",
      "learn_multisplits(): initial loss:    21.287599563598633\n",
      "learn_multisplits(): returning loss:  3.993001374672076\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.495306968688965\n",
      "learn_multisplits(): returning loss:  0.975171108016184\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.6882930994033813\n",
      "learn_multisplits(): returning loss:  0.20661722485033351\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.3298228979110718\n",
      "learn_multisplits(): returning loss:  0.08433333307635849\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.020941257476807\n",
      "learn_multisplits(): returning loss:  1.1339106115582922\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.74440598487854\n",
      "learn_multisplits(): returning loss:  0.13122564371536782\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.18525370955467224\n",
      "learn_multisplits(): returning loss:  0.05663010309990568\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.18439531326293945\n",
      "learn_multisplits(): returning loss:  0.04951733355392207\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.4430728554725647\n",
      "learn_multisplits(): returning loss:  0.11814224742050783\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.12580856680870056\n",
      "learn_multisplits(): returning loss:  0.04449524652841319\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.14988818764686584\n",
      "learn_multisplits(): returning loss:  0.04600847939207142\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.7542604207992554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.2260542402097485\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.758861541748047\n",
      "learn_multisplits(): returning loss:  3.384723482307436\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.4155852794647217\n",
      "learn_multisplits(): returning loss:  0.29218527963280394\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.44203466176986694\n",
      "learn_multisplits(): returning loss:  0.1474462671721426\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.1102139949798584\n",
      "learn_multisplits(): returning loss:  0.23253923854036884\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.458721160888672\n",
      "learn_multisplits(): returning loss:  4.335920313200859\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.9553980827331543\n",
      "learn_multisplits(): returning loss:  0.8936822312680306\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8480081558227539\n",
      "learn_multisplits(): returning loss:  0.24927854930899684\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.3873828947544098\n",
      "learn_multisplits(): returning loss:  0.09256907112481372\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.490264892578125\n",
      "learn_multisplits(): returning loss:  1.2816863139728412\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.7771845459938049\n",
      "learn_multisplits(): returning loss:  0.13767447891950724\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.21044448018074036\n",
      "learn_multisplits(): returning loss:  0.0632340981270545\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.22043399512767792\n",
      "learn_multisplits(): returning loss:  0.05624339655750532\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.4829912781715393\n",
      "learn_multisplits(): returning loss:  0.12742140909892896\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.14100201427936554\n",
      "learn_multisplits(): returning loss:  0.048898981806502206\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.16770200431346893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 1.06s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.05070699193893802\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.8813860416412354\n",
      "learn_multisplits(): returning loss:  0.2386500311138775\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.166105270385742\n",
      "learn_multisplits(): returning loss:  3.701253017345607\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.504663348197937\n",
      "learn_multisplits(): returning loss:  0.30605101152900716\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.4875791072845459\n",
      "learn_multisplits(): returning loss:  0.15785645106710433\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.3436957597732544\n",
      "learn_multisplits(): returning loss:  0.2676150599147604\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18.183162689208984\n",
      "learn_multisplits(): returning loss:  4.631452884782448\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.477822303771973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.97s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.9813974473518101\n",
      "================================\n",
      "learn_multisplits(): initial loss:    0.9515819549560547\n",
      "learn_multisplits(): returning loss:  0.2907766074913525\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.751148223876953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.50s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  1.617673521765937\n",
      "================================\n",
      "learn_multisplits(): initial loss:    108.89567565917969\n",
      "learn_multisplits(): returning loss:  24.50615552562158\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.941627502441406\n",
      "learn_multisplits(): returning loss:  1.9704403348441701\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.132079124450684\n",
      "learn_multisplits(): returning loss:  1.0818113530949631\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.811734199523926\n",
      "learn_multisplits(): returning loss:  0.9410990613672539\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.197708129882812\n",
      "learn_multisplits(): returning loss:  1.5587060405114244\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.16922926902771\n",
      "learn_multisplits(): returning loss:  0.6450624821541169\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.342769622802734\n",
      "learn_multisplits(): returning loss:  0.8905833026285563\n",
      "================================\n",
      "learn_multisplits(): initial loss:    26.828838348388672\n",
      "learn_multisplits(): returning loss:  4.267569979384007\n",
      "================================\n",
      "learn_multisplits(): initial loss:    228.63754272460938\n",
      "learn_multisplits(): returning loss:  61.21891568299907\n",
      "================================\n",
      "learn_multisplits(): initial loss:    20.09417724609375\n",
      "learn_multisplits(): returning loss:  4.697026726338663\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.308260917663574\n",
      "learn_multisplits(): returning loss:  2.3136407253914513\n",
      "================================\n",
      "learn_multisplits(): initial loss:    53.508872985839844\n",
      "learn_multisplits(): returning loss:  5.369292221557771\n",
      "================================\n",
      "learn_multisplits(): initial loss:    856.9932861328125\n",
      "learn_multisplits(): returning loss:  181.15549727308826\n",
      "================================\n",
      "learn_multisplits(): initial loss:    78.43882751464844\n",
      "learn_multisplits(): returning loss:  18.29666906829516\n",
      "================================\n",
      "learn_multisplits(): initial loss:    21.673595428466797\n",
      "learn_multisplits(): returning loss:  6.013079928025036\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10.845830917358398\n",
      "learn_multisplits(): returning loss:  2.6301208037364177\n",
      "================================\n",
      "learn_multisplits(): initial loss:    190.15907287597656\n",
      "learn_multisplits(): returning loss:  44.28587072213122\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.862342834472656\n",
      "learn_multisplits(): returning loss:  3.60302805921674\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.5781731605529785\n",
      "learn_multisplits(): returning loss:  1.6114657394691676\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.389199733734131\n",
      "learn_multisplits(): returning loss:  1.5197093694962405\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.64374542236328\n",
      "learn_multisplits(): returning loss:  3.6405725319473277\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.731924533843994\n",
      "learn_multisplits(): returning loss:  1.2747758887846885\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.249526500701904\n",
      "learn_multisplits(): returning loss:  1.5012685204694662\n",
      "================================\n",
      "learn_multisplits(): initial loss:    38.39326095581055\n",
      "learn_multisplits(): returning loss:  6.994298140823048\n",
      "================================\n",
      "learn_multisplits(): initial loss:    376.7375183105469\n",
      "learn_multisplits(): returning loss:  107.87099056683655\n",
      "================================\n",
      "learn_multisplits(): initial loss:    32.845191955566406\n",
      "learn_multisplits(): returning loss:  7.416050804844417\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10.991721153259277\n",
      "learn_multisplits(): returning loss:  3.7247398646850343\n",
      "================================\n",
      "learn_multisplits(): initial loss:    45.85755920410156\n",
      "learn_multisplits(): returning loss:  6.601538117633027\n",
      "================================\n",
      "learn_multisplits(): initial loss:    960.4136962890625\n",
      "learn_multisplits(): returning loss:  196.39713228656183\n",
      "================================\n",
      "learn_multisplits(): initial loss:    151.61866760253906\n",
      "learn_multisplits(): returning loss:  32.674216363577216\n",
      "================================\n",
      "learn_multisplits(): initial loss:    26.962425231933594\n",
      "learn_multisplits(): returning loss:  7.631473982855823\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.949603080749512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.95s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  2.6671559400028855\n",
      "================================\n",
      "learn_multisplits(): initial loss:    115.23602294921875\n",
      "learn_multisplits(): returning loss:  32.78076748127296\n",
      "================================\n",
      "learn_multisplits(): initial loss:    21.708805084228516\n",
      "learn_multisplits(): returning loss:  3.4921304502631756\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6.846871376037598\n",
      "learn_multisplits(): returning loss:  1.7789245722487976\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.392395973205566\n",
      "learn_multisplits(): returning loss:  1.6212072979687946\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13.772331237792969\n",
      "learn_multisplits(): returning loss:  3.409834056325053\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.2880024909973145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  1.5044285450187544\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.3666815757751465\n",
      "learn_multisplits(): returning loss:  1.6564907797060187\n",
      "================================\n",
      "learn_multisplits(): initial loss:    24.87674903869629\n",
      "learn_multisplits(): returning loss:  7.2265000973111455\n",
      "================================\n",
      "learn_multisplits(): initial loss:    295.7879943847656\n",
      "learn_multisplits(): returning loss:  101.64738956173096\n",
      "================================\n",
      "learn_multisplits(): initial loss:    40.94367980957031\n",
      "learn_multisplits(): returning loss:  9.03813730404363\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14.529218673706055\n",
      "learn_multisplits(): returning loss:  4.880170391599677\n",
      "================================\n",
      "learn_multisplits(): initial loss:    39.265899658203125\n",
      "learn_multisplits(): returning loss:  7.491626765477122\n",
      "================================\n",
      "learn_multisplits(): initial loss:    622.5762939453125\n",
      "learn_multisplits(): returning loss:  184.382526412257\n",
      "================================\n",
      "learn_multisplits(): initial loss:    131.01266479492188\n",
      "learn_multisplits(): returning loss:  30.233918381127296\n",
      "================================\n",
      "learn_multisplits(): initial loss:    27.87750244140625\n",
      "learn_multisplits(): returning loss:  8.372066087002167\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14.296027183532715\n",
      "learn_multisplits(): returning loss:  3.2154956980944007\n",
      "================================\n",
      "learn_multisplits(): initial loss:    138.63356018066406\n",
      "learn_multisplits(): returning loss:  39.268187811088865\n",
      "================================\n",
      "learn_multisplits(): initial loss:    24.029632568359375\n",
      "learn_multisplits(): returning loss:  3.982564109712257\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.977315902709961\n",
      "learn_multisplits(): returning loss:  2.113607626472003\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.42054271697998\n",
      "learn_multisplits(): returning loss:  1.8167265145921192\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.091876983642578\n",
      "learn_multisplits(): returning loss:  3.827716484112898\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6.185179710388184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.64s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  1.7643380970678209\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.05978012084961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.91s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  1.9110592354627443\n",
      "================================\n",
      "learn_multisplits(): initial loss:    27.2127685546875\n",
      "learn_multisplits(): returning loss:  8.034705965175817\n",
      "================================\n",
      "learn_multisplits(): initial loss:    347.33697509765625\n",
      "learn_multisplits(): returning loss:  111.3104541789944\n",
      "================================\n",
      "learn_multisplits(): initial loss:    46.27684020996094\n",
      "learn_multisplits(): returning loss:  10.020431556651602\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.52015495300293\n",
      "learn_multisplits(): returning loss:  5.478300955321174\n",
      "================================\n",
      "learn_multisplits(): initial loss:    46.97695541381836\n",
      "learn_multisplits(): returning loss:  7.831821685929754\n",
      "================================\n",
      "learn_multisplits(): initial loss:    729.42626953125\n",
      "learn_multisplits(): returning loss:  196.5629573254846\n",
      "================================\n",
      "learn_multisplits(): initial loss:    153.1374053955078\n",
      "learn_multisplits(): returning loss:  33.94250755511894\n",
      "================================\n",
      "learn_multisplits(): initial loss:    32.21315002441406\n",
      "learn_multisplits(): returning loss:  9.829752657222343\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.468451499938965\n",
      "learn_multisplits(): returning loss:  1.7203535345852288\n",
      "================================\n",
      "learn_multisplits(): initial loss:    105.87266540527344\n",
      "learn_multisplits(): returning loss:  23.783641682854977\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9.963863372802734\n",
      "learn_multisplits(): returning loss:  1.8047046460396814\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.315903663635254\n",
      "learn_multisplits(): returning loss:  0.9355030741162409\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.393543243408203\n",
      "learn_multisplits(): returning loss:  0.7975463430334457\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.020408630371094\n",
      "learn_multisplits(): returning loss:  1.469135424108572\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1.9400209188461304\n",
      "learn_multisplits(): returning loss:  0.5792981024854953\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2.534634590148926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.83s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  0.7259889886167912\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.965604782104492\n",
      "learn_multisplits(): returning loss:  4.231274491054137\n",
      "================================\n",
      "learn_multisplits(): initial loss:    229.91812133789062\n",
      "learn_multisplits(): returning loss:  62.16547374663651\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19.779747009277344\n",
      "learn_multisplits(): returning loss:  4.788833654331029\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.399316787719727\n",
      "learn_multisplits(): returning loss:  2.2856696613002896\n",
      "================================\n",
      "learn_multisplits(): initial loss:    57.61167907714844\n",
      "learn_multisplits(): returning loss:  5.533070375234274\n",
      "================================\n",
      "learn_multisplits(): initial loss:    847.919921875\n",
      "learn_multisplits(): returning loss:  165.3120364980423\n",
      "================================\n",
      "learn_multisplits(): initial loss:    76.47163391113281\n",
      "learn_multisplits(): returning loss:  18.21149342825811\n",
      "================================\n",
      "learn_multisplits(): initial loss:    22.60369873046875\n",
      "learn_multisplits(): returning loss:  6.171644407146232\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.770975112915039\n",
      "learn_multisplits(): returning loss:  2.7621246759431415\n",
      "================================\n",
      "learn_multisplits(): initial loss:    188.16970825195312\n",
      "learn_multisplits(): returning loss:  49.00121099358512\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.815181732177734\n",
      "learn_multisplits(): returning loss:  3.845336312077052\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6.08590030670166\n",
      "learn_multisplits(): returning loss:  1.7420498194383072\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8.261502265930176\n",
      "learn_multisplits(): returning loss:  1.6372523094738654\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.55025863647461\n",
      "learn_multisplits(): returning loss:  3.58060799267696\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3.8171863555908203\n",
      "learn_multisplits(): returning loss:  1.3162614443972416\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.840447425842285\n",
      "learn_multisplits(): returning loss:  1.608959006238365\n",
      "================================\n",
      "learn_multisplits(): initial loss:    39.4503173828125\n",
      "learn_multisplits(): returning loss:  7.553297636791285\n",
      "================================\n",
      "learn_multisplits(): initial loss:    380.84881591796875\n",
      "learn_multisplits(): returning loss:  114.42181666821125\n",
      "================================\n",
      "learn_multisplits(): initial loss:    34.0925178527832\n",
      "learn_multisplits(): returning loss:  8.092465429361255\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.564661026000977\n",
      "learn_multisplits(): returning loss:  3.7377532297532525\n",
      "================================\n",
      "learn_multisplits(): initial loss:    43.89942169189453\n",
      "learn_multisplits(): returning loss:  6.894382125227821\n",
      "================================\n",
      "learn_multisplits(): initial loss:    985.312744140625\n",
      "learn_multisplits(): returning loss:  200.84189427465026\n",
      "================================\n",
      "learn_multisplits(): initial loss:    148.74168395996094\n",
      "learn_multisplits(): returning loss:  33.49558930785133\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28.296401977539062\n",
      "learn_multisplits(): returning loss:  8.076696259653545\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10.087440490722656\n",
      "learn_multisplits(): returning loss:  2.6099726206246032\n",
      "================================\n",
      "learn_multisplits(): initial loss:    119.74203491210938\n",
      "learn_multisplits(): returning loss:  33.21381662144995\n",
      "================================\n",
      "learn_multisplits(): initial loss:    20.69076156616211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.54s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  3.4113058177736093\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.5245866775512695\n",
      "learn_multisplits(): returning loss:  1.6478503560101672\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.189905643463135\n",
      "learn_multisplits(): returning loss:  1.4904377877905972\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13.075784683227539\n",
      "learn_multisplits(): returning loss:  3.519463509464458\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.04479455947876\n",
      "learn_multisplits(): returning loss:  1.3786665126649496\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.6735615730285645\n",
      "learn_multisplits(): returning loss:  1.5193565023173505\n",
      "================================\n",
      "learn_multisplits(): initial loss:    25.315290451049805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  7.480410398713502\n",
      "================================\n",
      "learn_multisplits(): initial loss:    286.88238525390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n",
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.78s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  103.28229702465978\n",
      "================================\n",
      "learn_multisplits(): initial loss:    44.37852478027344\n",
      "learn_multisplits(): returning loss:  9.806975938321557\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.124500274658203\n",
      "learn_multisplits(): returning loss:  5.269780109621024\n",
      "================================\n",
      "learn_multisplits(): initial loss:    44.20198440551758\n",
      "learn_multisplits(): returning loss:  8.331622273677112\n",
      "================================\n",
      "learn_multisplits(): initial loss:    617.45068359375\n",
      "learn_multisplits(): returning loss:  186.46860224312513\n",
      "================================\n",
      "learn_multisplits(): initial loss:    128.41749572753906\n",
      "learn_multisplits(): returning loss:  30.572480225906475\n",
      "================================\n",
      "learn_multisplits(): initial loss:    29.738935470581055\n",
      "learn_multisplits(): returning loss:  9.234338947844662\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.392220497131348\n",
      "learn_multisplits(): returning loss:  2.8093723433253217\n",
      "================================\n",
      "learn_multisplits(): initial loss:    136.70745849609375\n",
      "learn_multisplits(): returning loss:  38.83293511575357\n",
      "================================\n",
      "learn_multisplits(): initial loss:    21.70858383178711\n",
      "learn_multisplits(): returning loss:  3.5451926717614697\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6.0868330001831055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.98s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  1.7577587006235262\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8.355480194091797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  1.64205873816627\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14.038578033447266\n",
      "learn_multisplits(): returning loss:  3.738777753612794\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4.551261901855469\n",
      "learn_multisplits(): returning loss:  1.5694987067790862\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.656859874725342\n",
      "learn_multisplits(): returning loss:  1.6079018758498478\n",
      "================================\n",
      "learn_multisplits(): initial loss:    27.83416175842285\n",
      "learn_multisplits(): returning loss:  8.150288270142482\n",
      "================================\n",
      "learn_multisplits(): initial loss:    341.4059753417969\n",
      "learn_multisplits(): returning loss:  116.6794429087895\n",
      "================================\n",
      "learn_multisplits(): initial loss:    46.628631591796875\n",
      "learn_multisplits(): returning loss:  9.998236339514733\n",
      "================================\n",
      "learn_multisplits(): initial loss:    16.72370147705078\n",
      "learn_multisplits(): returning loss:  5.452375871480399\n",
      "================================\n",
      "learn_multisplits(): initial loss:    48.27838134765625\n",
      "learn_multisplits(): returning loss:  8.35657532892219\n",
      "================================\n",
      "learn_multisplits(): initial loss:    716.2825317382812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  208.04948867764324\n",
      "================================\n",
      "learn_multisplits(): initial loss:    149.28152465820312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../clusterize.py:134: UserWarning: Persisting input arguments took 0.66s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return optimal_split_val(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_multisplits(): returning loss:  33.77085117386741\n",
      "================================\n",
      "learn_multisplits(): initial loss:    32.080142974853516\n",
      "learn_multisplits(): returning loss:  9.899177164548746\n",
      "X_res mse / X mse:  0.24933168\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(20800, 256) Y:(20800, 2048)\n",
      "fitted dense lstsq with W:(65536, 2048)\n",
      "X_res mse / X mse after lstsq:  0.40351063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/CsiTransformerAMM/../vquantizers.py:640: UserWarning: Persisting input arguments took 1.92s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  self.splits_lists, self.centroids = clusterize.learn_mithral(\n"
     ]
    }
   ],
   "source": [
    "if method == METHOD_PLUTO:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut, \n",
    "                        upcast_every=upcast_every, bias_path=AMM_train_dirs[\"biaspath\"])\n",
    "else:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut,\n",
    "                        upcast_every=upcast_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(AMM_train_dirs[\"dir_test\"]+'/'+AMM_train_dirs[\"linearin_path_test\"])\n",
    "w_test = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"weightpath\"])\n",
    "bias = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"biaspath\"])\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "if method == METHOD_PLUTO:\n",
    "    y_out_last = y_out_matmul\n",
    "else:\n",
    "    y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05257927 -0.41137594  0.3493459  ... -0.335446    0.11707558\n",
      "  -0.11652333]\n",
      " [-0.07266535 -0.03564208 -0.7778557  ... -0.46069062 -0.32128057\n",
      "  -0.17914565]\n",
      " [-0.01004304  0.02698022  0.16147895 ... -0.2102014   0.17969789\n",
      "  -0.05390103]\n",
      " ...\n",
      " [-0.13528766  0.21484716 -0.6526111  ...  0.10291016 -0.2586583\n",
      "   0.07134359]\n",
      " [ 0.36569083  0.15222485 -0.52736646 ...  0.04028785 -0.07079136\n",
      "   0.13396591]\n",
      " [-0.5110215   0.02698022 -0.3394995  ... -0.08495677 -0.2586583\n",
      "  -0.24176796]]\n",
      "y_out_last.shape:  (32000, 256)\n",
      "y_out_last_re.shape:  (1000, 32, 256)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), \n",
    "                                                        y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL or method == METHOD_PQ or method == METHOD_PLUTO or method == METHOD_MITHRALPQ:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i_ql%i_nb%i_uc%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids, quantize_lut, nbits, upcast_every)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids)), y_out_last_re)\n",
    "    # np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%iori.npy' % \n",
    "    #                                                     (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "    #                                                     ncodebooks, ncentroids)), y_out_last)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pqhdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
