{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "dir_now = os.getcwd()\n",
    "sys.path.append(dir_now)\n",
    "sys.path.append(os.path.join(dir_now, '../'))\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # 防止jupyter爆内存\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_PLUTO\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "quantize_lut = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "\n",
    "auto_train = False # 是否根据已运行的训练性能结果自动训练，（train_sam_num取已训练的最大值）\n",
    "\n",
    "nbits_trained = 8\n",
    "nbits_goal = 12\n",
    "if quantize_lut == False:\n",
    "    nbits_goal = 0\n",
    "nbits = nbits_goal # 要运行的量化比特数\n",
    "\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "\n",
    "if not auto_train:\n",
    "    ncodebooks = 128 # max:512\n",
    "    ncentroids = 16\n",
    "    train_sam_num = 100 # 训练集样本数\n",
    "else:\n",
    "    cb_ct_ntr_combinations_unique = change_nbits_auto_run_list(linear_name, method, feedback_bits, nbits_trained, nbits_goal)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMM_train_dirs = get_AMM_train_dirs(linear_name, linear_name_full, method, feedback_bits, train_sam_num, test_sam_num)\n",
    "create_dir(AMM_train_dirs[\"dir_result\"])\n",
    "\n",
    "# host_name = socket.gethostname()\n",
    "# if host_name == 'DESKTOP-PLRL7TK':\n",
    "#     dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "#     dir_result = ''\n",
    "# elif host_name == 'DESKTOP-6FOH47P':\n",
    "#     dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "#     dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "#     linearin_path_train= ''\n",
    "#     linearout_path_train= ''\n",
    "#     linearin_path_test = ''\n",
    "#     linearout_path_test = ''\n",
    "# elif host_name == 'jm-System-Product-Name':\n",
    "#     dir_joined = '/data/hdr/transformer_data/joined'\n",
    "#     dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "#     dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "#     dir_result = '/data/hdr/pq/res'\n",
    "#     linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "#     linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "# else:\n",
    "#     raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "# weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "# biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "# dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "# try:\n",
    "#     os.mkdir(dir_result)\n",
    "# except FileNotFoundError:\n",
    "#     os.makedirs(dir_result)\n",
    "# except FileExistsError:\n",
    "#     pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(AMM_train_dirs[\"dir_joined\"], linear_name_full, feedback_bits, [train_sam_num, test_sam_num], \n",
    "                batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (102400, 512)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    38.521650519160175\n",
      "learn_multisplits(): returning loss:  6.342335985031003\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12029.24512644927\n",
      "learn_multisplits(): returning loss:  213.97922933101654\n",
      "================================\n",
      "learn_multisplits(): initial loss:    362.6251833778368\n",
      "learn_multisplits(): returning loss:  41.49015614877706\n",
      "================================\n",
      "learn_multisplits(): initial loss:    75.07336708581478\n",
      "learn_multisplits(): returning loss:  10.344249208895436\n",
      "================================\n",
      "learn_multisplits(): initial loss:    94.37168779768142\n",
      "learn_multisplits(): returning loss:  6.2384423750420375\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7194.117493112084\n",
      "learn_multisplits(): returning loss:  45.71520411968231\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11052.863543709547\n",
      "learn_multisplits(): returning loss:  193.75118398666382\n",
      "================================\n",
      "learn_multisplits(): initial loss:    670.9603980856156\n",
      "learn_multisplits(): returning loss:  140.7190557718277\n",
      "================================\n",
      "learn_multisplits(): initial loss:    69.00873671556134\n",
      "learn_multisplits(): returning loss:  5.230172380615832\n",
      "================================\n",
      "learn_multisplits(): initial loss:    63.809311828079466\n",
      "learn_multisplits(): returning loss:  8.584205012934735\n",
      "================================\n",
      "learn_multisplits(): initial loss:    37.11618272746047\n",
      "learn_multisplits(): returning loss:  1.6844644022613338\n",
      "================================\n",
      "learn_multisplits(): initial loss:    29.24465948781056\n",
      "learn_multisplits(): returning loss:  3.2024893703530832\n",
      "================================\n",
      "learn_multisplits(): initial loss:    194.19898125682596\n",
      "learn_multisplits(): returning loss:  20.8298041076526\n",
      "================================\n",
      "learn_multisplits(): initial loss:    44.637948094614295\n",
      "learn_multisplits(): returning loss:  6.1910713976325535\n",
      "================================\n",
      "learn_multisplits(): initial loss:    148.73745298497062\n",
      "learn_multisplits(): returning loss:  12.7275440003048\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6358.355308632841\n",
      "learn_multisplits(): returning loss:  53.28646969795227\n",
      "================================\n",
      "learn_multisplits(): initial loss:    457.04402501604653\n",
      "learn_multisplits(): returning loss:  49.68808723986149\n",
      "================================\n",
      "learn_multisplits(): initial loss:    716.1275769077138\n",
      "learn_multisplits(): returning loss:  126.8296218663454\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13997.550764274196\n",
      "learn_multisplits(): returning loss:  112.23234033584595\n",
      "================================\n",
      "learn_multisplits(): initial loss:    552.8919872775252\n",
      "learn_multisplits(): returning loss:  13.077027847740245\n",
      "================================\n",
      "learn_multisplits(): initial loss:    129.0437815242841\n",
      "learn_multisplits(): returning loss:  12.251484784089987\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10916.888643642305\n",
      "learn_multisplits(): returning loss:  48.67114853858948\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13409.134486265462\n",
      "learn_multisplits(): returning loss:  66.5756664276123\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23798.82162735775\n",
      "learn_multisplits(): returning loss:  768.8409805297852\n",
      "================================\n",
      "learn_multisplits(): initial loss:    231.95523682570519\n",
      "learn_multisplits(): returning loss:  21.53370536863804\n",
      "================================\n",
      "learn_multisplits(): initial loss:    91.25663008558782\n",
      "learn_multisplits(): returning loss:  8.20950822267033\n",
      "================================\n",
      "learn_multisplits(): initial loss:    68.92292817726442\n",
      "learn_multisplits(): returning loss:  11.32672847873761\n",
      "================================\n",
      "learn_multisplits(): initial loss:    49.360281852084675\n",
      "learn_multisplits(): returning loss:  5.174172909290513\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12083.319767135297\n",
      "learn_multisplits(): returning loss:  154.66907930374146\n",
      "================================\n",
      "learn_multisplits(): initial loss:    109.81458339683432\n",
      "learn_multisplits(): returning loss:  4.523846571506248\n",
      "================================\n",
      "learn_multisplits(): initial loss:    77.13968921063736\n",
      "learn_multisplits(): returning loss:  6.542930602284903\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13.342623453246546\n",
      "learn_multisplits(): returning loss:  1.497956125450791\n",
      "================================\n",
      "learn_multisplits(): initial loss:    170.42958626307367\n",
      "learn_multisplits(): returning loss:  16.661946834435625\n",
      "================================\n",
      "learn_multisplits(): initial loss:    75.13206312335541\n",
      "learn_multisplits(): returning loss:  8.882461487116025\n",
      "================================\n",
      "learn_multisplits(): initial loss:    62.186698787827005\n",
      "learn_multisplits(): returning loss:  3.6491975607307987\n",
      "================================\n",
      "learn_multisplits(): initial loss:    868.9774414143698\n",
      "learn_multisplits(): returning loss:  120.11959719657898\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2308.3521540066145\n",
      "learn_multisplits(): returning loss:  144.79238510131836\n",
      "================================\n",
      "learn_multisplits(): initial loss:    73.62069432254886\n",
      "learn_multisplits(): returning loss:  2.380311459510158\n",
      "================================\n",
      "learn_multisplits(): initial loss:    508.70487743012745\n",
      "learn_multisplits(): returning loss:  74.89284495729953\n",
      "================================\n",
      "learn_multisplits(): initial loss:    143.12436572082615\n",
      "learn_multisplits(): returning loss:  22.77041578155249\n",
      "================================\n",
      "learn_multisplits(): initial loss:    21.982394570071126\n",
      "learn_multisplits(): returning loss:  1.0373396665198067\n",
      "================================\n",
      "learn_multisplits(): initial loss:    157.69277738564537\n",
      "learn_multisplits(): returning loss:  24.2035823845733\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13289.385895277417\n",
      "learn_multisplits(): returning loss:  61.58680009841919\n",
      "================================\n",
      "learn_multisplits(): initial loss:    518.8446228925015\n",
      "learn_multisplits(): returning loss:  53.76578971743584\n",
      "================================\n",
      "learn_multisplits(): initial loss:    131.99963075204593\n",
      "learn_multisplits(): returning loss:  23.446949239820892\n",
      "================================\n",
      "learn_multisplits(): initial loss:    48.88911490098095\n",
      "learn_multisplits(): returning loss:  5.532734636141754\n",
      "================================\n",
      "learn_multisplits(): initial loss:    153.01009875915636\n",
      "learn_multisplits(): returning loss:  24.277283095826057\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13.55742663577509\n",
      "learn_multisplits(): returning loss:  1.3836358626725387\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9835.986062502234\n",
      "learn_multisplits(): returning loss:  52.899166226387024\n",
      "================================\n",
      "learn_multisplits(): initial loss:    532.9228733357587\n",
      "learn_multisplits(): returning loss:  16.03969396650791\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1091.1931586994694\n",
      "learn_multisplits(): returning loss:  91.15607190132141\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1240.8398377836363\n",
      "learn_multisplits(): returning loss:  10.707472562789917\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9727.769190629444\n",
      "learn_multisplits(): returning loss:  53.75751841068268\n",
      "================================\n",
      "learn_multisplits(): initial loss:    641.1804087728127\n",
      "learn_multisplits(): returning loss:  99.99999153614044\n",
      "================================\n",
      "learn_multisplits(): initial loss:    82.84152455004715\n",
      "learn_multisplits(): returning loss:  4.2621461766480095\n",
      "================================\n",
      "learn_multisplits(): initial loss:    63.99162871870349\n",
      "learn_multisplits(): returning loss:  7.396272936770419\n",
      "================================\n",
      "learn_multisplits(): initial loss:    920.6909718002452\n",
      "learn_multisplits(): returning loss:  52.3433473110199\n",
      "================================\n",
      "learn_multisplits(): initial loss:    36.00417110851533\n",
      "learn_multisplits(): returning loss:  0.7222358225473351\n",
      "================================\n",
      "learn_multisplits(): initial loss:    206.02955859881246\n",
      "learn_multisplits(): returning loss:  29.285817471187304\n",
      "================================\n",
      "learn_multisplits(): initial loss:    78.56454655909306\n",
      "learn_multisplits(): returning loss:  16.504148865520932\n",
      "================================\n",
      "learn_multisplits(): initial loss:    39.624479875352776\n",
      "learn_multisplits(): returning loss:  4.895912121280638\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17169.918958876195\n",
      "learn_multisplits(): returning loss:  622.0914115905762\n",
      "================================\n",
      "learn_multisplits(): initial loss:    689.7854264449744\n",
      "learn_multisplits(): returning loss:  13.935374855995178\n",
      "================================\n",
      "learn_multisplits(): initial loss:    115.25973671892962\n",
      "learn_multisplits(): returning loss:  17.953814336230266\n",
      "================================\n",
      "learn_multisplits(): initial loss:    754.9338112549106\n",
      "learn_multisplits(): returning loss:  69.87785145640373\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.911437595192204\n",
      "learn_multisplits(): returning loss:  2.6759315595792446\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11129.967014037118\n",
      "learn_multisplits(): returning loss:  430.7613830566406\n",
      "================================\n",
      "learn_multisplits(): initial loss:    962.339395832056\n",
      "learn_multisplits(): returning loss:  129.69513508677483\n",
      "================================\n",
      "learn_multisplits(): initial loss:    326.17096755538023\n",
      "learn_multisplits(): returning loss:  60.48387588458786\n",
      "================================\n",
      "learn_multisplits(): initial loss:    43.03137134617602\n",
      "learn_multisplits(): returning loss:  1.966957541382272\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28.67380322174785\n",
      "learn_multisplits(): returning loss:  1.5492972766487993\n",
      "================================\n",
      "learn_multisplits(): initial loss:    75.61373619758389\n",
      "learn_multisplits(): returning loss:  4.722172708776734\n",
      "================================\n",
      "learn_multisplits(): initial loss:    43.355535545719874\n",
      "learn_multisplits(): returning loss:  2.4137480786574725\n",
      "================================\n",
      "learn_multisplits(): initial loss:    512.7330175994358\n",
      "learn_multisplits(): returning loss:  22.960253059864044\n",
      "================================\n",
      "learn_multisplits(): initial loss:    33.44658072635759\n",
      "learn_multisplits(): returning loss:  2.2845365234063193\n",
      "================================\n",
      "learn_multisplits(): initial loss:    132.44269515148622\n",
      "learn_multisplits(): returning loss:  15.76326771502298\n",
      "================================\n",
      "learn_multisplits(): initial loss:    686.4173732632142\n",
      "learn_multisplits(): returning loss:  37.794013161212206\n",
      "================================\n",
      "learn_multisplits(): initial loss:    43.036106324887925\n",
      "learn_multisplits(): returning loss:  8.117177326021459\n",
      "================================\n",
      "learn_multisplits(): initial loss:    32.78266357682823\n",
      "learn_multisplits(): returning loss:  3.2351896321007168\n",
      "================================\n",
      "learn_multisplits(): initial loss:    204.90285629259387\n",
      "learn_multisplits(): returning loss:  2.0622880841471494\n",
      "================================\n",
      "learn_multisplits(): initial loss:    113.40102756231634\n",
      "learn_multisplits(): returning loss:  5.3248876422421\n",
      "================================\n",
      "learn_multisplits(): initial loss:    75.36031777349945\n",
      "learn_multisplits(): returning loss:  6.9770765234568035\n",
      "================================\n",
      "learn_multisplits(): initial loss:    192.01944263620277\n",
      "learn_multisplits(): returning loss:  7.538214737638567\n",
      "================================\n",
      "learn_multisplits(): initial loss:    38.63383494422734\n",
      "learn_multisplits(): returning loss:  2.1960356996082737\n",
      "================================\n",
      "learn_multisplits(): initial loss:    47.89722929643319\n",
      "learn_multisplits(): returning loss:  5.189859459014524\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.93406816469232\n",
      "learn_multisplits(): returning loss:  1.6809727548444997\n",
      "================================\n",
      "learn_multisplits(): initial loss:    72.77428096616048\n",
      "learn_multisplits(): returning loss:  3.4500457417947867\n",
      "================================\n",
      "learn_multisplits(): initial loss:    120.04172204130846\n",
      "learn_multisplits(): returning loss:  7.016742400891104\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1107.2897597686542\n",
      "learn_multisplits(): returning loss:  181.76284012198448\n",
      "================================\n",
      "learn_multisplits(): initial loss:    89.57927748703264\n",
      "learn_multisplits(): returning loss:  10.3306175760046\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1265.245461237558\n",
      "learn_multisplits(): returning loss:  79.0525661855936\n",
      "================================\n",
      "learn_multisplits(): initial loss:    29.597637408604417\n",
      "learn_multisplits(): returning loss:  2.6362203853682376\n",
      "================================\n",
      "learn_multisplits(): initial loss:    262.8817010635308\n",
      "learn_multisplits(): returning loss:  17.3464662589858\n",
      "================================\n",
      "learn_multisplits(): initial loss:    290.12129162727956\n",
      "learn_multisplits(): returning loss:  11.253184612522269\n",
      "================================\n",
      "learn_multisplits(): initial loss:    48.72119852989229\n",
      "learn_multisplits(): returning loss:  5.692601661612118\n",
      "================================\n",
      "learn_multisplits(): initial loss:    239.1272535090779\n",
      "learn_multisplits(): returning loss:  37.19914828411094\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2.074005815435622\n",
      "learn_multisplits(): returning loss:  0.18821114656668714\n",
      "================================\n",
      "learn_multisplits(): initial loss:    211.2935245004445\n",
      "learn_multisplits(): returning loss:  18.178424878957607\n",
      "================================\n",
      "learn_multisplits(): initial loss:    64.15028791169743\n",
      "learn_multisplits(): returning loss:  6.940359849770768\n",
      "================================\n",
      "learn_multisplits(): initial loss:    241.84127502578406\n",
      "learn_multisplits(): returning loss:  19.32402605190873\n",
      "================================\n",
      "learn_multisplits(): initial loss:    66.28853737658385\n",
      "learn_multisplits(): returning loss:  3.7089467249407964\n",
      "================================\n",
      "learn_multisplits(): initial loss:    51.04977774849861\n",
      "learn_multisplits(): returning loss:  5.23275216914305\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11803.382403124533\n",
      "learn_multisplits(): returning loss:  143.3384804725647\n",
      "================================\n",
      "learn_multisplits(): initial loss:    78.61406680313354\n",
      "learn_multisplits(): returning loss:  3.7897657713164765\n",
      "================================\n",
      "learn_multisplits(): initial loss:    58.573142277769854\n",
      "learn_multisplits(): returning loss:  4.091792851637198\n",
      "================================\n",
      "learn_multisplits(): initial loss:    168.57499154640624\n",
      "learn_multisplits(): returning loss:  21.90775064647437\n",
      "================================\n",
      "learn_multisplits(): initial loss:    115.8243845081486\n",
      "learn_multisplits(): returning loss:  18.00039957446382\n",
      "================================\n",
      "learn_multisplits(): initial loss:    170.3736923279086\n",
      "learn_multisplits(): returning loss:  8.734713339672295\n",
      "================================\n",
      "learn_multisplits(): initial loss:    113.52028903365286\n",
      "learn_multisplits(): returning loss:  10.926509045799401\n",
      "================================\n",
      "learn_multisplits(): initial loss:    121.02650911788534\n",
      "learn_multisplits(): returning loss:  13.842609742869731\n",
      "================================\n",
      "learn_multisplits(): initial loss:    37.65079131822317\n",
      "learn_multisplits(): returning loss:  4.749898089166878\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6664.949970547682\n",
      "learn_multisplits(): returning loss:  254.3023042678833\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18.594856677820292\n",
      "learn_multisplits(): returning loss:  1.4743952268814526\n",
      "================================\n",
      "learn_multisplits(): initial loss:    134.88494330683642\n",
      "learn_multisplits(): returning loss:  14.708136691445642\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7718.051028590677\n",
      "learn_multisplits(): returning loss:  127.66675007343292\n",
      "================================\n",
      "learn_multisplits(): initial loss:    25.761406427564037\n",
      "learn_multisplits(): returning loss:  4.530164644378743\n",
      "================================\n",
      "learn_multisplits(): initial loss:    163.89047828745882\n",
      "learn_multisplits(): returning loss:  20.235437749619475\n",
      "================================\n",
      "learn_multisplits(): initial loss:    289.79879939779687\n",
      "learn_multisplits(): returning loss:  34.81044888496399\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28.75865081169565\n",
      "learn_multisplits(): returning loss:  5.310837045531727\n",
      "================================\n",
      "learn_multisplits(): initial loss:    123.51733919376161\n",
      "learn_multisplits(): returning loss:  10.403567046081829\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13011.464051603118\n",
      "learn_multisplits(): returning loss:  74.34338736534119\n",
      "================================\n",
      "learn_multisplits(): initial loss:    71.71931812892548\n",
      "learn_multisplits(): returning loss:  9.82666070437432\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5.806070197339908\n",
      "learn_multisplits(): returning loss:  1.0268523065850896\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8.692851445948095\n",
      "learn_multisplits(): returning loss:  1.4335569869192506\n",
      "================================\n",
      "learn_multisplits(): initial loss:    272.78271209532244\n",
      "learn_multisplits(): returning loss:  55.29933502525091\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9460.518776596864\n",
      "learn_multisplits(): returning loss:  111.3759286403656\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18.352094980784237\n",
      "learn_multisplits(): returning loss:  1.2434055986052175\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9152.805689829678\n",
      "learn_multisplits(): returning loss:  80.67143845558167\n",
      "X_res mse / X mse:  0.0021653322\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(102400, 128) Y:(102400, 512)\n",
      "fitted dense lstsq with W:(2048, 512)\n",
      "X_res mse / X mse after lstsq:  0.0011035061\n",
      "learn_mithral\n",
      "all_centroids:\n",
      " [[[ 3.22480197e-03  9.12906718e-04  1.54937821e-04 ...  7.43754848e-04\n",
      "    3.75552219e-03 -1.38109177e-02]\n",
      "  [ 4.70402651e-02  4.24613681e-04 -1.03234581e-03 ... -6.67349261e-04\n",
      "   -5.18966001e-04 -1.59363244e-02]\n",
      "  [ 2.46612937e-03 -7.89009221e-03 -2.99943436e-04 ...  2.47471966e-03\n",
      "    8.61546781e-04  3.05229740e-04]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  1.72160275e-04  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 6.88337168e-05  9.95931550e-05  5.10742364e-04 ...  1.17597554e-03\n",
      "   -4.15730476e-03 -9.38666402e-04]\n",
      "  [-4.80799557e-04 -2.30832302e-04  1.67451348e-04 ...  7.15051079e-04\n",
      "    1.14818662e-02 -4.24528448e-03]\n",
      "  [-2.57100532e-04  9.19702652e-05  4.27362887e-04 ...  9.49482725e-04\n",
      "   -4.65891464e-03 -6.22293912e-04]\n",
      "  ...\n",
      "  [ 9.06736605e-05 -2.86748516e-04 -1.23328136e-04 ... -6.95613155e-04\n",
      "   -3.80971847e-04  4.82662814e-04]\n",
      "  [ 1.30727421e-04 -3.57690296e-04 -2.38385124e-04 ... -8.24657734e-04\n",
      "   -3.44229979e-03 -3.67418485e-04]\n",
      "  [ 1.13281138e-04 -3.14603298e-04 -1.69563791e-04 ... -7.11343600e-04\n",
      "   -2.67373514e-03  1.53401503e-04]]\n",
      "\n",
      " [[-1.11915164e-04 -7.46885125e-05  2.30849721e-04 ...  8.53284437e-04\n",
      "    1.10490033e-02  6.91724999e-04]\n",
      "  [ 2.17711422e-04  2.23143375e-04  3.20117659e-04 ... -4.67754464e-04\n",
      "   -8.34917836e-03 -8.84509471e-04]\n",
      "  [-9.43808209e-06  3.38630140e-04  2.24659438e-04 ...  4.70973639e-04\n",
      "    1.16269719e-02  1.22819445e-03]\n",
      "  ...\n",
      "  [ 5.84677851e-04  3.07329465e-04 -2.25769822e-04 ... -1.51781045e-04\n",
      "   -1.40783936e-02  1.98044116e-04]\n",
      "  [-2.51177495e-04 -2.23154831e-03  2.75184284e-04 ...  1.36290269e-03\n",
      "    1.71213073e-03 -2.73653073e-03]\n",
      "  [-7.15867034e-04  4.31525055e-04 -4.81738680e-04 ...  3.96390184e-04\n",
      "   -2.12386865e-02 -4.36343579e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 9.39787307e-04  1.81128527e-03 -6.19857339e-04 ... -5.41283982e-03\n",
      "   -3.89728928e-03  7.61673180e-03]\n",
      "  [ 4.51421394e-04  1.37744541e-03 -2.13089108e-04 ... -4.22850810e-03\n",
      "    2.39117217e-04  4.38686181e-03]\n",
      "  [ 6.69007932e-05  9.86236962e-04 -1.88147023e-04 ... -3.33166029e-03\n",
      "    1.48548663e-03  2.43590027e-03]\n",
      "  ...\n",
      "  [-1.90368068e-04 -9.38761048e-04  2.22062416e-04 ...  2.42933305e-03\n",
      "   -1.34124875e-03 -3.10210278e-03]\n",
      "  [-1.87034981e-04 -9.75717965e-04  2.44845287e-04 ...  2.54214811e-03\n",
      "   -1.45534845e-03 -3.21002654e-03]\n",
      "  [-1.38004616e-04 -1.00193510e-03  2.85533461e-04 ...  2.70289858e-03\n",
      "   -1.32487505e-03 -3.32144741e-03]]\n",
      "\n",
      " [[-6.47665074e-05  5.38598470e-05 -3.22160107e-04 ... -3.50240432e-03\n",
      "   -1.19987095e-03  2.14875978e-03]\n",
      "  [-3.09318857e-04 -2.32784150e-04 -3.58710909e-04 ... -3.91187333e-03\n",
      "    5.18792681e-03  6.49565773e-04]\n",
      "  [-1.17147159e-04  6.53176539e-06  1.74885185e-03 ... -4.79649846e-03\n",
      "   -1.00872573e-02  2.53278040e-03]\n",
      "  ...\n",
      "  [-4.50655323e-04 -4.49894142e-04 -6.47289562e-04 ... -1.06032868e-03\n",
      "    8.55170377e-03  4.88652056e-03]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.13971531e-03  3.91579524e-04  4.56187787e-04 ...  3.06956004e-04\n",
      "    3.54912519e-01  4.16666409e-03]\n",
      "  [ 2.20334096e-05  4.10829438e-04 -8.53526726e-05 ...  3.59127502e-04\n",
      "    4.47072148e-01  3.14604142e-03]\n",
      "  [-4.47632156e-05  3.28224269e-04  9.33988704e-05 ...  1.79675757e-04\n",
      "    5.19637465e-01  2.63310573e-03]\n",
      "  ...\n",
      "  [-6.60258593e-05 -2.11395833e-04 -8.88911818e-05 ...  2.63090711e-04\n",
      "    1.13006353e+00 -1.52258773e-03]\n",
      "  [-6.66613996e-05 -1.89356375e-04 -1.08259730e-04 ...  1.97682399e-04\n",
      "    1.17574239e+00 -1.59592857e-03]\n",
      "  [-6.66330743e-05 -1.66774713e-04 -1.27549356e-04 ...  1.72788539e-04\n",
      "    1.21292794e+00 -1.67189073e-03]]]\n"
     ]
    }
   ],
   "source": [
    "if method == METHOD_PLUTO:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut, \n",
    "                        bias_path=AMM_train_dirs[\"biaspath\"])\n",
    "else:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(AMM_train_dirs[\"dir_test\"]+'/'+AMM_train_dirs[\"linearin_path_test\"])\n",
    "w_test = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"weightpath\"])\n",
    "bias = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"biaspath\"])\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "if method == METHOD_PLUTO:\n",
    "    y_out_last = y_out_matmul\n",
    "else:\n",
    "    y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5673933   0.00718309  0.56820637 ...  0.2896802   0.05458234\n",
      "   0.5743582 ]\n",
      " [ 0.51416415  0.01694991  0.53109246 ...  0.33167753  0.03797875\n",
      "   0.53040755]\n",
      " [ 0.57032335  0.01157816  0.55697453 ...  0.2930986   0.03309534\n",
      "   0.5621497 ]\n",
      " ...\n",
      " [ 0.46240005  0.05845888  0.25517988 ...  0.18126856  0.1795976\n",
      "   0.27549365]\n",
      " [ 0.42040274 -0.12857567  0.18974222 ...  0.31019053  0.03553705\n",
      "   0.37951028]\n",
      " [ 0.31345606 -0.02162902  0.29034042 ...  0.0933672   0.12881015\n",
      "   0.28526047]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), \n",
    "                                                        y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL or method == METHOD_PQ or method == METHOD_PLUTO or method == METHOD_MITHRALPQ:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_ql%i_nbits%i_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, quantize_lut, nbits, train_sam_num, test_sam_num, \n",
    "                                                        feedback_bits, ncodebooks, ncentroids)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids)), y_out_last_re)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pqhdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
