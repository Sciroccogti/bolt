{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "dir_now = os.getcwd()\n",
    "sys.path.append(dir_now)\n",
    "sys.path.append(os.path.join(dir_now, '../'))\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # 防止jupyter爆内存\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_PLUTO\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "quantize_lut = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "\n",
    "auto_train = False # 是否根据已运行的训练性能结果自动训练，（train_sam_num取已训练的最大值）\n",
    "\n",
    "nbits_trained = 8\n",
    "nbits_goal = 8\n",
    "if quantize_lut == False:\n",
    "    nbits_goal = 0\n",
    "nbits = nbits_goal # 要运行的量化比特数\n",
    "\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "\n",
    "if not auto_train:\n",
    "    ncodebooks = 64 # max:512\n",
    "    ncentroids = 16\n",
    "    train_sam_num = 100 # 训练集样本数\n",
    "else:\n",
    "    cb_ct_ntr_combinations_unique = change_nbits_auto_run_list(linear_name, method, feedback_bits, nbits_trained, nbits_goal)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMM_train_dirs = get_AMM_train_dirs(linear_name, linear_name_full, method, feedback_bits, train_sam_num, test_sam_num)\n",
    "create_dir(AMM_train_dirs[\"dir_result\"])\n",
    "\n",
    "# host_name = socket.gethostname()\n",
    "# if host_name == 'DESKTOP-PLRL7TK':\n",
    "#     dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "#     dir_result = ''\n",
    "# elif host_name == 'DESKTOP-6FOH47P':\n",
    "#     dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "#     dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "#     linearin_path_train= ''\n",
    "#     linearout_path_train= ''\n",
    "#     linearin_path_test = ''\n",
    "#     linearout_path_test = ''\n",
    "# elif host_name == 'jm-System-Product-Name':\n",
    "#     dir_joined = '/data/hdr/transformer_data/joined'\n",
    "#     dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "#     dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "#     dir_result = '/data/hdr/pq/res'\n",
    "#     linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "#     linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "# else:\n",
    "#     raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "# weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "# biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "# dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "# try:\n",
    "#     os.mkdir(dir_result)\n",
    "# except FileNotFoundError:\n",
    "#     os.makedirs(dir_result)\n",
    "# except FileExistsError:\n",
    "#     pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(AMM_train_dirs[\"dir_joined\"], linear_name_full, feedback_bits, [train_sam_num, test_sam_num], \n",
    "                batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (102400, 512)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12067.76677696843\n",
      "learn_multisplits(): returning loss:  248.2884956598282\n",
      "================================\n",
      "learn_multisplits(): initial loss:    437.69855046365154\n",
      "learn_multisplits(): returning loss:  115.66391400824564\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7288.489180909765\n",
      "learn_multisplits(): returning loss:  137.677903175354\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11723.823941795165\n",
      "learn_multisplits(): returning loss:  638.7195053100586\n",
      "================================\n",
      "learn_multisplits(): initial loss:    132.81804854364083\n",
      "learn_multisplits(): returning loss:  36.05963219350335\n",
      "================================\n",
      "learn_multisplits(): initial loss:    66.36084221527102\n",
      "learn_multisplits(): returning loss:  16.74364847965531\n",
      "================================\n",
      "learn_multisplits(): initial loss:    238.83692935144026\n",
      "learn_multisplits(): returning loss:  61.94578514468796\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6507.092761617811\n",
      "learn_multisplits(): returning loss:  169.4231570661068\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1173.1716019237604\n",
      "learn_multisplits(): returning loss:  357.00137892737985\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14550.442751551722\n",
      "learn_multisplits(): returning loss:  352.66711020469666\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11045.93242516659\n",
      "learn_multisplits(): returning loss:  175.99935913085938\n",
      "================================\n",
      "learn_multisplits(): initial loss:    37207.95611362321\n",
      "learn_multisplits(): returning loss:  1779.1509170532227\n",
      "================================\n",
      "learn_multisplits(): initial loss:    323.211866911293\n",
      "learn_multisplits(): returning loss:  89.34920440241694\n",
      "================================\n",
      "learn_multisplits(): initial loss:    118.2832100293491\n",
      "learn_multisplits(): returning loss:  37.79689294592941\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12193.134350532131\n",
      "learn_multisplits(): returning loss:  261.7787837982178\n",
      "================================\n",
      "learn_multisplits(): initial loss:    90.4823126638839\n",
      "learn_multisplits(): returning loss:  16.559177781443868\n",
      "================================\n",
      "learn_multisplits(): initial loss:    245.56164938642908\n",
      "learn_multisplits(): returning loss:  68.47680334722978\n",
      "================================\n",
      "learn_multisplits(): initial loss:    931.1641402021968\n",
      "learn_multisplits(): returning loss:  180.37963819503784\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2381.9728483291633\n",
      "learn_multisplits(): returning loss:  216.16434049606323\n",
      "================================\n",
      "learn_multisplits(): initial loss:    651.8292431509536\n",
      "learn_multisplits(): returning loss:  205.61959701776505\n",
      "================================\n",
      "learn_multisplits(): initial loss:    179.6751719557165\n",
      "learn_multisplits(): returning loss:  46.13570196124711\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13808.23051816992\n",
      "learn_multisplits(): returning loss:  449.3239002227783\n",
      "================================\n",
      "learn_multisplits(): initial loss:    180.8887456530269\n",
      "learn_multisplits(): returning loss:  64.70245490629017\n",
      "================================\n",
      "learn_multisplits(): initial loss:    166.5675253949315\n",
      "learn_multisplits(): returning loss:  37.79097882201248\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10368.908935837993\n",
      "learn_multisplits(): returning loss:  333.5731234550476\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2332.0329964831058\n",
      "learn_multisplits(): returning loss:  271.11500358581543\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10368.949599402256\n",
      "learn_multisplits(): returning loss:  434.88461923599243\n",
      "================================\n",
      "learn_multisplits(): initial loss:    146.83315326875064\n",
      "learn_multisplits(): returning loss:  36.40142983867845\n",
      "================================\n",
      "learn_multisplits(): initial loss:    956.6951429087605\n",
      "learn_multisplits(): returning loss:  88.03345950320363\n",
      "================================\n",
      "learn_multisplits(): initial loss:    284.5941051579056\n",
      "learn_multisplits(): returning loss:  92.5885127190465\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17209.54343875155\n",
      "learn_multisplits(): returning loss:  660.781415939331\n",
      "================================\n",
      "learn_multisplits(): initial loss:    805.0451631639039\n",
      "learn_multisplits(): returning loss:  95.9853530228138\n",
      "================================\n",
      "learn_multisplits(): initial loss:    778.8452488501028\n",
      "learn_multisplits(): returning loss:  93.5369026362896\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12092.306409869176\n",
      "learn_multisplits(): returning loss:  1011.7932252883911\n",
      "================================\n",
      "learn_multisplits(): initial loss:    369.20233890155623\n",
      "learn_multisplits(): returning loss:  103.37654502074022\n",
      "================================\n",
      "learn_multisplits(): initial loss:    104.28753941933175\n",
      "learn_multisplits(): returning loss:  12.886525573006217\n",
      "================================\n",
      "learn_multisplits(): initial loss:    556.0885531451556\n",
      "learn_multisplits(): returning loss:  61.69498671311885\n",
      "================================\n",
      "learn_multisplits(): initial loss:    165.8892758778438\n",
      "learn_multisplits(): returning loss:  31.09977098264637\n",
      "================================\n",
      "learn_multisplits(): initial loss:    729.4534795881021\n",
      "learn_multisplits(): returning loss:  80.61334689334035\n",
      "================================\n",
      "learn_multisplits(): initial loss:    237.68551986942208\n",
      "learn_multisplits(): returning loss:  19.842561447616834\n",
      "================================\n",
      "learn_multisplits(): initial loss:    188.7613453358158\n",
      "learn_multisplits(): returning loss:  40.50965127537025\n",
      "================================\n",
      "learn_multisplits(): initial loss:    230.6532775804301\n",
      "learn_multisplits(): returning loss:  25.413626903846016\n",
      "================================\n",
      "learn_multisplits(): initial loss:    71.8312974611255\n",
      "learn_multisplits(): returning loss:  14.941065850879179\n",
      "================================\n",
      "learn_multisplits(): initial loss:    192.81600300746894\n",
      "learn_multisplits(): returning loss:  32.562727519088014\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1196.8690372556869\n",
      "learn_multisplits(): returning loss:  267.378802806139\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1294.8430986461624\n",
      "learn_multisplits(): returning loss:  108.57658018171787\n",
      "================================\n",
      "learn_multisplits(): initial loss:    553.0029926908103\n",
      "learn_multisplits(): returning loss:  93.84306863802645\n",
      "================================\n",
      "learn_multisplits(): initial loss:    287.8484520389702\n",
      "learn_multisplits(): returning loss:  72.46439473186368\n",
      "================================\n",
      "learn_multisplits(): initial loss:    213.36753031588012\n",
      "learn_multisplits(): returning loss:  20.252275782778295\n",
      "================================\n",
      "learn_multisplits(): initial loss:    305.9915629374815\n",
      "learn_multisplits(): returning loss:  67.42308669164777\n",
      "================================\n",
      "learn_multisplits(): initial loss:    117.33831512508246\n",
      "learn_multisplits(): returning loss:  24.776101525974983\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11881.996469927666\n",
      "learn_multisplits(): returning loss:  220.55741500854492\n",
      "================================\n",
      "learn_multisplits(): initial loss:    227.14813382417609\n",
      "learn_multisplits(): returning loss:  73.38677593414371\n",
      "================================\n",
      "learn_multisplits(): initial loss:    286.1980768360572\n",
      "learn_multisplits(): returning loss:  60.961218262423465\n",
      "================================\n",
      "learn_multisplits(): initial loss:    234.5467981515382\n",
      "learn_multisplits(): returning loss:  70.91892710145089\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6702.600761865905\n",
      "learn_multisplits(): returning loss:  290.5786323547363\n",
      "================================\n",
      "learn_multisplits(): initial loss:    153.47979998465672\n",
      "learn_multisplits(): returning loss:  27.62792071177045\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7743.812435018241\n",
      "learn_multisplits(): returning loss:  153.12874519824982\n",
      "================================\n",
      "learn_multisplits(): initial loss:    453.68927768525566\n",
      "learn_multisplits(): returning loss:  120.07009440660477\n",
      "================================\n",
      "learn_multisplits(): initial loss:    152.2759900054573\n",
      "learn_multisplits(): returning loss:  30.85264190748492\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13083.183369732044\n",
      "learn_multisplits(): returning loss:  142.17414009571075\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14.498921643288003\n",
      "learn_multisplits(): returning loss:  5.036688133647312\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9733.301488692186\n",
      "learn_multisplits(): returning loss:  364.89823722839355\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9171.157784810463\n",
      "learn_multisplits(): returning loss:  98.83200192451477\n",
      "X_res mse / X mse:  0.0040984917\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(102400, 64) Y:(102400, 512)\n",
      "fitted dense lstsq with W:(1024, 512)\n",
      "X_res mse / X mse after lstsq:  0.0021118498\n",
      "learn_mithral\n",
      "all_centroids:\n",
      " [[[ 7.10644480e-03  6.49576832e-04 -4.42077406e-04 ...  8.54550512e-04\n",
      "   -5.33470511e-03 -1.75292254e-03]\n",
      "  [ 2.97305849e-03  1.50031003e-03  1.19184726e-04 ...  8.55438644e-04\n",
      "    7.68309878e-03 -4.58348542e-03]\n",
      "  [ 3.39981681e-03  5.95193887e-05 -1.79810566e-04 ...  8.27314507e-04\n",
      "   -5.00277989e-03 -1.00134080e-03]\n",
      "  ...\n",
      "  [-2.00153281e-05 -7.94074091e-04 -9.65518993e-05 ... -4.21199540e-04\n",
      "    1.41145277e-03  8.80998792e-04]\n",
      "  [-6.89144304e-04 -8.30574660e-04 -1.53885616e-04 ... -3.76614655e-04\n",
      "   -1.14855357e-03  1.98233858e-04]\n",
      "  [ 1.55301168e-05 -8.26850301e-04 -1.45828730e-04 ... -2.61677953e-04\n",
      "   -3.77492979e-04  6.42693252e-04]]\n",
      "\n",
      " [[-3.21029656e-04 -2.33380735e-04  3.12766526e-04 ... -6.34681710e-05\n",
      "   -2.66890856e-03  2.20365240e-04]\n",
      "  [ 1.69224525e-03 -9.56519798e-04 -1.22328813e-03 ... -2.54605518e-04\n",
      "    4.70361626e-03 -2.44945777e-03]\n",
      "  [ 1.55133210e-04  7.28597282e-04  4.65999852e-04 ... -2.74449878e-04\n",
      "   -2.53985520e-03  1.44615513e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [-3.06103029e-03  2.56657205e-03 -4.31378838e-04 ... -1.75516261e-03\n",
      "   -1.37044666e-02 -2.77790765e-04]\n",
      "  [-1.97376707e-03 -3.35777295e-03  8.60972214e-04 ...  1.33844826e-03\n",
      "    6.26898138e-03 -3.87769891e-03]]\n",
      "\n",
      " [[-3.69655341e-03 -4.40262433e-04 -1.67468027e-03 ... -1.75607088e-03\n",
      "   -1.67989563e-02  6.57773635e-05]\n",
      "  [-4.51614428e-03  2.08635538e-04 -1.64125464e-03 ... -1.21731276e-03\n",
      "   -1.23697743e-02  8.40723864e-04]\n",
      "  [-4.27061599e-03  8.80322303e-04 -1.35239621e-03 ... -7.96835229e-04\n",
      "   -1.05956364e-02  9.01176885e-04]\n",
      "  ...\n",
      "  [ 1.54758221e-03 -5.91781340e-04  6.15288562e-04 ...  6.00720465e-04\n",
      "    9.78661608e-03 -1.10203715e-03]\n",
      "  [ 1.67199282e-03 -6.31573319e-04  6.29800314e-04 ...  6.00906380e-04\n",
      "    1.21768098e-02 -1.13258825e-03]\n",
      "  [ 1.73959078e-03 -6.98091229e-04  6.62286824e-04 ...  5.76491526e-04\n",
      "    1.53160878e-02 -1.15264638e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.60130323e-03 -2.31679901e-03 -4.08894004e-04 ... -4.53076180e-04\n",
      "   -1.31342257e-03 -1.51092745e-03]\n",
      "  [-1.55375211e-03 -2.20837072e-03 -7.97505490e-04 ...  4.66796989e-03\n",
      "    1.02764750e-02 -4.37232695e-04]\n",
      "  [-3.87819513e-04 -2.44902773e-03 -1.50648190e-03 ... -1.47501857e-03\n",
      "   -3.77362571e-03 -3.86580732e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.05525283e-02  7.29731517e-03 -2.55428022e-04 ... -3.47895804e-03\n",
      "   -3.08698043e-03  7.87727814e-03]\n",
      "  [ 5.82856731e-03  4.86929668e-03  3.12283955e-04 ... -3.03774863e-03\n",
      "    1.03276712e-03  5.20090992e-03]\n",
      "  [ 3.28376610e-03  3.43559356e-03  2.01347168e-04 ... -2.36839242e-03\n",
      "    1.28479174e-03  3.07748513e-03]\n",
      "  ...\n",
      "  [-3.13918712e-03 -3.15710832e-03 -3.09642259e-04 ...  1.69385911e-03\n",
      "   -1.01724605e-03 -3.42838303e-03]\n",
      "  [-3.10482364e-03 -3.26890941e-03 -3.07274720e-04 ...  1.72797113e-03\n",
      "   -9.38382174e-04 -3.46907717e-03]\n",
      "  [-2.93354061e-03 -3.37518984e-03 -3.27514659e-04 ...  1.74625264e-03\n",
      "   -1.07121981e-04 -3.44678573e-03]]\n",
      "\n",
      " [[ 1.20800547e-02  1.04076322e-03  2.25065439e-03 ...  1.16971562e-04\n",
      "    3.15243751e-01  1.35046314e-03]\n",
      "  [ 4.27366188e-03  8.71754950e-04  1.35722721e-03 ...  2.64818314e-04\n",
      "    4.18838233e-01  9.80733545e-04]\n",
      "  [ 2.68814107e-03  8.95432662e-04  1.08015607e-03 ...  1.49859348e-04\n",
      "    4.97647464e-01  1.11089856e-03]\n",
      "  ...\n",
      "  [-1.93374557e-03 -6.89851528e-04 -8.16572050e-04 ...  6.40651560e-04\n",
      "    1.15000606e+00 -1.62871274e-05]\n",
      "  [-1.91202026e-03 -6.45570166e-04 -8.26374278e-04 ...  6.47001609e-04\n",
      "    1.19749439e+00  1.86096840e-05]\n",
      "  [-1.91246415e-03 -6.19309896e-04 -8.49915436e-04 ...  6.53723197e-04\n",
      "    1.23678446e+00  4.40235526e-05]]]\n"
     ]
    }
   ],
   "source": [
    "if method == METHOD_PLUTO:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut, \n",
    "                        bias_path=AMM_train_dirs[\"biaspath\"])\n",
    "else:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刚好在生成lut的代码前的质心：\n",
      " [[[ 7.10644480e-03  6.49576832e-04 -4.42077406e-04 ...  8.54550512e-04\n",
      "   -5.33470511e-03 -1.75292254e-03]\n",
      "  [ 2.97305849e-03  1.50031003e-03  1.19184726e-04 ...  8.55438644e-04\n",
      "    7.68309878e-03 -4.58348542e-03]\n",
      "  [ 3.39981681e-03  5.95193887e-05 -1.79810566e-04 ...  8.27314507e-04\n",
      "   -5.00277989e-03 -1.00134080e-03]\n",
      "  ...\n",
      "  [-2.00153281e-05 -7.94074091e-04 -9.65518993e-05 ... -4.21199540e-04\n",
      "    1.41145277e-03  8.80998792e-04]\n",
      "  [-6.89144304e-04 -8.30574660e-04 -1.53885616e-04 ... -3.76614655e-04\n",
      "   -1.14855357e-03  1.98233858e-04]\n",
      "  [ 1.55301168e-05 -8.26850301e-04 -1.45828730e-04 ... -2.61677953e-04\n",
      "   -3.77492979e-04  6.42693252e-04]]\n",
      "\n",
      " [[-3.21029656e-04 -2.33380735e-04  3.12766526e-04 ... -6.34681710e-05\n",
      "   -2.66890856e-03  2.20365240e-04]\n",
      "  [ 1.69224525e-03 -9.56519798e-04 -1.22328813e-03 ... -2.54605518e-04\n",
      "    4.70361626e-03 -2.44945777e-03]\n",
      "  [ 1.55133210e-04  7.28597282e-04  4.65999852e-04 ... -2.74449878e-04\n",
      "   -2.53985520e-03  1.44615513e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [-3.06103029e-03  2.56657205e-03 -4.31378838e-04 ... -1.75516261e-03\n",
      "   -1.37044666e-02 -2.77790765e-04]\n",
      "  [-1.97376707e-03 -3.35777295e-03  8.60972214e-04 ...  1.33844826e-03\n",
      "    6.26898138e-03 -3.87769891e-03]]\n",
      "\n",
      " [[-3.69655341e-03 -4.40262433e-04 -1.67468027e-03 ... -1.75607088e-03\n",
      "   -1.67989563e-02  6.57773635e-05]\n",
      "  [-4.51614428e-03  2.08635538e-04 -1.64125464e-03 ... -1.21731276e-03\n",
      "   -1.23697743e-02  8.40723864e-04]\n",
      "  [-4.27061599e-03  8.80322303e-04 -1.35239621e-03 ... -7.96835229e-04\n",
      "   -1.05956364e-02  9.01176885e-04]\n",
      "  ...\n",
      "  [ 1.54758221e-03 -5.91781340e-04  6.15288562e-04 ...  6.00720465e-04\n",
      "    9.78661608e-03 -1.10203715e-03]\n",
      "  [ 1.67199282e-03 -6.31573319e-04  6.29800314e-04 ...  6.00906380e-04\n",
      "    1.21768098e-02 -1.13258825e-03]\n",
      "  [ 1.73959078e-03 -6.98091229e-04  6.62286824e-04 ...  5.76491526e-04\n",
      "    1.53160878e-02 -1.15264638e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.60130323e-03 -2.31679901e-03 -4.08894004e-04 ... -4.53076180e-04\n",
      "   -1.31342257e-03 -1.51092745e-03]\n",
      "  [-1.55375211e-03 -2.20837072e-03 -7.97505490e-04 ...  4.66796989e-03\n",
      "    1.02764750e-02 -4.37232695e-04]\n",
      "  [-3.87819513e-04 -2.44902773e-03 -1.50648190e-03 ... -1.47501857e-03\n",
      "   -3.77362571e-03 -3.86580732e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.05525283e-02  7.29731517e-03 -2.55428022e-04 ... -3.47895804e-03\n",
      "   -3.08698043e-03  7.87727814e-03]\n",
      "  [ 5.82856731e-03  4.86929668e-03  3.12283955e-04 ... -3.03774863e-03\n",
      "    1.03276712e-03  5.20090992e-03]\n",
      "  [ 3.28376610e-03  3.43559356e-03  2.01347168e-04 ... -2.36839242e-03\n",
      "    1.28479174e-03  3.07748513e-03]\n",
      "  ...\n",
      "  [-3.13918712e-03 -3.15710832e-03 -3.09642259e-04 ...  1.69385911e-03\n",
      "   -1.01724605e-03 -3.42838303e-03]\n",
      "  [-3.10482364e-03 -3.26890941e-03 -3.07274720e-04 ...  1.72797113e-03\n",
      "   -9.38382174e-04 -3.46907717e-03]\n",
      "  [-2.93354061e-03 -3.37518984e-03 -3.27514659e-04 ...  1.74625264e-03\n",
      "   -1.07121981e-04 -3.44678573e-03]]\n",
      "\n",
      " [[ 1.20800547e-02  1.04076322e-03  2.25065439e-03 ...  1.16971562e-04\n",
      "    3.15243751e-01  1.35046314e-03]\n",
      "  [ 4.27366188e-03  8.71754950e-04  1.35722721e-03 ...  2.64818314e-04\n",
      "    4.18838233e-01  9.80733545e-04]\n",
      "  [ 2.68814107e-03  8.95432662e-04  1.08015607e-03 ...  1.49859348e-04\n",
      "    4.97647464e-01  1.11089856e-03]\n",
      "  ...\n",
      "  [-1.93374557e-03 -6.89851528e-04 -8.16572050e-04 ...  6.40651560e-04\n",
      "    1.15000606e+00 -1.62871274e-05]\n",
      "  [-1.91202026e-03 -6.45570166e-04 -8.26374278e-04 ...  6.47001609e-04\n",
      "    1.19749439e+00  1.86096840e-05]\n",
      "  [-1.91246415e-03 -6.19309896e-04 -8.49915436e-04 ...  6.53723197e-04\n",
      "    1.23678446e+00  4.40235526e-05]]]\n",
      "量化的luts：\n",
      " [[[59 58 60 ... 65 65 65]\n",
      "  [35 37 35 ... 35 36 39]\n",
      "  [40 41 41 ... 41 41 41]\n",
      "  ...\n",
      "  [ 7  8  7 ...  7  7  7]\n",
      "  [61 61 61 ... 63 64 63]\n",
      "  [52 53 54 ... 60 61 61]]\n",
      "\n",
      " [[53 53 52 ... 49 48 49]\n",
      "  [34 41 34 ... 35 28 22]\n",
      "  [42 42 43 ... 47 47 48]\n",
      "  ...\n",
      "  [ 7  9  6 ...  7  7  7]\n",
      "  [63 63 62 ... 63 64 64]\n",
      "  [54 55 55 ... 59 60 60]]\n",
      "\n",
      " [[57 57 57 ... 58 59 59]\n",
      "  [34 27 33 ... 35 37 29]\n",
      "  [40 41 41 ... 44 44 45]\n",
      "  ...\n",
      "  [ 7 11  8 ...  7  7  7]\n",
      "  [61 61 61 ... 62 62 63]\n",
      "  [56 58 59 ... 67 68 69]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[53 55 54 ... 55 55 55]\n",
      "  [36 35 35 ... 35 34 35]\n",
      "  [41 41 41 ... 42 42 42]\n",
      "  ...\n",
      "  [ 8  7  7 ...  7  7  7]\n",
      "  [60 60 58 ... 56 56 56]\n",
      "  [50 48 47 ... 38 37 36]]\n",
      "\n",
      " [[52 57 52 ... 54 54 54]\n",
      "  [32 37 34 ... 35 43 30]\n",
      "  [37 37 37 ... 32 32 31]\n",
      "  ...\n",
      "  [ 7  8  7 ...  7  7  7]\n",
      "  [59 59 59 ... 53 53 53]\n",
      "  [50 50 51 ... 49 49 49]]\n",
      "\n",
      " [[51 52 51 ... 50 51 51]\n",
      "  [33 34 32 ... 35 39 32]\n",
      "  [39 40 40 ... 40 40 40]\n",
      "  ...\n",
      "  [ 8  8  8 ...  7  7  7]\n",
      "  [57 57 56 ... 57 57 58]\n",
      "  [56 58 59 ... 66 67 67]]]\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n",
      "self.upcast_every:\n",
      " 16\n"
     ]
    }
   ],
   "source": [
    "x_test = np.load(AMM_train_dirs[\"dir_test\"]+'/'+AMM_train_dirs[\"linearin_path_test\"])\n",
    "w_test = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"weightpath\"])\n",
    "bias = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"biaspath\"])\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "if method == METHOD_PLUTO:\n",
    "    y_out_last = y_out_matmul\n",
    "else:\n",
    "    y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21000968 -0.3227942   0.6800591  ...  0.19014056  0.00927814\n",
      "   0.38835266]\n",
      " [ 0.21000968 -0.07230497  0.6800591  ...  0.19014056  0.00927814\n",
      "   0.38835266]\n",
      " [ 0.21000968 -0.3227942   0.6800591  ...  0.19014056  0.00927814\n",
      "   0.63884187]\n",
      " ...\n",
      " [ 0.21000968  0.17818426  0.42956984 ...  0.19014056  0.25976738\n",
      "   0.1378634 ]\n",
      " [ 0.46049893 -0.3227942   0.6800591  ... -0.06034867  0.00927814\n",
      "   0.38835266]\n",
      " [ 0.46049893  0.17818426  0.42956984 ...  0.19014056  0.25976738\n",
      "   0.1378634 ]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), \n",
    "                                                        y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL or method == METHOD_PQ or method == METHOD_PLUTO or method == METHOD_MITHRALPQ:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_ql%i_nbits%i_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, quantize_lut, nbits, train_sam_num, test_sam_num, \n",
    "                                                        feedback_bits, ncodebooks, ncentroids)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids)), y_out_last_re)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4b0463fbd4fb37ab8da7acaa6f8646d32ffed54f9f2f3ef4d077d151a736e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
