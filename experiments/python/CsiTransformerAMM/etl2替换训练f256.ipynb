{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "dir_now = os.getcwd()\n",
    "sys.path.append(dir_now)\n",
    "sys.path.append(os.path.join(dir_now, '../'))\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # 防止jupyter爆内存\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_PLUTO\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "quantize_lut = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "\n",
    "auto_train = False # 是否根据已运行的训练性能结果自动训练，（train_sam_num取已训练的最大值）\n",
    "\n",
    "nbits_trained = 8\n",
    "nbits_goal = 8\n",
    "if quantize_lut == False:\n",
    "    nbits_goal = 0\n",
    "nbits = nbits_goal # 要运行的量化比特数\n",
    "\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "\n",
    "if not auto_train:\n",
    "    ncodebooks = 128 # max:512\n",
    "    ncentroids = 16\n",
    "    train_sam_num = 100 # 训练集样本数\n",
    "else:\n",
    "    cb_ct_ntr_combinations_unique = change_nbits_auto_run_list(linear_name, method, feedback_bits, nbits_trained, nbits_goal)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMM_train_dirs = get_AMM_train_dirs(linear_name, linear_name_full, method, feedback_bits, train_sam_num, test_sam_num)\n",
    "create_dir(AMM_train_dirs[\"dir_result\"])\n",
    "\n",
    "# host_name = socket.gethostname()\n",
    "# if host_name == 'DESKTOP-PLRL7TK':\n",
    "#     dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "#     dir_result = ''\n",
    "# elif host_name == 'DESKTOP-6FOH47P':\n",
    "#     dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "#     dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "#     linearin_path_train= ''\n",
    "#     linearout_path_train= ''\n",
    "#     linearin_path_test = ''\n",
    "#     linearout_path_test = ''\n",
    "# elif host_name == 'jm-System-Product-Name':\n",
    "#     dir_joined = '/data/hdr/transformer_data/joined'\n",
    "#     dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "#     dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "#     dir_result = '/data/hdr/pq/res'\n",
    "#     linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "#     linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "#     linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "# else:\n",
    "#     raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "# weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "# biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "# dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "# try:\n",
    "#     os.mkdir(dir_result)\n",
    "# except FileNotFoundError:\n",
    "#     os.makedirs(dir_result)\n",
    "# except FileExistsError:\n",
    "#     pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(AMM_train_dirs[\"dir_joined\"], linear_name_full, feedback_bits, [train_sam_num, test_sam_num], \n",
    "                batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (102400, 512)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    48.87861592479377\n",
      "learn_multisplits(): returning loss:  9.690204874532427\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9724.825918970564\n",
      "learn_multisplits(): returning loss:  131.4862334728241\n",
      "================================\n",
      "learn_multisplits(): initial loss:    344.20393908444737\n",
      "learn_multisplits(): returning loss:  36.511000621637265\n",
      "================================\n",
      "learn_multisplits(): initial loss:    811.1675905607776\n",
      "learn_multisplits(): returning loss:  117.7429518699646\n",
      "================================\n",
      "learn_multisplits(): initial loss:    109.07573994303644\n",
      "learn_multisplits(): returning loss:  6.04623505831367\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6783.18478452502\n",
      "learn_multisplits(): returning loss:  36.99919390678406\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10895.418135684531\n",
      "learn_multisplits(): returning loss:  144.59437704086304\n",
      "================================\n",
      "learn_multisplits(): initial loss:    925.6868399221742\n",
      "learn_multisplits(): returning loss:  208.0370578765869\n",
      "================================\n",
      "learn_multisplits(): initial loss:    42.90604598754898\n",
      "learn_multisplits(): returning loss:  4.259336689524028\n",
      "================================\n",
      "learn_multisplits(): initial loss:    54.60514062987135\n",
      "learn_multisplits(): returning loss:  5.6585488283000895\n",
      "================================\n",
      "learn_multisplits(): initial loss:    73.15043390462523\n",
      "learn_multisplits(): returning loss:  7.594930317236459\n",
      "================================\n",
      "learn_multisplits(): initial loss:    32.64495364840213\n",
      "learn_multisplits(): returning loss:  4.309377972364127\n",
      "================================\n",
      "learn_multisplits(): initial loss:    192.68426717146392\n",
      "learn_multisplits(): returning loss:  23.718054424751152\n",
      "================================\n",
      "learn_multisplits(): initial loss:    159.8760335127977\n",
      "learn_multisplits(): returning loss:  16.363553586724343\n",
      "================================\n",
      "learn_multisplits(): initial loss:    303.58571359015053\n",
      "learn_multisplits(): returning loss:  8.356576301542486\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1059.8543238398117\n",
      "learn_multisplits(): returning loss:  34.19505262374878\n",
      "================================\n",
      "learn_multisplits(): initial loss:    240.47345939506297\n",
      "learn_multisplits(): returning loss:  31.282761052250862\n",
      "================================\n",
      "learn_multisplits(): initial loss:    123.617206199845\n",
      "learn_multisplits(): returning loss:  14.984952019982902\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14745.892980513618\n",
      "learn_multisplits(): returning loss:  146.9901614189148\n",
      "================================\n",
      "learn_multisplits(): initial loss:    580.1508008652637\n",
      "learn_multisplits(): returning loss:  13.33381548071344\n",
      "================================\n",
      "learn_multisplits(): initial loss:    803.589973208487\n",
      "learn_multisplits(): returning loss:  43.98637120053172\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11984.909789142062\n",
      "learn_multisplits(): returning loss:  54.787962913513184\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10445.143765830555\n",
      "learn_multisplits(): returning loss:  73.36266374588013\n",
      "================================\n",
      "learn_multisplits(): initial loss:    24752.238021044737\n",
      "learn_multisplits(): returning loss:  789.373851776123\n",
      "================================\n",
      "learn_multisplits(): initial loss:    63.26930576288189\n",
      "learn_multisplits(): returning loss:  7.389091127702933\n",
      "================================\n",
      "learn_multisplits(): initial loss:    37.41615198161943\n",
      "learn_multisplits(): returning loss:  5.775540027996186\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18.289527069763153\n",
      "learn_multisplits(): returning loss:  2.4421490239222097\n",
      "================================\n",
      "learn_multisplits(): initial loss:    84.74229085447834\n",
      "learn_multisplits(): returning loss:  3.1170358553261357\n",
      "================================\n",
      "learn_multisplits(): initial loss:    15220.584631298194\n",
      "learn_multisplits(): returning loss:  189.0734145641327\n",
      "================================\n",
      "learn_multisplits(): initial loss:    718.6443357180501\n",
      "learn_multisplits(): returning loss:  61.94458189420402\n",
      "================================\n",
      "learn_multisplits(): initial loss:    538.0137786929887\n",
      "learn_multisplits(): returning loss:  27.19827893562615\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10.72749794991439\n",
      "learn_multisplits(): returning loss:  1.3935780707642162\n",
      "================================\n",
      "learn_multisplits(): initial loss:    151.8816385200041\n",
      "learn_multisplits(): returning loss:  10.42235106458024\n",
      "================================\n",
      "learn_multisplits(): initial loss:    248.26782174028105\n",
      "learn_multisplits(): returning loss:  36.85393601283431\n",
      "================================\n",
      "learn_multisplits(): initial loss:    492.4220932300956\n",
      "learn_multisplits(): returning loss:  59.8830344080925\n",
      "================================\n",
      "learn_multisplits(): initial loss:    270.0792612416979\n",
      "learn_multisplits(): returning loss:  22.241531401872635\n",
      "================================\n",
      "learn_multisplits(): initial loss:    204.79178429427398\n",
      "learn_multisplits(): returning loss:  25.84205113653409\n",
      "================================\n",
      "learn_multisplits(): initial loss:    132.4638165050243\n",
      "learn_multisplits(): returning loss:  5.822023450873447\n",
      "================================\n",
      "learn_multisplits(): initial loss:    450.07050609077066\n",
      "learn_multisplits(): returning loss:  51.60228366032243\n",
      "================================\n",
      "learn_multisplits(): initial loss:    90.39537604452637\n",
      "learn_multisplits(): returning loss:  3.038920489217687\n",
      "================================\n",
      "learn_multisplits(): initial loss:    31.52321261050237\n",
      "learn_multisplits(): returning loss:  4.061541475836936\n",
      "================================\n",
      "learn_multisplits(): initial loss:    41.065839601581835\n",
      "learn_multisplits(): returning loss:  4.298055295893143\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12685.266242516378\n",
      "learn_multisplits(): returning loss:  57.90555548667908\n",
      "================================\n",
      "learn_multisplits(): initial loss:    354.8880614380569\n",
      "learn_multisplits(): returning loss:  11.499407529830933\n",
      "================================\n",
      "learn_multisplits(): initial loss:    178.68845948856142\n",
      "learn_multisplits(): returning loss:  22.46404995408234\n",
      "================================\n",
      "learn_multisplits(): initial loss:    35.94641297486311\n",
      "learn_multisplits(): returning loss:  4.081308972291344\n",
      "================================\n",
      "learn_multisplits(): initial loss:    301.79022983384124\n",
      "learn_multisplits(): returning loss:  26.727582961320877\n",
      "================================\n",
      "learn_multisplits(): initial loss:    15.531831359295623\n",
      "learn_multisplits(): returning loss:  1.3553272170289978\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7834.104293756496\n",
      "learn_multisplits(): returning loss:  220.69962978363037\n",
      "================================\n",
      "learn_multisplits(): initial loss:    228.61787333087221\n",
      "learn_multisplits(): returning loss:  6.067112125456333\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1635.035998886677\n",
      "learn_multisplits(): returning loss:  186.0837378501892\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1467.1350760055861\n",
      "learn_multisplits(): returning loss:  13.999485284090042\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12803.010650666502\n",
      "learn_multisplits(): returning loss:  141.99382376670837\n",
      "================================\n",
      "learn_multisplits(): initial loss:    194.45139290702477\n",
      "learn_multisplits(): returning loss:  26.009622653601614\n",
      "================================\n",
      "learn_multisplits(): initial loss:    80.69905304849458\n",
      "learn_multisplits(): returning loss:  5.625159298655404\n",
      "================================\n",
      "learn_multisplits(): initial loss:    81.6599382869922\n",
      "learn_multisplits(): returning loss:  10.067067886475323\n",
      "================================\n",
      "learn_multisplits(): initial loss:    118.61084461199036\n",
      "learn_multisplits(): returning loss:  7.864243635337857\n",
      "================================\n",
      "learn_multisplits(): initial loss:    58.24507361867731\n",
      "learn_multisplits(): returning loss:  4.508529868237207\n",
      "================================\n",
      "learn_multisplits(): initial loss:    121.52126689274772\n",
      "learn_multisplits(): returning loss:  18.89207434937313\n",
      "================================\n",
      "learn_multisplits(): initial loss:    75.46032226725617\n",
      "learn_multisplits(): returning loss:  16.386808385173012\n",
      "================================\n",
      "learn_multisplits(): initial loss:    72.88902337651257\n",
      "learn_multisplits(): returning loss:  11.000522702796012\n",
      "================================\n",
      "learn_multisplits(): initial loss:    17726.52066404263\n",
      "learn_multisplits(): returning loss:  472.1473217010498\n",
      "================================\n",
      "learn_multisplits(): initial loss:    814.4256463054796\n",
      "learn_multisplits(): returning loss:  36.80370133370161\n",
      "================================\n",
      "learn_multisplits(): initial loss:    174.2440600876553\n",
      "learn_multisplits(): returning loss:  18.9606454578297\n",
      "================================\n",
      "learn_multisplits(): initial loss:    261.7734804160917\n",
      "learn_multisplits(): returning loss:  33.136934608221054\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10.658037656528391\n",
      "learn_multisplits(): returning loss:  1.377320271552736\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7948.868912618994\n",
      "learn_multisplits(): returning loss:  257.4008598327637\n",
      "================================\n",
      "learn_multisplits(): initial loss:    818.6057227812018\n",
      "learn_multisplits(): returning loss:  191.25349926948547\n",
      "================================\n",
      "learn_multisplits(): initial loss:    309.00201274815413\n",
      "learn_multisplits(): returning loss:  61.312036859974874\n",
      "================================\n",
      "learn_multisplits(): initial loss:    60.18348026153543\n",
      "learn_multisplits(): returning loss:  5.186515188212047\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.766162823232992\n",
      "learn_multisplits(): returning loss:  2.9741066533649363\n",
      "================================\n",
      "learn_multisplits(): initial loss:    26.580393459317584\n",
      "learn_multisplits(): returning loss:  2.841693134174631\n",
      "================================\n",
      "learn_multisplits(): initial loss:    38.17065286126591\n",
      "learn_multisplits(): returning loss:  2.066347484322861\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1124.0033186152043\n",
      "learn_multisplits(): returning loss:  64.99537390470505\n",
      "================================\n",
      "learn_multisplits(): initial loss:    43.69743184290654\n",
      "learn_multisplits(): returning loss:  2.9027045762927193\n",
      "================================\n",
      "learn_multisplits(): initial loss:    93.81162237529949\n",
      "learn_multisplits(): returning loss:  10.060307529462342\n",
      "================================\n",
      "learn_multisplits(): initial loss:    289.29078786414567\n",
      "learn_multisplits(): returning loss:  22.671264052391052\n",
      "================================\n",
      "learn_multisplits(): initial loss:    117.89322589353915\n",
      "learn_multisplits(): returning loss:  14.268656600162728\n",
      "================================\n",
      "learn_multisplits(): initial loss:    54.25954312908047\n",
      "learn_multisplits(): returning loss:  6.126420075906793\n",
      "================================\n",
      "learn_multisplits(): initial loss:    666.7269360364113\n",
      "learn_multisplits(): returning loss:  6.827836349606514\n",
      "================================\n",
      "learn_multisplits(): initial loss:    31.138384704645702\n",
      "learn_multisplits(): returning loss:  3.0949761433230965\n",
      "================================\n",
      "learn_multisplits(): initial loss:    80.27518673095828\n",
      "learn_multisplits(): returning loss:  1.8174352549529875\n",
      "================================\n",
      "learn_multisplits(): initial loss:    105.03372470666385\n",
      "learn_multisplits(): returning loss:  18.650927703240537\n",
      "================================\n",
      "learn_multisplits(): initial loss:    39.421107689620825\n",
      "learn_multisplits(): returning loss:  3.253157375461383\n",
      "================================\n",
      "learn_multisplits(): initial loss:    47.27964104839223\n",
      "learn_multisplits(): returning loss:  2.8686921051821805\n",
      "================================\n",
      "learn_multisplits(): initial loss:    25.565260223019386\n",
      "learn_multisplits(): returning loss:  1.9138522701445582\n",
      "================================\n",
      "learn_multisplits(): initial loss:    917.0918667471981\n",
      "learn_multisplits(): returning loss:  32.2788460701704\n",
      "================================\n",
      "learn_multisplits(): initial loss:    108.03203681962916\n",
      "learn_multisplits(): returning loss:  5.111715861763083\n",
      "================================\n",
      "learn_multisplits(): initial loss:    260.4151201812831\n",
      "learn_multisplits(): returning loss:  49.14458458404988\n",
      "================================\n",
      "learn_multisplits(): initial loss:    307.0087374848922\n",
      "learn_multisplits(): returning loss:  55.61821015999511\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5383.3637536263295\n",
      "learn_multisplits(): returning loss:  165.13850700855255\n",
      "================================\n",
      "learn_multisplits(): initial loss:    42.80736489219697\n",
      "learn_multisplits(): returning loss:  3.10051551500172\n",
      "================================\n",
      "learn_multisplits(): initial loss:    400.1022031741082\n",
      "learn_multisplits(): returning loss:  25.048863665165467\n",
      "================================\n",
      "learn_multisplits(): initial loss:    219.20673019156578\n",
      "learn_multisplits(): returning loss:  28.928628628232808\n",
      "================================\n",
      "learn_multisplits(): initial loss:    67.27620385844139\n",
      "learn_multisplits(): returning loss:  5.137845340007351\n",
      "================================\n",
      "learn_multisplits(): initial loss:    186.45631737372966\n",
      "learn_multisplits(): returning loss:  19.035980255571754\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23.835386153282855\n",
      "learn_multisplits(): returning loss:  3.2162242414914566\n",
      "================================\n",
      "learn_multisplits(): initial loss:    419.2194046563781\n",
      "learn_multisplits(): returning loss:  35.67778453230858\n",
      "================================\n",
      "learn_multisplits(): initial loss:    25.08821149984948\n",
      "learn_multisplits(): returning loss:  4.538128840929378\n",
      "================================\n",
      "learn_multisplits(): initial loss:    395.97798617488684\n",
      "learn_multisplits(): returning loss:  9.985886543989182\n",
      "================================\n",
      "learn_multisplits(): initial loss:    38.94563489146809\n",
      "learn_multisplits(): returning loss:  5.166436721259794\n",
      "================================\n",
      "learn_multisplits(): initial loss:    113.77927681397287\n",
      "learn_multisplits(): returning loss:  8.427333173967066\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13009.939958798757\n",
      "learn_multisplits(): returning loss:  346.1131429672241\n",
      "================================\n",
      "learn_multisplits(): initial loss:    95.37181093226094\n",
      "learn_multisplits(): returning loss:  12.055160093890546\n",
      "================================\n",
      "learn_multisplits(): initial loss:    141.29510421893917\n",
      "learn_multisplits(): returning loss:  8.701542944071491\n",
      "================================\n",
      "learn_multisplits(): initial loss:    105.57672594954379\n",
      "learn_multisplits(): returning loss:  9.844127147628866\n",
      "================================\n",
      "learn_multisplits(): initial loss:    380.66971810512905\n",
      "learn_multisplits(): returning loss:  57.777853943407536\n",
      "================================\n",
      "learn_multisplits(): initial loss:    70.56681417742011\n",
      "learn_multisplits(): returning loss:  3.8728077517560253\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11.661718471437029\n",
      "learn_multisplits(): returning loss:  1.5202700058134588\n",
      "================================\n",
      "learn_multisplits(): initial loss:    110.51063132288186\n",
      "learn_multisplits(): returning loss:  12.682658996448467\n",
      "================================\n",
      "learn_multisplits(): initial loss:    357.94173442127374\n",
      "learn_multisplits(): returning loss:  9.42456242442131\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10604.077713136145\n",
      "learn_multisplits(): returning loss:  429.40336751937866\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.710287264000257\n",
      "learn_multisplits(): returning loss:  0.6881563871110022\n",
      "================================\n",
      "learn_multisplits(): initial loss:    102.98295129989467\n",
      "learn_multisplits(): returning loss:  19.04384553838667\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8057.342909821965\n",
      "learn_multisplits(): returning loss:  156.94903559982777\n",
      "================================\n",
      "learn_multisplits(): initial loss:    94.18437264416738\n",
      "learn_multisplits(): returning loss:  12.141651362346046\n",
      "================================\n",
      "learn_multisplits(): initial loss:    627.2635489843159\n",
      "learn_multisplits(): returning loss:  55.399438336491585\n",
      "================================\n",
      "learn_multisplits(): initial loss:    501.02671184122704\n",
      "learn_multisplits(): returning loss:  74.9479576125741\n",
      "================================\n",
      "learn_multisplits(): initial loss:    121.73795684354755\n",
      "learn_multisplits(): returning loss:  11.056229272335715\n",
      "================================\n",
      "learn_multisplits(): initial loss:    40.37269393370519\n",
      "learn_multisplits(): returning loss:  2.7663580010682436\n",
      "================================\n",
      "learn_multisplits(): initial loss:    14399.974056473113\n",
      "learn_multisplits(): returning loss:  75.47268915176392\n",
      "================================\n",
      "learn_multisplits(): initial loss:    87.35899600437975\n",
      "learn_multisplits(): returning loss:  9.695507323654201\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7.140987929981177\n",
      "learn_multisplits(): returning loss:  0.9612631829411237\n",
      "================================\n",
      "learn_multisplits(): initial loss:    660.5441749421542\n",
      "learn_multisplits(): returning loss:  21.54175356030464\n",
      "================================\n",
      "learn_multisplits(): initial loss:    120.45716887344042\n",
      "learn_multisplits(): returning loss:  19.908813287744483\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6729.315355335319\n",
      "learn_multisplits(): returning loss:  44.91401505470276\n",
      "================================\n",
      "learn_multisplits(): initial loss:    64.15046450076248\n",
      "learn_multisplits(): returning loss:  3.6992096086867967\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12565.940984432531\n",
      "learn_multisplits(): returning loss:  155.02169370651245\n",
      "X_res mse / X mse:  0.0022499764\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(102400, 128) Y:(102400, 512)\n",
      "fitted dense lstsq with W:(2048, 512)\n",
      "X_res mse / X mse after lstsq:  0.0011237178\n",
      "learn_mithral\n",
      "all_centroids:\n",
      " [[[ 7.56360823e-03 -1.13929017e-03  1.03348400e-02 ...  1.00889441e-03\n",
      "    3.03740264e-03 -5.89149713e-04]\n",
      "  [ 4.29146318e-03  1.20437302e-01  9.72807035e-03 ...  7.12804831e-05\n",
      "   -5.50671713e-03 -7.81392306e-03]\n",
      "  [ 7.03571457e-03 -8.30247416e-04  7.64972940e-02 ...  8.27232841e-04\n",
      "    8.38215742e-03 -2.68486998e-04]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.86314173e-05  2.63641443e-04  4.38401854e-04 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 5.56945754e-03  1.13442598e-04 -3.66335642e-03 ...  5.49140561e-04\n",
      "   -1.41158616e-02  5.07961679e-03]\n",
      "  [ 1.62773998e-03 -1.73518070e-04 -2.44430918e-03 ...  9.71463742e-04\n",
      "   -1.21997492e-02  5.98252285e-03]\n",
      "  [ 7.54025008e-04 -2.78484251e-04 -1.82967656e-03 ...  6.04431378e-04\n",
      "   -1.09108398e-02  5.03583346e-03]\n",
      "  ...\n",
      "  [-8.28142802e-04 -1.33411377e-04  1.46039634e-03 ... -8.99412698e-05\n",
      "    9.54601727e-03 -2.45459354e-03]\n",
      "  [-7.64618977e-04 -1.72015352e-04  1.35640451e-03 ... -1.05387116e-04\n",
      "    1.01391226e-02 -2.72234809e-03]\n",
      "  [-7.58333597e-04 -2.04982061e-04  1.28203945e-03 ... -1.06426894e-04\n",
      "    1.21433875e-02 -2.23464891e-03]]\n",
      "\n",
      " [[ 7.74960354e-05 -5.77240658e-04  1.62083001e-04 ...  3.63972125e-04\n",
      "   -3.40416376e-03  6.00515492e-03]\n",
      "  [-1.87554979e-04 -2.54251994e-04  2.90392840e-04 ...  9.95557712e-05\n",
      "   -2.46922951e-03  4.79120621e-03]\n",
      "  [ 2.25449228e-04 -5.49974618e-04  4.47123661e-04 ...  5.10798069e-04\n",
      "   -4.95193526e-03  9.94939823e-03]\n",
      "  ...\n",
      "  [-9.78975324e-04 -6.60541555e-05 -1.51851203e-03 ... -5.89062838e-05\n",
      "   -4.64532245e-03  3.71343689e-03]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.80816930e-03 -5.64786664e-04  8.44854396e-03 ...  2.18895948e-04\n",
      "   -2.92346645e-02  1.49377594e-02]\n",
      "  [ 6.61310507e-04 -3.83993261e-04  4.76905983e-03 ... -7.72692583e-05\n",
      "   -2.03406606e-02  1.56876855e-02]\n",
      "  [ 2.77266459e-04 -3.23382992e-04  3.08399205e-03 ... -9.62857084e-05\n",
      "   -1.59814153e-02  1.32435411e-02]\n",
      "  ...\n",
      "  [-5.90531563e-04 -1.19338139e-04 -1.86048681e-03 ...  1.68541170e-04\n",
      "    1.35569973e-02 -9.22842510e-03]\n",
      "  [-5.45319868e-04 -1.16576099e-04 -1.73497887e-03 ...  1.57407747e-04\n",
      "    1.55649064e-02 -1.03005329e-02]\n",
      "  [-4.88941034e-04 -1.09735316e-04 -1.53648783e-03 ...  1.40196324e-04\n",
      "    2.05095839e-02 -1.11678047e-02]]\n",
      "\n",
      " [[ 3.04489047e-04 -1.25453429e-04 -8.68005081e-05 ...  5.39230648e-04\n",
      "    1.45813043e-03  2.98042968e-03]\n",
      "  [-1.67689566e-03 -9.45838750e-04 -1.53900962e-03 ... -6.32020237e-04\n",
      "    8.82008858e-03  2.37258464e-05]\n",
      "  [ 3.28831258e-04 -1.85379700e-04  8.92044118e-05 ...  5.74749487e-04\n",
      "    2.85182009e-03  2.67730933e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [-1.08098891e-03  4.44099918e-04  2.45816121e-03 ... -8.65773982e-05\n",
      "    6.86043920e-03  3.23108397e-03]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.52628438e-03  2.34453962e-03 -1.84189528e-03 ...  3.87094806e-05\n",
      "    4.31470573e-01 -2.18307097e-02]\n",
      "  [ 5.99924169e-05  1.95028505e-03 -1.29760336e-03 ...  1.28404156e-03\n",
      "    5.33586144e-01 -1.56403910e-02]\n",
      "  [-2.37103650e-05  1.01853046e-03 -9.42384882e-04 ...  8.82740482e-04\n",
      "    6.19216681e-01 -1.23310443e-02]\n",
      "  ...\n",
      "  [-5.85026050e-04 -8.72804783e-04  1.22891867e-03 ...  1.39482832e-03\n",
      "    1.33000731e+00  1.92873832e-02]\n",
      "  [-6.30896480e-04 -9.13675118e-04  1.22848211e-03 ...  1.41990639e-03\n",
      "    1.37077880e+00  2.01266054e-02]\n",
      "  [-6.73293776e-04 -9.53912851e-04  1.25198578e-03 ...  1.42450421e-03\n",
      "    1.40488517e+00  2.02716459e-02]]]\n"
     ]
    }
   ],
   "source": [
    "if method == METHOD_PLUTO:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut, \n",
    "                        bias_path=AMM_train_dirs[\"biaspath\"])\n",
    "else:\n",
    "    est3 = mm.estFactory(X_path=AMM_train_dirs[\"linearin_path_train\"], W_path=AMM_train_dirs[\"weightpath\"], \n",
    "                        Y_path=AMM_train_dirs[\"y_train\"], dir= AMM_train_dirs[\"dir_train\"], ncodebooks=ncodebooks, \n",
    "                        ncentroids=ncentroids, methods=[method], nbits=nbits, quantize_lut = quantize_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(AMM_train_dirs[\"dir_test\"]+'/'+AMM_train_dirs[\"linearin_path_test\"])\n",
    "w_test = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"weightpath\"])\n",
    "bias = np.load(AMM_train_dirs[\"dir_train\"]+'/'+AMM_train_dirs[\"biaspath\"])\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "if method == METHOD_PLUTO:\n",
    "    y_out_last = y_out_matmul\n",
    "else:\n",
    "    y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46771798  0.21552014  0.3277554  ... -0.05278443  0.00682067\n",
      "   0.11001119]\n",
      " [ 0.46771798  0.09027553  0.3277554  ...  0.07246019  0.00682067\n",
      "   0.11001119]\n",
      " [ 0.5929626   0.09027553  0.453      ...  0.07246019  0.1320653\n",
      "   0.11001119]\n",
      " ...\n",
      " [ 0.21722874  0.09027553  0.20251077 ...  0.1977048   0.2573099\n",
      "   0.23525581]\n",
      " [ 0.34247336  0.09027553  0.3277554  ...  0.1977048   0.00682067\n",
      "   0.48574507]\n",
      " [ 0.46771798  0.09027553  0.07726616 ...  0.44819403  0.00682067\n",
      "   0.23525581]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), \n",
    "                                                        y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL or method == METHOD_PQ or method == METHOD_PLUTO or method == METHOD_MITHRALPQ:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_ql%i_nbits%i_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, quantize_lut, nbits, train_sam_num, test_sam_num, \n",
    "                                                        feedback_bits, ncodebooks, ncentroids)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(AMM_train_dirs[\"dir_result\"], '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % \n",
    "                                                        (method, linear_name, train_sam_num, test_sam_num, feedback_bits, \n",
    "                                                        ncodebooks, ncentroids)), y_out_last_re)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4b0463fbd4fb37ab8da7acaa6f8646d32ffed54f9f2f3ef4d077d151a736e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
