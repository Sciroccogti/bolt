{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../../../csi_transformer/src/')\n",
    "# import backbone\n",
    "# import image_segmentation\n",
    "import numpy as np\n",
    "import os\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "quantize_lut = False\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "nbits = 8 # 量化比特数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "ncodebooks = 64 # max:512\n",
    "ncentroids = 16\n",
    "\n",
    "train_sam_num = 1000 # 训练集样本数\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = socket.gethostname()\n",
    "if host_name == 'DESKTOP-PLRL7TK':\n",
    "    dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "    dir_result = ''\n",
    "elif host_name == 'DESKTOP-6FOH47P':\n",
    "    dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "    dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "    linearin_path_train= ''\n",
    "    linearout_path_train= ''\n",
    "    linearin_path_test = ''\n",
    "    linearout_path_test = ''\n",
    "elif host_name == 'jm-System-Product-Name':\n",
    "    dir_joined = '/data/hdr/transformer_data/joined'\n",
    "    dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "    dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "    dir_result = '/data/hdr/pq/res'\n",
    "    linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "    linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "else:\n",
    "    raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "try:\n",
    "    os.mkdir(dir_result)\n",
    "except FileNotFoundError:\n",
    "    os.makedirs(dir_result)\n",
    "except FileExistsError:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(dir_joined, linear_name_full, feedback_bits, [train_sam_num, test_sam_num], batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (1024000, 512)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    120314.85963698717\n",
      "learn_multisplits(): returning loss:  3268.6341634478745\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4290.847345591481\n",
      "learn_multisplits(): returning loss:  1178.4152986347658\n",
      "================================\n",
      "learn_multisplits(): initial loss:    72771.6794878001\n",
      "learn_multisplits(): returning loss:  2417.341288024808\n",
      "================================\n",
      "learn_multisplits(): initial loss:    116794.49026703188\n",
      "learn_multisplits(): returning loss:  6356.5859216623285\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1301.899688992608\n",
      "learn_multisplits(): returning loss:  353.157186591269\n",
      "================================\n",
      "learn_multisplits(): initial loss:    681.9981742242628\n",
      "learn_multisplits(): returning loss:  169.44290513187144\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2417.8987373317023\n",
      "learn_multisplits(): returning loss:  638.4456759935385\n",
      "================================\n",
      "learn_multisplits(): initial loss:    65178.43776484956\n",
      "learn_multisplits(): returning loss:  2253.533859306866\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11679.717043149998\n",
      "learn_multisplits(): returning loss:  3604.794132386184\n",
      "================================\n",
      "learn_multisplits(): initial loss:    145111.1661859332\n",
      "learn_multisplits(): returning loss:  3964.655885708615\n",
      "================================\n",
      "learn_multisplits(): initial loss:    110039.91512528704\n",
      "learn_multisplits(): returning loss:  2266.763460584916\n",
      "================================\n",
      "learn_multisplits(): initial loss:    370910.626235957\n",
      "learn_multisplits(): returning loss:  18969.118442864838\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3276.448217955729\n",
      "learn_multisplits(): returning loss:  900.6972147098394\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1160.8050438674145\n",
      "learn_multisplits(): returning loss:  368.74093541286896\n",
      "================================\n",
      "learn_multisplits(): initial loss:    121375.19235972941\n",
      "learn_multisplits(): returning loss:  2737.60892453116\n",
      "================================\n",
      "learn_multisplits(): initial loss:    924.1365649117805\n",
      "learn_multisplits(): returning loss:  171.53848942037214\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2396.354612732993\n",
      "learn_multisplits(): returning loss:  660.7143294524769\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9421.632992579718\n",
      "learn_multisplits(): returning loss:  1868.5333554346676\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23775.930461184405\n",
      "learn_multisplits(): returning loss:  2261.209488914196\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6513.9339987850435\n",
      "learn_multisplits(): returning loss:  2087.6070887701144\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1707.1575167170813\n",
      "learn_multisplits(): returning loss:  414.48414685686043\n",
      "================================\n",
      "learn_multisplits(): initial loss:    138030.7851358725\n",
      "learn_multisplits(): returning loss:  4683.043558437279\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1785.628694545382\n",
      "learn_multisplits(): returning loss:  638.2615947247779\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1688.7462108957288\n",
      "learn_multisplits(): returning loss:  366.48003347573666\n",
      "================================\n",
      "learn_multisplits(): initial loss:    102897.80683636734\n",
      "learn_multisplits(): returning loss:  3146.8828295861854\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23233.732211856528\n",
      "learn_multisplits(): returning loss:  2766.7045506507175\n",
      "================================\n",
      "learn_multisplits(): initial loss:    103062.128348658\n",
      "learn_multisplits(): returning loss:  4645.263099038802\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1452.5933511254213\n",
      "learn_multisplits(): returning loss:  361.6896639181877\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9228.062706804152\n",
      "learn_multisplits(): returning loss:  808.9048544007741\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2983.878116645588\n",
      "learn_multisplits(): returning loss:  956.9774354682643\n",
      "================================\n",
      "learn_multisplits(): initial loss:    171626.41160933522\n",
      "learn_multisplits(): returning loss:  7245.732550240256\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7895.822105132558\n",
      "learn_multisplits(): returning loss:  938.9635204393015\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7651.926288302692\n",
      "learn_multisplits(): returning loss:  859.041395833498\n",
      "================================\n",
      "learn_multisplits(): initial loss:    120727.52969710702\n",
      "learn_multisplits(): returning loss:  10713.646513429136\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3682.5958770479897\n",
      "learn_multisplits(): returning loss:  1028.3433174016704\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1000.5879507597094\n",
      "learn_multisplits(): returning loss:  121.67382408459713\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5571.440746820795\n",
      "learn_multisplits(): returning loss:  608.2423829353625\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1660.7179197641217\n",
      "learn_multisplits(): returning loss:  341.42592908341214\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7064.269216941962\n",
      "learn_multisplits(): returning loss:  781.5651227169781\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2403.7580823063117\n",
      "learn_multisplits(): returning loss:  213.01237631747216\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1828.6621768073633\n",
      "learn_multisplits(): returning loss:  396.06487618160827\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2342.9690856077036\n",
      "learn_multisplits(): returning loss:  268.37128227445237\n",
      "================================\n",
      "learn_multisplits(): initial loss:    734.6685201916141\n",
      "learn_multisplits(): returning loss:  147.33419764209555\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1954.9157101942396\n",
      "learn_multisplits(): returning loss:  325.3083514709772\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12000.35123586464\n",
      "learn_multisplits(): returning loss:  2691.6374337872253\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13072.668183668426\n",
      "learn_multisplits(): returning loss:  1088.035616300585\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5470.338664551283\n",
      "learn_multisplits(): returning loss:  922.8638324314959\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3011.849191696268\n",
      "learn_multisplits(): returning loss:  766.1518289551673\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2059.0232002019306\n",
      "learn_multisplits(): returning loss:  202.94398820663935\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3011.97473729762\n",
      "learn_multisplits(): returning loss:  688.8525644715606\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1262.523839428226\n",
      "learn_multisplits(): returning loss:  256.25849946155574\n",
      "================================\n",
      "learn_multisplits(): initial loss:    118337.23924560308\n",
      "learn_multisplits(): returning loss:  2047.0863118135323\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2261.2193570430923\n",
      "learn_multisplits(): returning loss:  736.6826896404436\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2849.309900133272\n",
      "learn_multisplits(): returning loss:  608.8940664651393\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2350.312581688885\n",
      "learn_multisplits(): returning loss:  711.7277837521599\n",
      "================================\n",
      "learn_multisplits(): initial loss:    67065.02701237261\n",
      "learn_multisplits(): returning loss:  4760.710827203987\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1553.4075814331165\n",
      "learn_multisplits(): returning loss:  272.17283173349665\n",
      "================================\n",
      "learn_multisplits(): initial loss:    77255.63148706952\n",
      "learn_multisplits(): returning loss:  2294.8997506644228\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4616.300667988995\n",
      "learn_multisplits(): returning loss:  1217.897631650043\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1587.8669760648243\n",
      "learn_multisplits(): returning loss:  313.4290453441156\n",
      "================================\n",
      "learn_multisplits(): initial loss:    130950.33230948438\n",
      "learn_multisplits(): returning loss:  1866.7737860299583\n",
      "================================\n",
      "learn_multisplits(): initial loss:    157.7177509616486\n",
      "learn_multisplits(): returning loss:  57.382364857709376\n",
      "================================\n",
      "learn_multisplits(): initial loss:    96857.37840073706\n",
      "learn_multisplits(): returning loss:  4414.65500418535\n",
      "================================\n",
      "learn_multisplits(): initial loss:    91585.81676614357\n",
      "learn_multisplits(): returning loss:  1116.0904555984919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/clusterize.py:1917: UserWarning: Persisting input arguments took 2.13s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  _learn_mithral_initialization(X, ncodebooks, ncentroids=ncentroids, pq_perm_algo='start', **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_res mse / X mse:  0.0044924794\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(1024000, 64) Y:(1024000, 512)\n",
      "fitted dense lstsq with W:(1024, 512)\n",
      "X_res mse / X mse after lstsq:  0.0022449528\n",
      "learn_mithral\n",
      "all_centroids:\n",
      " [[[ 1.45322224e-02  3.13039660e-03 -9.69780376e-04 ... -2.10224243e-04\n",
      "   -4.23338870e-03 -2.29741260e-03]\n",
      "  [ 8.88461713e-03  2.15063780e-03 -5.69185824e-04 ...  1.54080026e-05\n",
      "   -2.54020328e-03 -1.74793310e-03]\n",
      "  [ 6.63551781e-03  1.53242936e-03 -3.26566282e-04 ... -2.76135834e-05\n",
      "   -2.08812300e-03 -1.61394733e-03]\n",
      "  ...\n",
      "  [-1.50239293e-03 -7.96448148e-04  4.43731697e-04 ... -8.19556044e-06\n",
      "    1.76027755e-03 -7.72904459e-05]\n",
      "  [-1.51722052e-03 -7.90609221e-04  4.41892887e-04 ... -1.68926351e-06\n",
      "    2.32751458e-03 -5.28989513e-05]\n",
      "  [-1.44420355e-03 -8.30353296e-04  4.22648183e-04 ...  4.19919270e-05\n",
      "    5.97597333e-03 -5.57548410e-05]]\n",
      "\n",
      " [[ 9.80123412e-04 -1.75773259e-03  1.28981995e-03 ...  1.05602434e-03\n",
      "    6.47093914e-03  1.80635136e-03]\n",
      "  [-7.18182055e-05  6.77167554e-04 -1.09359808e-03 ... -9.59926459e-04\n",
      "   -8.47531296e-03  6.26695168e-04]\n",
      "  [ 1.74262165e-03 -1.24945131e-03  1.26184919e-03 ...  8.16344575e-04\n",
      "    6.61809696e-03  3.23335920e-03]\n",
      "  ...\n",
      "  [-8.95545178e-04  9.10622650e-04 -1.02525472e-03 ... -1.87385979e-03\n",
      "   -8.17091577e-03 -4.60333703e-03]\n",
      "  [ 4.76341083e-05 -6.66582433e-04  1.13570981e-03 ... -1.49369810e-03\n",
      "    8.32290854e-03 -6.74099196e-03]\n",
      "  [-4.08345251e-04  1.96481519e-03  1.61923017e-04 ... -2.27162777e-03\n",
      "    5.07434551e-03 -2.70597450e-03]]\n",
      "\n",
      " [[-6.28269417e-03 -2.84707261e-04 -1.12529029e-03 ... -1.14439509e-03\n",
      "   -1.42990379e-02 -1.73675304e-04]\n",
      "  [-4.78415797e-03 -4.53101675e-05 -1.04507373e-03 ... -9.38912737e-04\n",
      "   -1.09020835e-02  3.34004406e-04]\n",
      "  [-3.29878880e-03  1.47935716e-04 -8.27828131e-04 ... -8.12197803e-04\n",
      "   -8.96661263e-03  7.96668901e-05]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.39229279e-03 -6.45708875e-04  5.87309245e-04 ...  4.47417144e-04\n",
      "    1.16321919e-02 -1.32584444e-03]\n",
      "  [ 1.58583815e-03 -6.87340624e-04  5.60832792e-04 ...  4.48326755e-04\n",
      "    1.36853186e-02 -1.35835796e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-8.12517028e-05 -2.03741691e-03  7.01019773e-04 ...  7.53499568e-04\n",
      "   -1.99523452e-03  4.83565702e-04]\n",
      "  [ 1.85817145e-04 -3.10867233e-03  9.89489723e-04 ...  3.41682375e-04\n",
      "   -1.52656832e-03 -3.11697787e-03]\n",
      "  [-2.01245164e-03 -2.09929934e-03 -1.59109026e-04 ... -2.19939757e-04\n",
      "   -4.38140659e-03 -2.01993925e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 9.75381117e-03  6.62777014e-03 -8.43404268e-05 ... -3.15646688e-03\n",
      "   -4.37937630e-03  7.24209612e-03]\n",
      "  [ 4.91623441e-03  4.20372933e-03  2.91820499e-04 ... -2.53398647e-03\n",
      "   -1.10603648e-03  4.90667438e-03]\n",
      "  [ 2.52332306e-03  2.73783645e-03  4.42627206e-04 ... -2.21898663e-03\n",
      "   -4.50700609e-04  2.48772465e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [-3.61636188e-03 -3.09350574e-03 -1.54049470e-04 ...  1.87882257e-03\n",
      "    3.37402540e-04 -4.17512422e-03]\n",
      "  [-3.52125824e-03 -3.20005696e-03 -1.76290632e-04 ...  1.97769958e-03\n",
      "    3.82910541e-04 -4.24740836e-03]]\n",
      "\n",
      " [[ 1.09370556e-02  5.05757693e-04  2.29326892e-03 ...  5.36197273e-04\n",
      "    3.13849688e-01  3.66463064e-04]\n",
      "  [ 3.62647744e-03  6.71530433e-04  1.41608121e-03 ...  4.85844153e-04\n",
      "    4.23881263e-01  1.11861585e-03]\n",
      "  [ 2.06644391e-03  4.26978047e-04  1.16907747e-03 ...  6.36667770e-04\n",
      "    4.97288555e-01  8.85817572e-04]\n",
      "  ...\n",
      "  [-2.12456775e-03 -5.50629862e-04 -5.34267165e-04 ...  3.81562393e-04\n",
      "    1.19046187e+00 -1.98994938e-04]\n",
      "  [-2.13569775e-03 -5.39425528e-04 -5.36463514e-04 ...  3.75045813e-04\n",
      "    1.19505107e+00 -1.89359242e-04]\n",
      "  [-2.10685004e-03 -5.25033858e-04 -5.43650705e-04 ...  3.56688804e-04\n",
      "    1.21409369e+00 -1.79860421e-04]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/vquantizers.py:628: UserWarning: Persisting input arguments took 2.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  self.splits_lists, self.centroids = clusterize.learn_mithral(\n",
      "/data/hdr/pq/bolt/experiments/python/matmul.py:328: UserWarning: Persisting input arguments took 2.37s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  est = _fitted_est_for_hparams(\n"
     ]
    }
   ],
   "source": [
    "est3 = mm.estFactory(X_path=linearin_path_train, W_path=weightpath, Y_path=y_train, dir= dir_train,\\\n",
    "                    ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method], nbits=nbits, \\\n",
    "                    quantize_lut = quantize_lut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刚好在生成lut的代码前的质心：\n",
      " [[[ 1.45322224e-02  3.13039660e-03 -9.69780376e-04 ... -2.10224243e-04\n",
      "   -4.23338870e-03 -2.29741260e-03]\n",
      "  [ 8.88461713e-03  2.15063780e-03 -5.69185824e-04 ...  1.54080026e-05\n",
      "   -2.54020328e-03 -1.74793310e-03]\n",
      "  [ 6.63551781e-03  1.53242936e-03 -3.26566282e-04 ... -2.76135834e-05\n",
      "   -2.08812300e-03 -1.61394733e-03]\n",
      "  ...\n",
      "  [-1.50239293e-03 -7.96448148e-04  4.43731697e-04 ... -8.19556044e-06\n",
      "    1.76027755e-03 -7.72904459e-05]\n",
      "  [-1.51722052e-03 -7.90609221e-04  4.41892887e-04 ... -1.68926351e-06\n",
      "    2.32751458e-03 -5.28989513e-05]\n",
      "  [-1.44420355e-03 -8.30353296e-04  4.22648183e-04 ...  4.19919270e-05\n",
      "    5.97597333e-03 -5.57548410e-05]]\n",
      "\n",
      " [[ 9.80123412e-04 -1.75773259e-03  1.28981995e-03 ...  1.05602434e-03\n",
      "    6.47093914e-03  1.80635136e-03]\n",
      "  [-7.18182055e-05  6.77167554e-04 -1.09359808e-03 ... -9.59926459e-04\n",
      "   -8.47531296e-03  6.26695168e-04]\n",
      "  [ 1.74262165e-03 -1.24945131e-03  1.26184919e-03 ...  8.16344575e-04\n",
      "    6.61809696e-03  3.23335920e-03]\n",
      "  ...\n",
      "  [-8.95545178e-04  9.10622650e-04 -1.02525472e-03 ... -1.87385979e-03\n",
      "   -8.17091577e-03 -4.60333703e-03]\n",
      "  [ 4.76341083e-05 -6.66582433e-04  1.13570981e-03 ... -1.49369810e-03\n",
      "    8.32290854e-03 -6.74099196e-03]\n",
      "  [-4.08345251e-04  1.96481519e-03  1.61923017e-04 ... -2.27162777e-03\n",
      "    5.07434551e-03 -2.70597450e-03]]\n",
      "\n",
      " [[-6.28269417e-03 -2.84707261e-04 -1.12529029e-03 ... -1.14439509e-03\n",
      "   -1.42990379e-02 -1.73675304e-04]\n",
      "  [-4.78415797e-03 -4.53101675e-05 -1.04507373e-03 ... -9.38912737e-04\n",
      "   -1.09020835e-02  3.34004406e-04]\n",
      "  [-3.29878880e-03  1.47935716e-04 -8.27828131e-04 ... -8.12197803e-04\n",
      "   -8.96661263e-03  7.96668901e-05]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.39229279e-03 -6.45708875e-04  5.87309245e-04 ...  4.47417144e-04\n",
      "    1.16321919e-02 -1.32584444e-03]\n",
      "  [ 1.58583815e-03 -6.87340624e-04  5.60832792e-04 ...  4.48326755e-04\n",
      "    1.36853186e-02 -1.35835796e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-8.12517028e-05 -2.03741691e-03  7.01019773e-04 ...  7.53499568e-04\n",
      "   -1.99523452e-03  4.83565702e-04]\n",
      "  [ 1.85817145e-04 -3.10867233e-03  9.89489723e-04 ...  3.41682375e-04\n",
      "   -1.52656832e-03 -3.11697787e-03]\n",
      "  [-2.01245164e-03 -2.09929934e-03 -1.59109026e-04 ... -2.19939757e-04\n",
      "   -4.38140659e-03 -2.01993925e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 9.75381117e-03  6.62777014e-03 -8.43404268e-05 ... -3.15646688e-03\n",
      "   -4.37937630e-03  7.24209612e-03]\n",
      "  [ 4.91623441e-03  4.20372933e-03  2.91820499e-04 ... -2.53398647e-03\n",
      "   -1.10603648e-03  4.90667438e-03]\n",
      "  [ 2.52332306e-03  2.73783645e-03  4.42627206e-04 ... -2.21898663e-03\n",
      "   -4.50700609e-04  2.48772465e-03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [-3.61636188e-03 -3.09350574e-03 -1.54049470e-04 ...  1.87882257e-03\n",
      "    3.37402540e-04 -4.17512422e-03]\n",
      "  [-3.52125824e-03 -3.20005696e-03 -1.76290632e-04 ...  1.97769958e-03\n",
      "    3.82910541e-04 -4.24740836e-03]]\n",
      "\n",
      " [[ 1.09370556e-02  5.05757693e-04  2.29326892e-03 ...  5.36197273e-04\n",
      "    3.13849688e-01  3.66463064e-04]\n",
      "  [ 3.62647744e-03  6.71530433e-04  1.41608121e-03 ...  4.85844153e-04\n",
      "    4.23881263e-01  1.11861585e-03]\n",
      "  [ 2.06644391e-03  4.26978047e-04  1.16907747e-03 ...  6.36667770e-04\n",
      "    4.97288555e-01  8.85817572e-04]\n",
      "  ...\n",
      "  [-2.12456775e-03 -5.50629862e-04 -5.34267165e-04 ...  3.81562393e-04\n",
      "    1.19046187e+00 -1.98994938e-04]\n",
      "  [-2.13569775e-03 -5.39425528e-04 -5.36463514e-04 ...  3.75045813e-04\n",
      "    1.19505107e+00 -1.89359242e-04]\n",
      "  [-2.10685004e-03 -5.25033858e-04 -5.43650705e-04 ...  3.56688804e-04\n",
      "    1.21409369e+00 -1.79860421e-04]]]\n",
      "未量化的luts：\n",
      " [[[ 8.24400261e-02  1.08284995e-01  1.17650121e-01 ...  1.99766204e-01\n",
      "   -2.84718424e-02  2.04115242e-01]\n",
      "  [-3.09044309e-02  9.52005610e-02 -2.77350917e-02 ...  1.99917063e-01\n",
      "    1.21289745e-01  2.02989936e-01]\n",
      "  [-8.21472891e-03 -8.73807631e-03 -9.90686193e-03 ...  0.00000000e+00\n",
      "   -2.31488608e-02 -2.38713287e-02]\n",
      "  ...\n",
      "  [ 3.11409831e-02 -2.14023199e-02  2.20209248e-02 ...  0.00000000e+00\n",
      "    0.00000000e+00 -1.35802943e-03]\n",
      "  [ 1.77705809e-02  1.86180249e-02  1.57806240e-02 ...  0.00000000e+00\n",
      "    4.52914834e-02  4.26671505e-02]\n",
      "  [-4.78255190e-02 -2.91667003e-02 -1.49284936e-02 ...  1.17272884e-01\n",
      "    1.17932409e-01  1.22305766e-01]]\n",
      "\n",
      " [[ 5.92323169e-02  4.21981215e-02  3.43709216e-02 ... -4.66847382e-02\n",
      "   -3.80264856e-02 -5.11643514e-02]\n",
      "  [ 2.61604600e-03 -1.89379573e-01  1.27165047e-02 ... -3.76327306e-01\n",
      "    1.32255815e-02 -4.08615559e-01]\n",
      "  [-1.16992490e-02  7.56031484e-04  9.37127322e-03 ...  0.00000000e+00\n",
      "    7.61599839e-02  8.50418136e-02]\n",
      "  ...\n",
      "  [-2.38001486e-03  8.32195859e-03  1.73389278e-02 ...  0.00000000e+00\n",
      "    0.00000000e+00  6.45421632e-03]\n",
      "  [ 2.45375559e-02  2.40359195e-02  2.49909051e-02 ...  0.00000000e+00\n",
      "    5.65375164e-02  5.79424426e-02]\n",
      "  [-1.50113385e-02 -1.54698268e-03  6.76346570e-03 ...  1.04294866e-01\n",
      "    1.05058499e-01  1.06973574e-01]]\n",
      "\n",
      " [[ 5.38698658e-02  6.35953769e-02  6.17147833e-02 ...  9.76264402e-02\n",
      "   -1.46320332e-02  1.01608932e-01]\n",
      "  [-1.04357265e-02 -7.20438585e-02 -2.38643847e-02 ... -1.01905227e-01\n",
      "   -3.21672261e-02 -1.18433766e-01]\n",
      "  [-2.75786407e-02 -1.42286383e-02 -5.92620112e-03 ...  0.00000000e+00\n",
      "    3.46420072e-02  3.84597406e-02]\n",
      "  ...\n",
      "  [ 6.28266484e-03 -3.63897271e-02 -1.06587242e-02 ...  0.00000000e+00\n",
      "    0.00000000e+00 -3.68188657e-02]\n",
      "  [ 1.08017102e-02  7.17482669e-03  6.91857468e-03 ...  0.00000000e+00\n",
      "    3.17519307e-02  3.50573435e-02]\n",
      "  [ 5.72893173e-02  7.44897723e-02  8.61291960e-02 ...  2.08793610e-01\n",
      "    2.09784061e-01  2.15511248e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.26331691e-02  1.78379640e-02  2.04270128e-02 ...  4.37298715e-02\n",
      "   -9.99690965e-04  4.54504341e-02]\n",
      "  [ 5.69897592e-02  4.17996161e-02  4.32126895e-02 ... -1.88155249e-02\n",
      "   -8.34789649e-02 -2.17137691e-02]\n",
      "  [-1.26158968e-02 -8.80938955e-03 -8.59008916e-03 ...  0.00000000e+00\n",
      "    1.77560654e-03  3.72182950e-03]\n",
      "  ...\n",
      "  [-1.42618828e-03 -3.30789946e-04 -1.75925083e-02 ...  0.00000000e+00\n",
      "    0.00000000e+00 -7.35391080e-02]\n",
      "  [-5.58848679e-03 -1.37618277e-02 -2.95419283e-02 ...  0.00000000e+00\n",
      "   -6.04997166e-02 -6.44440800e-02]\n",
      "  [-5.37021682e-02 -7.91967809e-02 -9.99958143e-02 ... -2.74311453e-01\n",
      "   -2.75618851e-01 -2.81400740e-01]]\n",
      "\n",
      " [[ 5.65264635e-02  3.22136991e-02  2.02477593e-02 ...  3.42086144e-02\n",
      "   -3.13765183e-03  3.62364389e-02]\n",
      "  [-6.78627044e-02 -7.62811676e-02 -4.33229953e-02 ... -9.31790099e-03\n",
      "    1.68448865e-01  1.62028298e-02]\n",
      "  [-1.00420691e-01 -9.26970467e-02 -9.04802978e-02 ...  0.00000000e+00\n",
      "   -1.32122472e-01 -1.35246918e-01]\n",
      "  ...\n",
      "  [ 7.57270493e-03  6.20372593e-04 -4.37882356e-03 ...  0.00000000e+00\n",
      "    0.00000000e+00 -2.38532573e-02]\n",
      "  [-4.50143367e-02 -4.28689346e-02 -3.84253412e-02 ...  0.00000000e+00\n",
      "   -8.82210955e-02 -8.76885355e-02]\n",
      "  [-5.31179085e-02 -5.09423986e-02 -4.82847169e-02 ... -7.50400722e-02\n",
      "   -7.59397522e-02 -7.80617222e-02]]\n",
      "\n",
      " [[-1.83314586e-03  5.25297923e-03  3.23835853e-03 ... -8.05609021e-03\n",
      "   -1.34708849e-03 -5.54635469e-03]\n",
      "  [-7.25753903e-02  3.67742777e-02 -8.08980688e-02 ...  1.31125957e-01\n",
      "   -7.01695681e-02  4.05765325e-02]\n",
      "  [-3.41248661e-02 -2.55931094e-02 -2.55996082e-02 ...  0.00000000e+00\n",
      "   -2.55098511e-02 -2.50004418e-02]\n",
      "  ...\n",
      "  [-1.16802799e-03 -4.56304802e-03 -4.00085142e-03 ...  0.00000000e+00\n",
      "    0.00000000e+00 -2.44193133e-02]\n",
      "  [-3.36881205e-02 -3.81860808e-02 -5.02996743e-02 ...  0.00000000e+00\n",
      "   -4.81642485e-02 -4.78535220e-02]\n",
      "  [ 2.28084512e-02  5.27998917e-02  7.00470880e-02 ...  2.01390266e-01\n",
      "    2.02209473e-01  2.06004202e-01]]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.load(dir_test+'/'+linearin_path_test)\n",
    "w_test = np.load(dir_train+'/'+weightpath)\n",
    "bias = np.load(dir_train+'/'+biaspath)\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5545293  -0.00719963  0.55830157 ...  0.29180697  0.05797084\n",
      "   0.558193  ]\n",
      " [ 0.52302873  0.01604709  0.53401285 ...  0.316234    0.05467195\n",
      "   0.5186102 ]\n",
      " [ 0.5737991  -0.01289414  0.5650175  ...  0.30335885  0.05428706\n",
      "   0.57328147]\n",
      " ...\n",
      " [ 0.45841107  0.0850561   0.23880523 ...  0.21537417  0.21531764\n",
      "   0.2863179 ]\n",
      " [ 0.36349076 -0.18696131  0.185161   ...  0.27989405  0.06499407\n",
      "   0.31983444]\n",
      " [ 0.33815238  0.07204408  0.29068702 ...  0.14704172  0.06083944\n",
      "   0.26588497]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL:\n",
    "    np.save(os.path.join(dir_result, '%s%s_ql%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, quantize_lut, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pqhdr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
