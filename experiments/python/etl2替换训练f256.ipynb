{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../../../csi_transformer/src/')\n",
    "# import backbone\n",
    "# import image_segmentation\n",
    "import numpy as np\n",
    "import os\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "nbits = 8 # 量化比特数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "ncodebooks = 32 # max:512\n",
    "ncentroids = 256\n",
    "\n",
    "train_sam_num = 150 # 训练集样本数\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = socket.gethostname()\n",
    "if host_name == 'DESKTOP-PLRL7TK':\n",
    "    dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "    dir_result = ''\n",
    "elif host_name == 'DESKTOP-6FOH47P':\n",
    "    dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "    dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "    linearin_path_train= ''\n",
    "    linearout_path_train= ''\n",
    "    linearin_path_test = ''\n",
    "    linearout_path_test = ''\n",
    "elif host_name == 'jm-System-Product-Name':\n",
    "    dir_joined = '/data/hdr/transformer_data/joined'\n",
    "    dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "    dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "    dir_result = '/data/hdr/pq/res'\n",
    "    linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "    linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "else:\n",
    "    raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "try:\n",
    "    os.mkdir(dir_result)\n",
    "except FileNotFoundError:\n",
    "    os.makedirs(dir_result)\n",
    "except FileExistsError:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(dir_joined, linear_name_full, feedback_bits, [train_sam_num, test_sam_num], batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18753.59989278329\n",
      "learn_multisplits(): returning loss:  381.7248059574049\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28489.33439981348\n",
      "learn_multisplits(): returning loss:  974.4537388905883\n",
      "================================\n",
      "learn_multisplits(): initial loss:    298.7503699480684\n",
      "learn_multisplits(): returning loss:  72.5439866623256\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10141.544971659741\n",
      "learn_multisplits(): returning loss:  262.61814920208417\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23556.00770876478\n",
      "learn_multisplits(): returning loss:  830.3013631533831\n",
      "================================\n",
      "learn_multisplits(): initial loss:    72355.41897514262\n",
      "learn_multisplits(): returning loss:  1526.5472004413605\n",
      "================================\n",
      "learn_multisplits(): initial loss:    655.6902937822678\n",
      "learn_multisplits(): returning loss:  146.9239067180315\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18411.96517800063\n",
      "learn_multisplits(): returning loss:  196.0421977280639\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1768.8861389349793\n",
      "learn_multisplits(): returning loss:  299.8191674842965\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4549.621083335587\n",
      "learn_multisplits(): returning loss:  556.4154875962413\n",
      "================================\n",
      "learn_multisplits(): initial loss:    20985.55254224377\n",
      "learn_multisplits(): returning loss:  343.25525660347193\n",
      "================================\n",
      "learn_multisplits(): initial loss:    520.3890252304996\n",
      "learn_multisplits(): returning loss:  149.32036044052836\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18984.853675958722\n",
      "learn_multisplits(): returning loss:  720.5932666379958\n",
      "================================\n",
      "learn_multisplits(): initial loss:    15757.074249591595\n",
      "learn_multisplits(): returning loss:  368.88357654120773\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1833.523694550659\n",
      "learn_multisplits(): returning loss:  259.2428199718124\n",
      "================================\n",
      "learn_multisplits(): initial loss:    27021.89817902914\n",
      "learn_multisplits(): returning loss:  576.1834386643022\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19323.141138407227\n",
      "learn_multisplits(): returning loss:  1145.3727514110506\n",
      "================================\n",
      "learn_multisplits(): initial loss:    712.9601022394579\n",
      "learn_multisplits(): returning loss:  115.0986903686266\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1087.7553154776951\n",
      "learn_multisplits(): returning loss:  120.31632784288377\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1458.8189586601245\n",
      "learn_multisplits(): returning loss:  132.53342267044354\n",
      "================================\n",
      "learn_multisplits(): initial loss:    636.0669080423304\n",
      "learn_multisplits(): returning loss:  97.16235385216908\n",
      "================================\n",
      "learn_multisplits(): initial loss:    408.1044236710735\n",
      "learn_multisplits(): returning loss:  59.96968165612195\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3746.0073823698353\n",
      "learn_multisplits(): returning loss:  486.1261450643651\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1283.4867203835213\n",
      "learn_multisplits(): returning loss:  248.59805527470417\n",
      "================================\n",
      "learn_multisplits(): initial loss:    779.1196155819639\n",
      "learn_multisplits(): returning loss:  131.07530699137723\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18006.545971116975\n",
      "learn_multisplits(): returning loss:  189.97291436803062\n",
      "================================\n",
      "learn_multisplits(): initial loss:    745.9847455520563\n",
      "learn_multisplits(): returning loss:  197.2388573043447\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10408.700404420475\n",
      "learn_multisplits(): returning loss:  398.9147061323747\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11843.575485823241\n",
      "learn_multisplits(): returning loss:  168.33682820247486\n",
      "================================\n",
      "learn_multisplits(): initial loss:    912.4842817338078\n",
      "learn_multisplits(): returning loss:  190.3325996217459\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19685.833126376878\n",
      "learn_multisplits(): returning loss:  103.51300269993953\n",
      "================================\n",
      "learn_multisplits(): initial loss:    28394.963501397815\n",
      "learn_multisplits(): returning loss:  594.8836430460215\n",
      "X_res mse / X mse:  0.0028320355\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(153600, 32) Y:(153600, 512)\n",
      "fitted dense lstsq with W:(8192, 512)\n",
      "X_res mse / X mse after lstsq:  0.04266296\n"
     ]
    }
   ],
   "source": [
    "est3 = mm.estFactory(X_path=linearin_path_train, W_path=weightpath, Y_path=y_train, dir= dir_train, ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method], nbits=nbits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(dir_test+'/'+linearin_path_test)\n",
    "w_test = np.load(dir_train+'/'+weightpath)\n",
    "bias = np.load(dir_train+'/'+biaspath)\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47736773  0.19505306  0.44643864 ...  0.20700936  0.02614694\n",
      "   0.40522146]\n",
      " [-0.02361076 -0.30592543  0.44643864 ...  0.20700936 -0.22434232\n",
      "   0.1547322 ]\n",
      " [-0.02361076 -0.30592543  0.44643864 ... -0.29396912 -0.47483155\n",
      "   0.40522146]\n",
      " ...\n",
      " [ 0.72785693 -0.05543617  0.19594939 ...  0.20700936 -0.22434232\n",
      "   0.40522146]\n",
      " [ 0.72785693 -0.05543617  0.19594939 ...  0.20700936  0.02614694\n",
      "   0.40522146]\n",
      " [ 0.47736773 -0.05543617  0.44643864 ...  0.20700936  0.02614694\n",
      "   0.40522146]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_EXACT:\n",
    "    train_sam_num = 0 # 训练集样本数\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), y_out_last_re.astype(np.float32))\n",
    "else:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pqhdr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
