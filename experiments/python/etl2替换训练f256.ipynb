{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../../../csi_transformer/src/')\n",
    "# import backbone\n",
    "# import image_segmentation\n",
    "import numpy as np\n",
    "import os\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "nbits = 8 # 量化比特数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "ncodebooks = 32 # max:512\n",
    "ncentroids = 64\n",
    "\n",
    "train_sam_num = 100 # 训练集样本数\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = socket.gethostname()\n",
    "if host_name == 'DESKTOP-PLRL7TK':\n",
    "    dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "    dir_result = ''\n",
    "elif host_name == 'DESKTOP-6FOH47P':\n",
    "    dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "    dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "    linearin_path_train= ''\n",
    "    linearout_path_train= ''\n",
    "    linearin_path_test = ''\n",
    "    linearout_path_test = ''\n",
    "elif host_name == 'jm-System-Product-Name':\n",
    "    dir_joined = '/data/hdr/transformer_data/joined'\n",
    "    dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "    dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "    dir_result = '/data/hdr/pq/res'\n",
    "    linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "    linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "else:\n",
    "    raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "try:\n",
    "    os.mkdir(dir_result)\n",
    "except FileNotFoundError:\n",
    "    os.makedirs(dir_result)\n",
    "except FileExistsError:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(dir_joined, linear_name_full, feedback_bits, [train_sam_num, test_sam_num], batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "got hparams: \n",
      "{'ncentroids': 64, 'ncodebooks': 32}\n",
      "-------- running task: DFT (1/2)\n",
      "X.shape:  (102400, 512)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12505.465327432083\n",
      "learn_multisplits(): returning loss:  373.2860610485077\n",
      "================================\n",
      "learn_multisplits(): initial loss:    19012.313122704927\n",
      "learn_multisplits(): returning loss:  866.577044069767\n",
      "================================\n",
      "learn_multisplits(): initial loss:    199.17889075891185\n",
      "learn_multisplits(): returning loss:  63.05750431368874\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6745.929690969252\n",
      "learn_multisplits(): returning loss:  235.43420307920314\n",
      "================================\n",
      "learn_multisplits(): initial loss:    15723.61435347548\n",
      "learn_multisplits(): returning loss:  834.787625849247\n",
      "================================\n",
      "learn_multisplits(): initial loss:    48253.888538789804\n",
      "learn_multisplits(): returning loss:  1636.5879745483398\n",
      "================================\n",
      "learn_multisplits(): initial loss:    441.49507694064215\n",
      "learn_multisplits(): returning loss:  136.48514203494415\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12283.616663196015\n",
      "learn_multisplits(): returning loss:  224.146010514698\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1176.725789588626\n",
      "learn_multisplits(): returning loss:  296.68853764235973\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3033.802091480117\n",
      "learn_multisplits(): returning loss:  512.6164027452469\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13987.905690125635\n",
      "learn_multisplits(): returning loss:  393.7804294228554\n",
      "================================\n",
      "learn_multisplits(): initial loss:    347.45627104795835\n",
      "learn_multisplits(): returning loss:  130.44090782517486\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12700.9419323211\n",
      "learn_multisplits(): returning loss:  693.5562837123871\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10515.782752671006\n",
      "learn_multisplits(): returning loss:  384.6606643348932\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1241.289248066666\n",
      "learn_multisplits(): returning loss:  235.1360104335472\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18014.588601915453\n",
      "learn_multisplits(): returning loss:  645.5959037542343\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12871.151658719278\n",
      "learn_multisplits(): returning loss:  1069.336901679635\n",
      "================================\n",
      "learn_multisplits(): initial loss:    473.489878320888\n",
      "learn_multisplits(): returning loss:  125.34145606625658\n",
      "================================\n",
      "learn_multisplits(): initial loss:    721.9778290229996\n",
      "learn_multisplits(): returning loss:  129.03883802786004\n",
      "================================\n",
      "learn_multisplits(): initial loss:    967.1389994575242\n",
      "learn_multisplits(): returning loss:  122.62702978495508\n",
      "================================\n",
      "learn_multisplits(): initial loss:    419.4146229162459\n",
      "learn_multisplits(): returning loss:  91.50594702414887\n",
      "================================\n",
      "learn_multisplits(): initial loss:    264.64730046859444\n",
      "learn_multisplits(): returning loss:  57.81793329323199\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2491.7121359018493\n",
      "learn_multisplits(): returning loss:  442.82929171505384\n",
      "================================\n",
      "learn_multisplits(): initial loss:    840.8514447297805\n",
      "learn_multisplits(): returning loss:  231.3382225099331\n",
      "================================\n",
      "learn_multisplits(): initial loss:    519.3590932533616\n",
      "learn_multisplits(): returning loss:  129.30992730148137\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11999.33478505275\n",
      "learn_multisplits(): returning loss:  211.22478337585926\n",
      "================================\n",
      "learn_multisplits(): initial loss:    513.3462106602333\n",
      "learn_multisplits(): returning loss:  182.76260749372818\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6937.147560017444\n",
      "learn_multisplits(): returning loss:  377.32004725933075\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7897.2922350028975\n",
      "learn_multisplits(): returning loss:  155.17062011919916\n",
      "================================\n",
      "learn_multisplits(): initial loss:    605.9652676907131\n",
      "learn_multisplits(): returning loss:  164.74576414190233\n",
      "================================\n",
      "learn_multisplits(): initial loss:    13097.682291375331\n",
      "learn_multisplits(): returning loss:  96.94690527813509\n",
      "================================\n",
      "learn_multisplits(): initial loss:    18904.459273502645\n",
      "learn_multisplits(): returning loss:  566.1808869838715\n",
      "X_res mse / X mse:  0.004168419\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(102400, 32) Y:(102400, 512)\n",
      "fitted dense lstsq with W:(2048, 512)\n",
      "X_res mse / X mse after lstsq:  0.08851691\n"
     ]
    }
   ],
   "source": [
    "est3 = mm.estFactory(X_path=linearin_path_train, W_path=weightpath, Y_path=y_train, dir= dir_train, ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method], nbits=nbits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(dir_test+'/'+linearin_path_test)\n",
    "w_test = np.load(dir_train+'/'+weightpath)\n",
    "bias = np.load(dir_train+'/'+biaspath)\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22512525  0.19329983  0.19419616 ... -0.0452331  -0.47658476\n",
      "   0.40346822]\n",
      " [-0.02536398 -0.0571894   0.4446854  ... -0.29572234 -0.72707397\n",
      "   0.15297897]\n",
      " [-0.02536398 -0.0571894   0.19419616 ... -0.29572234 -0.72707397\n",
      "   0.15297897]\n",
      " ...\n",
      " [ 0.4756145   0.19329983  0.19419616 ...  0.20525613  0.02439371\n",
      "   0.40346822]\n",
      " [ 0.22512525 -0.0571894   0.19419616 ...  0.45574537  0.02439371\n",
      "   0.40346822]\n",
      " [ 0.4756145  -0.0571894   0.19419616 ... -0.0452331   0.02439371\n",
      "   0.40346822]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_EXACT:\n",
    "    train_sam_num = 0 # 训练集样本数\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), y_out_last_re.astype(np.float32))\n",
    "else:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pqhdr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
