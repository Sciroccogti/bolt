{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear2层（etl2）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # 防止jupyter爆内存\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "from NNutils import *\n",
    "# import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = METHOD_MITHRAL\n",
    "# method = METHOD_PQ\n",
    "# method = METHOD_PLUTO\n",
    "# method = METHOD_MITHRALPQ\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE\n",
    "quantize_lut = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cb     ct  n_train_sam\n",
      "0    16.0   16.0         1000\n",
      "1    16.0   64.0          500\n",
      "2    16.0  128.0          500\n",
      "4    16.0  256.0          250\n",
      "5    32.0   16.0         1000\n",
      "6    32.0   64.0          575\n",
      "9    32.0  128.0          250\n",
      "10   32.0  256.0          150\n",
      "12   64.0   16.0         1000\n",
      "13   64.0   64.0          350\n",
      "16   64.0  128.0          200\n",
      "17   64.0  256.0           75\n",
      "20  128.0   64.0          125\n",
      "21  128.0  128.0           50\n",
      "22  128.0  256.0           20\n",
      "23  256.0   16.0          100\n",
      "24  256.0   64.0           75\n",
      "25  512.0   16.0           50\n",
      "26  512.0   64.0           20\n"
     ]
    }
   ],
   "source": [
    "linear_name = 'etl2'\n",
    "feedback_bits = 256\n",
    "linear_name_full = \"ex_linear2\"\n",
    "\n",
    "auto_train = False # 是否根据已运行的训练性能结果自动训练，（train_sam_num取已训练的最大值）\n",
    "\n",
    "nbits_trained = 8\n",
    "nbits_goal = 12\n",
    "if quantize_lut == False:\n",
    "    nbits_goal = 0\n",
    "nbits = nbits_goal # 要运行的量化比特数\n",
    "\n",
    "test_sam_num = 1000 # 测试集样本数(如需修改，请同时修改下面的读取文件，现文件默认1000个样本)\n",
    "\n",
    "if not auto_train:\n",
    "    ncodebooks = 128 # max:512\n",
    "    ncentroids = 16\n",
    "    train_sam_num = 1000 # 训练集样本数\n",
    "else:\n",
    "    cb_ct_ntr_combinations_unique = change_nbits_auto_run_list(linear_name, method, feedback_bits, nbits_trained, nbits_goal)\n",
    "    print(cb_ct_ntr_combinations_unique)\n",
    "    # 遍历每个cb、ct、n_train_sam组合\n",
    "    # for _, row_ref in cb_ct_ntr_combinations_unique.iterrows():\n",
    "    #     ncodebooks = int(row_ref['cb'])\n",
    "        # ncentroids = int(row_ref['ct'])\n",
    "        # train_sam_num = int(row_ref['n_train_sam'])\n",
    "\n",
    "batch_size = 32\n",
    "if method == METHOD_EXACT:\n",
    "    ncodebooks = 0\n",
    "    ncentroids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = socket.gethostname()\n",
    "if host_name == 'DESKTOP-PLRL7TK':\n",
    "    dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "    dir_result = ''\n",
    "elif host_name == 'DESKTOP-6FOH47P':\n",
    "    dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "    dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "    linearin_path_train= ''\n",
    "    linearout_path_train= ''\n",
    "    linearin_path_test = ''\n",
    "    linearout_path_test = ''\n",
    "elif host_name == 'jm-System-Product-Name':\n",
    "    dir_joined = '/data/hdr/transformer_data/joined'\n",
    "    dir_train = os.path.join(dir_joined, 'train', 'f'+str(feedback_bits))\n",
    "    dir_test = os.path.join(dir_joined, 'test', 'f'+str(feedback_bits))\n",
    "    dir_result = '/data/hdr/pq/res'\n",
    "    linearin_path_train= '%sin_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    y_train = '%s_y_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearout_path_train= '%sout_train_f%i_sam%i.npy' % (linear_name_full, feedback_bits, train_sam_num)\n",
    "    linearin_path_test = '%sin_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "    linearout_path_test = '%sout_test_f%i_sam%i.npy' % (linear_name_full, feedback_bits, test_sam_num)\n",
    "else:\n",
    "    raise NameError(\"You are running the script in a new computer %s, please define dirs\" % host_name)\n",
    "\n",
    "\n",
    "weightpath = '%s_w_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "biaspath = '%s_b_f%i.npy' % (linear_name_full, feedback_bits)\n",
    "dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, linear_name)\n",
    "try:\n",
    "    os.mkdir(dir_result)\n",
    "except FileNotFoundError:\n",
    "    os.makedirs(dir_result)\n",
    "except FileExistsError:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepare(dir_joined, linear_name_full, feedback_bits, [train_sam_num, test_sam_num], batch_size, S1 = S1_dict[linear_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Mithral\n",
      "X.shape:  (1024000, 512)\n",
      "_learn_mithral_initialization heuristic pq\n",
      "================================\n",
      "learn_multisplits(): initial loss:    385.33277815735505\n",
      "learn_multisplits(): returning loss:  66.31167496049021\n",
      "================================\n",
      "learn_multisplits(): initial loss:    119929.5268588298\n",
      "learn_multisplits(): returning loss:  2896.381732650906\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3520.040651905953\n",
      "learn_multisplits(): returning loss:  407.7406727028647\n",
      "================================\n",
      "learn_multisplits(): initial loss:    770.8066936855281\n",
      "learn_multisplits(): returning loss:  110.6724102839983\n",
      "================================\n",
      "learn_multisplits(): initial loss:    990.3958127583448\n",
      "learn_multisplits(): returning loss:  69.79453133563077\n",
      "================================\n",
      "learn_multisplits(): initial loss:    71781.28367504175\n",
      "learn_multisplits(): returning loss:  1450.0542891046648\n",
      "================================\n",
      "learn_multisplits(): initial loss:    110079.39097212609\n",
      "learn_multisplits(): returning loss:  1813.0831436144736\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6715.099294905793\n",
      "learn_multisplits(): returning loss:  1397.604245574878\n",
      "================================\n",
      "learn_multisplits(): initial loss:    685.6176050564538\n",
      "learn_multisplits(): returning loss:  52.736522457970295\n",
      "================================\n",
      "learn_multisplits(): initial loss:    616.282083936154\n",
      "learn_multisplits(): returning loss:  79.36556523216177\n",
      "================================\n",
      "learn_multisplits(): initial loss:    384.546376420757\n",
      "learn_multisplits(): returning loss:  19.087591002140858\n",
      "================================\n",
      "learn_multisplits(): initial loss:    297.4517978035058\n",
      "learn_multisplits(): returning loss:  31.70502768700263\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1960.6652055439838\n",
      "learn_multisplits(): returning loss:  212.95348691978444\n",
      "================================\n",
      "learn_multisplits(): initial loss:    457.2335317877186\n",
      "learn_multisplits(): returning loss:  67.01269356265612\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1515.3454152430468\n",
      "learn_multisplits(): returning loss:  149.98904034157258\n",
      "================================\n",
      "learn_multisplits(): initial loss:    63663.09234960651\n",
      "learn_multisplits(): returning loss:  771.4492642409066\n",
      "================================\n",
      "learn_multisplits(): initial loss:    4594.460091053194\n",
      "learn_multisplits(): returning loss:  490.2671716776897\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7085.256952096803\n",
      "learn_multisplits(): returning loss:  1258.8592907809707\n",
      "================================\n",
      "learn_multisplits(): initial loss:    139538.85928325474\n",
      "learn_multisplits(): returning loss:  1314.1920007295216\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5572.30690267844\n",
      "learn_multisplits(): returning loss:  135.06981717129852\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1320.1920288548868\n",
      "learn_multisplits(): returning loss:  130.77933303824966\n",
      "================================\n",
      "learn_multisplits(): initial loss:    108719.72309643215\n",
      "learn_multisplits(): returning loss:  962.1670855273484\n",
      "================================\n",
      "learn_multisplits(): initial loss:    133359.13966966054\n",
      "learn_multisplits(): returning loss:  1470.8320685607728\n",
      "================================\n",
      "learn_multisplits(): initial loss:    237551.48656629652\n",
      "learn_multisplits(): returning loss:  8828.782439596594\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2334.4637934995235\n",
      "learn_multisplits(): returning loss:  215.9280924993652\n",
      "================================\n",
      "learn_multisplits(): initial loss:    941.9844244562053\n",
      "learn_multisplits(): returning loss:  89.12182153101813\n",
      "================================\n",
      "learn_multisplits(): initial loss:    671.6318176799673\n",
      "learn_multisplits(): returning loss:  114.46396321522013\n",
      "================================\n",
      "learn_multisplits(): initial loss:    489.1732261874472\n",
      "learn_multisplits(): returning loss:  45.708651113660366\n",
      "================================\n",
      "learn_multisplits(): initial loss:    120353.89466736595\n",
      "learn_multisplits(): returning loss:  1741.309611592757\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1021.2976923634692\n",
      "learn_multisplits(): returning loss:  45.12099505600949\n",
      "================================\n",
      "learn_multisplits(): initial loss:    791.7575953133459\n",
      "learn_multisplits(): returning loss:  66.53472822519348\n",
      "================================\n",
      "learn_multisplits(): initial loss:    132.3789695984345\n",
      "learn_multisplits(): returning loss:  15.328415469251318\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1680.1037972238173\n",
      "learn_multisplits(): returning loss:  153.0542648215436\n",
      "================================\n",
      "learn_multisplits(): initial loss:    716.2508155091754\n",
      "learn_multisplits(): returning loss:  81.37220366362811\n",
      "================================\n",
      "learn_multisplits(): initial loss:    671.1987785345917\n",
      "learn_multisplits(): returning loss:  41.18896524437661\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8750.434214045124\n",
      "learn_multisplits(): returning loss:  1217.8131347555027\n",
      "================================\n",
      "learn_multisplits(): initial loss:    23016.03388823726\n",
      "learn_multisplits(): returning loss:  1521.0545927450423\n",
      "================================\n",
      "learn_multisplits(): initial loss:    759.8965729471495\n",
      "learn_multisplits(): returning loss:  23.72776032015726\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5067.311940080948\n",
      "learn_multisplits(): returning loss:  763.8430094714977\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1446.6220587040953\n",
      "learn_multisplits(): returning loss:  230.0505243746802\n",
      "================================\n",
      "learn_multisplits(): initial loss:    184.83339958510632\n",
      "learn_multisplits(): returning loss:  10.165360479352163\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1522.3241171319753\n",
      "learn_multisplits(): returning loss:  229.97358426458484\n",
      "================================\n",
      "learn_multisplits(): initial loss:    132900.09626977\n",
      "learn_multisplits(): returning loss:  561.548857017625\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5130.688866102471\n",
      "learn_multisplits(): returning loss:  515.618902760287\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1285.907361207091\n",
      "learn_multisplits(): returning loss:  234.50836448347914\n",
      "================================\n",
      "learn_multisplits(): initial loss:    499.72133333829083\n",
      "learn_multisplits(): returning loss:  58.798525593701214\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1562.1796343312487\n",
      "learn_multisplits(): returning loss:  240.23621079693163\n",
      "================================\n",
      "learn_multisplits(): initial loss:    126.5665765644799\n",
      "learn_multisplits(): returning loss:  15.322195935824244\n",
      "================================\n",
      "learn_multisplits(): initial loss:    97655.63602160609\n",
      "learn_multisplits(): returning loss:  435.2219465094221\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5242.170814761271\n",
      "learn_multisplits(): returning loss:  162.28701707206488\n",
      "================================\n",
      "learn_multisplits(): initial loss:    10886.870977439714\n",
      "learn_multisplits(): returning loss:  930.0198516338652\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12346.861234416812\n",
      "learn_multisplits(): returning loss:  113.6882896617781\n",
      "================================\n",
      "learn_multisplits(): initial loss:    96724.09218285193\n",
      "learn_multisplits(): returning loss:  505.32418299645485\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6338.0361658060765\n",
      "learn_multisplits(): returning loss:  988.0239833643165\n",
      "================================\n",
      "learn_multisplits(): initial loss:    817.4830320385026\n",
      "learn_multisplits(): returning loss:  42.361261590211754\n",
      "================================\n",
      "learn_multisplits(): initial loss:    635.1103190869187\n",
      "learn_multisplits(): returning loss:  80.37439326180424\n",
      "================================\n",
      "learn_multisplits(): initial loss:    8888.272364095037\n",
      "learn_multisplits(): returning loss:  472.09938433927834\n",
      "================================\n",
      "learn_multisplits(): initial loss:    339.7903427091143\n",
      "learn_multisplits(): returning loss:  7.1107587134956605\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2188.5880302940986\n",
      "learn_multisplits(): returning loss:  297.0669214335117\n",
      "================================\n",
      "learn_multisplits(): initial loss:    795.2900863514893\n",
      "learn_multisplits(): returning loss:  173.30833849647044\n",
      "================================\n",
      "learn_multisplits(): initial loss:    378.7460587560273\n",
      "learn_multisplits(): returning loss:  47.34635073909225\n",
      "================================\n",
      "learn_multisplits(): initial loss:    171247.6655505792\n",
      "learn_multisplits(): returning loss:  6875.851267631576\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6734.141794257731\n",
      "learn_multisplits(): returning loss:  134.4270657189529\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1161.6803108748272\n",
      "learn_multisplits(): returning loss:  177.8367546577098\n",
      "================================\n",
      "learn_multisplits(): initial loss:    7434.138082725\n",
      "learn_multisplits(): returning loss:  643.5475835853898\n",
      "================================\n",
      "learn_multisplits(): initial loss:    217.78820557769154\n",
      "learn_multisplits(): returning loss:  26.78637403070142\n",
      "================================\n",
      "learn_multisplits(): initial loss:    111184.89400791816\n",
      "learn_multisplits(): returning loss:  7103.43079342562\n",
      "================================\n",
      "learn_multisplits(): initial loss:    9542.635689188854\n",
      "learn_multisplits(): returning loss:  1264.001137604008\n",
      "================================\n",
      "learn_multisplits(): initial loss:    3251.9279064863795\n",
      "learn_multisplits(): returning loss:  598.7346156797418\n",
      "================================\n",
      "learn_multisplits(): initial loss:    430.6679705616103\n",
      "learn_multisplits(): returning loss:  20.181436399165793\n",
      "================================\n",
      "learn_multisplits(): initial loss:    283.9926325418449\n",
      "learn_multisplits(): returning loss:  15.354900542130103\n",
      "================================\n",
      "learn_multisplits(): initial loss:    716.5953182178647\n",
      "learn_multisplits(): returning loss:  41.84712961499475\n",
      "================================\n",
      "learn_multisplits(): initial loss:    432.51683327087414\n",
      "learn_multisplits(): returning loss:  24.016783068899038\n",
      "================================\n",
      "learn_multisplits(): initial loss:    5138.92391354992\n",
      "learn_multisplits(): returning loss:  225.29297719523447\n",
      "================================\n",
      "learn_multisplits(): initial loss:    327.17353396708825\n",
      "learn_multisplits(): returning loss:  24.284496144490017\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1333.5443857970333\n",
      "learn_multisplits(): returning loss:  168.17359536210296\n",
      "================================\n",
      "learn_multisplits(): initial loss:    6654.346859938593\n",
      "learn_multisplits(): returning loss:  374.13873025375017\n",
      "================================\n",
      "learn_multisplits(): initial loss:    409.9223570033692\n",
      "learn_multisplits(): returning loss:  78.2741131882336\n",
      "================================\n",
      "learn_multisplits(): initial loss:    347.82538359399587\n",
      "learn_multisplits(): returning loss:  32.58123488284237\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2055.932698712316\n",
      "learn_multisplits(): returning loss:  22.732878400017146\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1109.5443258224955\n",
      "learn_multisplits(): returning loss:  53.98240116130986\n",
      "================================\n",
      "learn_multisplits(): initial loss:    719.1178509848678\n",
      "learn_multisplits(): returning loss:  74.66697292724267\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1944.69269065661\n",
      "learn_multisplits(): returning loss:  84.41959589333838\n",
      "================================\n",
      "learn_multisplits(): initial loss:    398.2763949510935\n",
      "learn_multisplits(): returning loss:  24.036334145719607\n",
      "================================\n",
      "learn_multisplits(): initial loss:    504.7386850594212\n",
      "learn_multisplits(): returning loss:  49.034739068661835\n",
      "================================\n",
      "learn_multisplits(): initial loss:    229.92983513219298\n",
      "learn_multisplits(): returning loss:  19.10545591623744\n",
      "================================\n",
      "learn_multisplits(): initial loss:    759.1719548400041\n",
      "learn_multisplits(): returning loss:  35.047341795835095\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1195.7437553542354\n",
      "learn_multisplits(): returning loss:  71.91913830903488\n",
      "================================\n",
      "learn_multisplits(): initial loss:    11145.701186556124\n",
      "learn_multisplits(): returning loss:  1875.1086926505686\n",
      "================================\n",
      "learn_multisplits(): initial loss:    854.6500493085191\n",
      "learn_multisplits(): returning loss:  100.14253793682622\n",
      "================================\n",
      "learn_multisplits(): initial loss:    12806.531604099257\n",
      "learn_multisplits(): returning loss:  822.4548179555652\n",
      "================================\n",
      "learn_multisplits(): initial loss:    266.13657956916967\n",
      "learn_multisplits(): returning loss:  24.29267870978243\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2578.8974624810635\n",
      "learn_multisplits(): returning loss:  169.81215196552597\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2891.44120207022\n",
      "learn_multisplits(): returning loss:  112.28631256814552\n",
      "================================\n",
      "learn_multisplits(): initial loss:    498.9088316199891\n",
      "learn_multisplits(): returning loss:  60.902920441046575\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2512.9403600762794\n",
      "learn_multisplits(): returning loss:  388.54630190410427\n",
      "================================\n",
      "learn_multisplits(): initial loss:    24.202827188039038\n",
      "learn_multisplits(): returning loss:  3.486786648762752\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2034.8203730138916\n",
      "learn_multisplits(): returning loss:  178.74256336740046\n",
      "================================\n",
      "learn_multisplits(): initial loss:    662.6930036810235\n",
      "learn_multisplits(): returning loss:  73.85548933299627\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2349.2817336165963\n",
      "learn_multisplits(): returning loss:  186.07214312338468\n",
      "================================\n",
      "learn_multisplits(): initial loss:    715.9579825018379\n",
      "learn_multisplits(): returning loss:  34.779697815969946\n",
      "================================\n",
      "learn_multisplits(): initial loss:    546.565856926388\n",
      "learn_multisplits(): returning loss:  53.63059061892511\n",
      "================================\n",
      "learn_multisplits(): initial loss:    117573.29701644824\n",
      "learn_multisplits(): returning loss:  1294.8243825338025\n",
      "================================\n",
      "learn_multisplits(): initial loss:    763.9422291548367\n",
      "learn_multisplits(): returning loss:  36.849202580723095\n",
      "================================\n",
      "learn_multisplits(): initial loss:    580.2562214288708\n",
      "learn_multisplits(): returning loss:  41.436060175792875\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1680.963135614221\n",
      "learn_multisplits(): returning loss:  221.45738219264032\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1089.3748689776528\n",
      "learn_multisplits(): returning loss:  173.7085516720486\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1759.9350311556195\n",
      "learn_multisplits(): returning loss:  92.791167579367\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1156.7689252091184\n",
      "learn_multisplits(): returning loss:  107.02703089615193\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1193.5436564797665\n",
      "learn_multisplits(): returning loss:  140.9519307720955\n",
      "================================\n",
      "learn_multisplits(): initial loss:    364.13056601820904\n",
      "learn_multisplits(): returning loss:  45.19182871444103\n",
      "================================\n",
      "learn_multisplits(): initial loss:    66700.8964463544\n",
      "learn_multisplits(): returning loss:  4398.643105125934\n",
      "================================\n",
      "learn_multisplits(): initial loss:    174.11842735749642\n",
      "learn_multisplits(): returning loss:  14.682199473334244\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1379.28915407562\n",
      "learn_multisplits(): returning loss:  146.70390124425654\n",
      "================================\n",
      "learn_multisplits(): initial loss:    76989.33750568186\n",
      "learn_multisplits(): returning loss:  2031.5923871303103\n",
      "================================\n",
      "learn_multisplits(): initial loss:    266.29398138766373\n",
      "learn_multisplits(): returning loss:  43.69622479099295\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1656.8987056093429\n",
      "learn_multisplits(): returning loss:  190.6855857135153\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2959.401962379652\n",
      "learn_multisplits(): returning loss:  365.5773736276534\n",
      "================================\n",
      "learn_multisplits(): initial loss:    296.57046388993876\n",
      "learn_multisplits(): returning loss:  48.58583607192618\n",
      "================================\n",
      "learn_multisplits(): initial loss:    1291.2965121748855\n",
      "learn_multisplits(): returning loss:  109.8617732997337\n",
      "================================\n",
      "learn_multisplits(): initial loss:    130254.32046587134\n",
      "learn_multisplits(): returning loss:  1206.7703113882883\n",
      "================================\n",
      "learn_multisplits(): initial loss:    696.0118436130388\n",
      "learn_multisplits(): returning loss:  102.18140345791387\n",
      "================================\n",
      "learn_multisplits(): initial loss:    58.99641419684111\n",
      "learn_multisplits(): returning loss:  9.101484088799179\n",
      "================================\n",
      "learn_multisplits(): initial loss:    98.72133676480749\n",
      "learn_multisplits(): returning loss:  18.007105241614884\n",
      "================================\n",
      "learn_multisplits(): initial loss:    2680.7348790060337\n",
      "learn_multisplits(): returning loss:  540.6550547636399\n",
      "================================\n",
      "learn_multisplits(): initial loss:    94176.64352173104\n",
      "learn_multisplits(): returning loss:  1905.884424225188\n",
      "================================\n",
      "learn_multisplits(): initial loss:    193.50167893096506\n",
      "learn_multisplits(): returning loss:  14.541829833526258\n",
      "================================\n",
      "learn_multisplits(): initial loss:    91392.31508721261\n",
      "learn_multisplits(): returning loss:  924.1870184778796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hdr/pq/bolt/experiments/python/clusterize.py:1917: UserWarning: Persisting input arguments took 2.13s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  _learn_mithral_initialization(X, ncodebooks, ncentroids=ncentroids, pq_perm_algo='start', **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_res mse / X mse:  0.00263972\n",
      "fitting dense lstsq to X_res\n",
      "  with X_enc:(1024000, 128) Y:(1024000, 512)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if method == METHOD_PLUTO:\n",
    "    est3 = mm.estFactory(X_path=linearin_path_train, W_path=weightpath, Y_path=y_train, dir= dir_train,\\\n",
    "                        ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method], nbits=nbits, \\\n",
    "                        quantize_lut = quantize_lut, bias_path=biaspath)\n",
    "else:\n",
    "    est3 = mm.estFactory(X_path=linearin_path_train, W_path=weightpath, Y_path=y_train, dir= dir_train,\\\n",
    "                        ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method], nbits=nbits, \\\n",
    "                        quantize_lut = quantize_lut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(dir_test+'/'+linearin_path_test)\n",
    "w_test = np.load(dir_train+'/'+weightpath)\n",
    "bias = np.load(dir_train+'/'+biaspath)\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "if method == METHOD_PLUTO:\n",
    "    y_out_last = y_out_matmul\n",
    "else:\n",
    "    y_out_last = y_out_matmul + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17550986 -0.61532587  0.20481968 ... -0.0973627  -0.3168827\n",
      "   0.22384141]\n",
      " [ 0.15141429 -0.57918257  0.14458077 ... -0.08531491 -0.32893047\n",
      "   0.18769805]\n",
      " [ 0.22370099 -0.61532587  0.20481968 ... -0.0973627  -0.32893047\n",
      "   0.19974585]\n",
      " ...\n",
      " [ 0.03093646 -0.24184462 -0.12047046 ... -0.10941048 -0.17230928\n",
      "  -0.07735316]\n",
      " [-0.05339802 -0.49484807 -0.09637489 ... -0.0973627  -0.3891694\n",
      "   0.07926801]\n",
      " [-0.05339802 -0.32617912 -0.19275716 ... -0.25398389 -0.244596\n",
      "  -0.16168764]]\n",
      "y_out_last.shape:  (1024000, 64)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1]) #AMM字典模式需要复原y大小\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "if method == METHOD_SCALAR_QUANTIZE:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_nbits%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, nbits)), y_out_last_re.astype(np.float32))\n",
    "elif method == METHOD_MITHRAL or method == METHOD_PQ or method == METHOD_PLUTO or method == METHOD_MITHRALPQ:\n",
    "    np.save(os.path.join(dir_result, '%s%s_ql%i_nbits%i_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, quantize_lut, nbits, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)\n",
    "else:\n",
    "    np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pqhdr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
