{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder transformer层的linear1层（etl1）替换为近似矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matmul as mm\n",
    "import math_util as mu\n",
    "import scipy.io as io\n",
    "from amm_methods import *\n",
    "import socket # Obtain the current host name, which can be used to select different data directories and result saving directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = '/data/hdr/csi_transformer/pic/'\n",
    "# print(root_path)\n",
    "# root_path2 = '/'.join(root_path.split(\"/\")[:-2])\n",
    "# # sys.path.append(root_path)\n",
    "# print(root_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = METHOD_MITHRAL\n",
    "method = METHOD_PLUTO\n",
    "# method = METHOD_EXACT\n",
    "# method = METHOD_SCALAR_QUANTIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_name = 'etl1'\n",
    "feedback_bits = 256\n",
    "ncodebooks = 64 # max:64\n",
    "ncentroids = 256\n",
    "if method == METHOD_MITHRAL or METHOD_PLUTO:\n",
    "    ncentroids = 16\n",
    "train_sam_num = 1000 # 训练集样本数\n",
    "test_sam_num = 1000 # 测试集样本数\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = socket.gethostname()\n",
    "if host_name == 'DESKTOP-PLRL7TK':\n",
    "    dir_train = 'E:\\\\hdr\\\\研一\\\\华为-深度学习\\\\intermediate\\\\intermediate8dbfc1'\n",
    "    dir_result = ''\n",
    "elif host_name == 'DESKTOP-6FOH47P':\n",
    "    dir_train = 'F:\\\\Projects\\\\python\\\\PQ\\\\intermediate8dbfc1'\n",
    "    dir_result = 'F:\\\\Projects\\\\python\\\\PQ\\\\res'\n",
    "    data_to_fcpath_train= ''\n",
    "    featurepath_train= ''\n",
    "    data_to_fcpath_test = ''\n",
    "    featurepath_test = ''\n",
    "elif host_name == 'jm-System-Product-Name':\n",
    "    dir_train = os.path.join('/data/hdr/transformer_data/joined', 'train', 'f'+str(feedback_bits))\n",
    "    dir_test = os.path.join('/data/hdr/transformer_data/joined', 'test', 'f'+str(feedback_bits))\n",
    "    dir_result = '/data/hdr/pq/res'\n",
    "    data_to_fcpath_train= 'ex_linear1in_train_f%i_sam%i.npy' % (feedback_bits, train_sam_num)\n",
    "    y_train = 'ex_linear2in_train_f%i_sam%i.npy' % (feedback_bits, train_sam_num)\n",
    "    # y_train = 'ex_linear1_y_train_f%i_sam%i.npy' % (feedback_bits, train_sam_num)\n",
    "    featurepath_train= 'ex_linear1out_train_f%i_sam%i.npy' % (feedback_bits, train_sam_num)\n",
    "    data_to_fcpath_test = 'ex_linear1in_test.npy'\n",
    "    featurepath_test = 'ex_linear1out_test.npy'\n",
    "    # y_test = 'y_test.npy'\n",
    "else:\n",
    "    raise NameError(\"You are running the script in a new computer, please define dir_train\")\n",
    "\n",
    "\n",
    "weightpath = 'ex_linear1_w_f256.npy'\n",
    "biaspath = 'ex_linear1_b_f256.npy'\n",
    "dir_result = os.path.join(dir_result, method, \"f%i\" % feedback_bits, \"etl1\")\n",
    "try:\n",
    "    os.mkdir(dir_result)\n",
    "except FileNotFoundError:\n",
    "    os.makedirs(dir_result)\n",
    "except FileExistsError:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "running method:  Pluto\n",
      "(1024000, 512)\n",
      "(512,)\n",
      "<class 'numpy.ndarray'>\n",
      "learn_pluto activation: <function relu at 0x7f6c17064430>\n",
      "learn_pluto with N:1024000 D:64 M:512 DQ:64 obj:mse\n",
      "X_res mse / X mse:  0.005530617\n",
      "encoded_pluto bias: tensor([-0.0268, -0.0405, -0.1348, -0.0301, -0.0754, -0.0878,  0.0925, -0.1785,\n",
      "        -0.0925, -0.0966, -0.1247,  0.0067, -0.0831, -0.0332, -0.4592, -0.0810,\n",
      "        -0.1856, -0.0539, -0.0615, -0.2495,  0.0565, -0.1614, -0.1802, -0.0693,\n",
      "        -0.1954, -0.0729, -0.0446,  0.1022, -0.2439, -0.1410, -0.1176, -0.0733,\n",
      "        -0.1257, -0.0485, -0.2143, -0.2450, -0.1673, -0.0336, -0.1063, -0.0794,\n",
      "        -0.0860, -0.0284, -0.1015, -0.3457, -0.2414, -0.1669, -0.0887, -0.0667,\n",
      "         0.0249, -0.1095, -0.1175, -0.0603, -0.1833, -0.0470, -0.0740, -0.1681,\n",
      "        -0.0251, -0.2860, -0.0103, -0.1042, -0.5400, -0.1151, -0.2281,  0.0769,\n",
      "        -0.0552, -0.0649, -0.0488, -0.0457, -0.1391, -0.0608,  0.0294, -0.1072,\n",
      "         0.0978, -0.1705, -0.0619, -0.0858, -0.0692, -0.0308, -0.1295, -0.0760,\n",
      "        -0.1239, -0.1547, -0.1561, -0.1121,  0.0840, -0.2410, -0.1673, -0.1217,\n",
      "        -0.1570, -0.1641,  0.0810, -0.1298,  0.0686,  0.1114, -0.1607, -0.0730,\n",
      "         0.0508, -0.2480, -0.2118, -0.1241, -0.0965, -0.1575, -0.0217, -0.2720,\n",
      "        -0.1877, -0.1154, -0.2516, -0.1630, -0.1931, -0.0573, -0.2403, -0.0572,\n",
      "        -0.2427, -0.1590, -0.0239,  0.0625, -0.1380, -0.1826, -0.2277, -0.1390,\n",
      "        -0.0386, -0.1638, -0.1766, -0.0495, -0.2969, -0.2328, -0.2345, -0.1922,\n",
      "        -0.2542, -0.0657, -0.2585, -0.2821, -0.1318, -0.1529, -0.0430, -0.1686,\n",
      "        -0.3272, -0.2423, -0.2879, -0.2291, -0.2394,  0.0288,  0.0374, -0.1081,\n",
      "         0.0184, -0.1967,  0.0046, -0.3736, -0.0917, -0.1234, -0.1269, -0.2753,\n",
      "         0.0181, -0.1725, -0.0477, -0.0589, -0.0523, -0.0944, -0.1066, -0.1766,\n",
      "        -0.0531, -0.1884, -0.0259, -0.0458, -0.1004, -0.2228, -0.0410, -0.1179,\n",
      "        -0.1872, -0.1705, -0.0720,  0.0826,  0.0120, -0.1131, -0.1248, -0.1412,\n",
      "        -0.0890, -0.1938, -0.2245, -0.0317, -0.0837, -0.2141, -0.1346, -0.0661,\n",
      "        -0.2195, -0.0625, -0.1688, -0.1002, -0.0531, -0.1287, -0.1136, -0.1123,\n",
      "        -0.2005,  0.0893, -0.0725, -0.2535, -0.1311,  0.0275, -0.1465, -0.2979,\n",
      "        -0.0597, -0.3264,  0.0299,  0.0356, -0.2041,  0.0380, -0.0875, -0.2661,\n",
      "        -0.1589, -0.0941,  0.0718, -0.3241, -0.1005, -0.0581,  0.0431, -0.0967,\n",
      "        -0.0954, -0.1677, -0.2284, -0.1091, -0.0438, -0.0832, -0.1310, -0.1182,\n",
      "        -0.0234, -0.0348, -0.0798, -0.2806, -0.1910, -0.0211, -0.1972, -0.1958,\n",
      "        -0.0849, -0.1528, -0.0224, -0.1056, -0.0764, -0.3412, -0.0243, -0.0582,\n",
      "        -0.1513, -0.0610, -0.0860, -0.0573, -0.2102,  0.0542, -0.1251,  0.0952,\n",
      "        -0.1271, -0.3151, -0.2410, -0.0286, -0.1053, -0.0458, -0.0275, -0.1562,\n",
      "        -0.2053,  0.0161, -0.1483, -0.1800, -0.2255, -0.0690, -0.1962, -0.0671,\n",
      "        -0.0363, -0.0622,  0.0382,  0.0586, -0.2904, -0.2979,  0.0374, -0.0408,\n",
      "        -0.0292, -0.1413, -0.0280, -0.1574, -0.1296,  0.0287, -0.0874, -0.2456,\n",
      "        -0.2128, -0.2020, -0.1300, -0.1506, -0.0563, -0.2511, -0.0204, -0.2223,\n",
      "        -0.0869, -0.1526, -0.1453, -0.1306,  0.0183, -0.0374, -0.2436, -0.2545,\n",
      "        -0.1585, -0.3039, -0.1213,  0.0294, -0.2065, -0.0747, -0.0489, -0.2943,\n",
      "        -0.2906, -0.2161, -0.0391, -0.1416, -0.1569, -0.0154, -0.0909, -0.0963,\n",
      "        -0.1042, -0.1641, -0.2623, -0.1261, -0.2457, -0.2169, -0.1950, -0.1236,\n",
      "        -0.1049, -0.1982, -0.1894, -0.0909, -0.3613, -0.2339, -0.2288, -0.0399,\n",
      "        -0.0728, -0.1455, -0.0607, -0.0492, -0.1610, -0.3232, -0.0407, -0.0361,\n",
      "        -0.0555, -0.2249, -0.3176, -0.1505, -0.1934, -0.2761, -0.0403, -0.1323,\n",
      "        -0.0522, -0.0526, -0.0987, -0.1579, -0.0328, -0.2985, -0.1377, -0.0352,\n",
      "         0.0139, -0.0675, -0.0116, -0.0461, -0.3476, -0.1442, -0.0660, -0.1232,\n",
      "        -0.0595,  0.0162, -0.0530, -0.1716, -0.1708, -0.0954, -0.1767, -0.1693,\n",
      "        -0.2969, -0.0594, -0.2487, -0.2757, -0.0636, -0.1870, -0.2821, -0.0512,\n",
      "        -0.2205, -0.0519, -0.0751, -0.1184, -0.1429, -0.0599, -0.2005, -0.3086,\n",
      "        -0.2264, -0.3295, -0.3222, -0.1572, -0.3734, -0.2040, -0.1653, -0.0868,\n",
      "        -0.2663, -0.0677, -0.2135,  0.0350, -0.1217, -0.0365, -0.0807,  0.0333,\n",
      "        -0.2086, -0.3608, -0.1116, -0.1372, -0.1791, -0.0356, -0.0685, -0.1330,\n",
      "        -0.2657, -0.1891,  0.0742, -0.0026, -0.0884, -0.1161, -0.3357, -0.0734,\n",
      "        -0.3003, -0.0669, -0.2278, -0.0531, -0.0553, -0.3130, -0.1755, -0.0846,\n",
      "        -0.0736, -0.1268, -0.0277, -0.0930, -0.1558, -0.1489, -0.2290, -0.2419,\n",
      "        -0.0361, -0.1544, -0.0674, -0.0194, -0.0249, -0.0448, -0.2996, -0.0309,\n",
      "        -0.1502, -0.0585, -0.1669, -0.1265, -0.0504,  0.0773, -0.0253, -0.2557,\n",
      "        -0.1360, -0.1651, -0.1467, -0.1026, -0.2060, -0.0508, -0.1071, -0.0894,\n",
      "        -0.2678, -0.1163,  0.0487, -0.0321, -0.1324, -0.0680, -0.3016, -0.1536,\n",
      "        -0.1289, -0.0804, -0.0543, -0.0864, -0.2254, -0.2961, -0.1061,  0.0470,\n",
      "        -0.4751, -0.0229, -0.1276, -0.1998, -0.1902, -0.0382, -0.0911, -0.1413,\n",
      "        -0.0625,  0.0880, -0.3326, -0.0686, -0.0399, -0.0733, -0.0822, -0.0500,\n",
      "        -0.3615, -0.0748, -0.3057, -0.1008, -0.1387, -0.2019, -0.2654, -0.2159,\n",
      "        -0.0831, -0.0452, -0.2138, -0.0933, -0.1877, -0.0373, -0.0887,  0.0605,\n",
      "        -0.0518, -0.0925, -0.2762, -0.1383, -0.0556, -0.1614,  0.0897, -0.0848])\n",
      "encoded_pluto activation: <function relu at 0x7f6c17064430>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m est3 \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39;49mestFactory(X_path\u001b[39m=\u001b[39;49mdata_to_fcpath_train, W_path\u001b[39m=\u001b[39;49mweightpath, Y_path\u001b[39m=\u001b[39;49mfeaturepath_train, bias_path\u001b[39m=\u001b[39;49mbiaspath, \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49m dir_train, ncodebooks\u001b[39m=\u001b[39;49mncodebooks, ncentroids\u001b[39m=\u001b[39;49mncentroids, methods\u001b[39m=\u001b[39;49m[method])\n",
      "File \u001b[0;32m/data/hdr/pq/bolt/experiments/python/matmul.py:319\u001b[0m, in \u001b[0;36mestFactory\u001b[0;34m(methods, ntasks, ncodebooks, ncentroids, verbose, limit_ntasks, tasks_all_same_shape, tasks, X_path, W_path, Y_path, bias_path, dir)\u001b[0m\n\u001b[1;32m    317\u001b[0m task\u001b[39m.\u001b[39mW_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(task\u001b[39m.\u001b[39mW_train)\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m bias_path\u001b[39m!=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     est \u001b[39m=\u001b[39m _fitted_est_for_hparams(\n\u001b[1;32m    320\u001b[0m         method_id, hparams_dict,\n\u001b[1;32m    321\u001b[0m         task\u001b[39m.\u001b[39;49mX_train, task\u001b[39m.\u001b[39;49mW_train, task\u001b[39m.\u001b[39;49mY_train, bias\u001b[39m=\u001b[39;49mtask\u001b[39m.\u001b[39;49mbias)\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     est \u001b[39m=\u001b[39m _fitted_est_for_hparams(\n\u001b[1;32m    324\u001b[0m         method_id, hparams_dict,\n\u001b[1;32m    325\u001b[0m         task\u001b[39m.\u001b[39mX_train, task\u001b[39m.\u001b[39mW_train, task\u001b[39m.\u001b[39mY_train)\n",
      "File \u001b[0;32m/data/hdr/pq/bolt/experiments/python/matmul.py:247\u001b[0m, in \u001b[0;36m_fitted_est_for_hparams\u001b[0;34m(method_id, hparams_dict, X_train, W_train, Y_train, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fitted_est_for_hparams\u001b[39m(method_id, hparams_dict, X_train, W_train,\n\u001b[1;32m    245\u001b[0m                             Y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m     est \u001b[39m=\u001b[39m _estimator_for_method_id(method_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhparams_dict)\n\u001b[0;32m--> 247\u001b[0m     est\u001b[39m.\u001b[39;49mfit(X_train, W_train, Y\u001b[39m=\u001b[39;49mY_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m est\n",
      "File \u001b[0;32m/data/hdr/pq/bolt/experiments/python/vq_amm.py:426\u001b[0m, in \u001b[0;36mPlutoMatmul.fit\u001b[0;34m(self, A, B, Y, output, bias)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[39mraise\u001b[39;00m amm\u001b[39m.\u001b[39mInvalidParametersException(\n\u001b[1;32m    422\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mD < C: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m < \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(D, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mncodebooks))\n\u001b[1;32m    424\u001b[0m \u001b[39m# self.enc.fit sets self.enc.splits_lists and self.enc.centroids\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m# self.enc.fit also calls clusterize.learn_pluto\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mluts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    427\u001b[0m     A, B\u001b[39m.\u001b[39;49mT, output\u001b[39m=\u001b[39;49moutput, bias\u001b[39m=\u001b[39;49mbias)\u001b[39m#, activation = self.activation)\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstddevB0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(B, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstddevB1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(B, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/data/hdr/pq/bolt/experiments/python/vquantizers.py:728\u001b[0m, in \u001b[0;36mPlutoEncoder.fit\u001b[0;34m(self, X, Q, output, bias)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, Q, output\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, bias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    727\u001b[0m     \u001b[39m# Q = B.T, where A is (N, D) and B is (D, M). So Q is (M, D)\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplits_lists, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentroids, luts \u001b[39m=\u001b[39m clusterize\u001b[39m.\u001b[39;49mlearn_pluto(\n\u001b[1;32m    729\u001b[0m         X, Q, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncodebooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation, output, bias, \n\u001b[1;32m    730\u001b[0m         nonzeros_heuristic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnonzeros_heuristic, objective\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjective,\n\u001b[1;32m    731\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    732\u001b[0m     \u001b[39m# self._learn_lut_quantization(X, Q)\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[39m    Q = np.atleast_2d(Q)\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39m    luts = np.zeros((Q.shape[0], self.ncodebooks, self.ncentroids))\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m    for i, q in enumerate(Q):\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39m        luts[i] = clusterize.mithral_lut(q, self.centroids) # reshapes\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/data/hdr/pq/bolt/experiments/python/clusterize.py:1813\u001b[0m, in \u001b[0;36mlearn_pluto\u001b[0;34m(X, Q, ncodebooks, activation, output, bias, **kwargs)\u001b[0m\n\u001b[1;32m   1807\u001b[0m X_enc \u001b[39m=\u001b[39m mithral_encode(X, all_splits)\n\u001b[1;32m   1809\u001b[0m \u001b[39m#rint(\"pluto fitting dense lstsq to X_res\")\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m \u001b[39m#rint(f\"  with X_enc:{X_enc.shape} Y:{X_res.shape}\")\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m \u001b[39m# W = encoded_lstsq(X_enc=X_enc, Y=X_res)\u001b[39;00m\n\u001b[0;32m-> 1813\u001b[0m T_badshape \u001b[39m=\u001b[39m encoded_pluto(\n\u001b[1;32m   1814\u001b[0m     X_orig\u001b[39m=\u001b[39;49mX_orig, all_centroids\u001b[39m=\u001b[39;49mall_centroids,\n\u001b[1;32m   1815\u001b[0m     X_enc\u001b[39m=\u001b[39;49mX_enc, B\u001b[39m=\u001b[39;49mQ\u001b[39m.\u001b[39;49mT, output\u001b[39m=\u001b[39;49moutput, bias\u001b[39m=\u001b[39;49mbias,\n\u001b[1;32m   1816\u001b[0m     activation\u001b[39m=\u001b[39;49mactivation, objective\u001b[39m=\u001b[39;49mobjective)\n\u001b[1;32m   1817\u001b[0m \u001b[39m# shape: (n_codebooks*16, M)\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m luts \u001b[39m=\u001b[39m T_badshape\u001b[39m.\u001b[39mT \u001b[39m# (M, n_codebooks*16)\u001b[39;00m\n",
      "File \u001b[0;32m/data/hdr/pq/bolt/experiments/python/clusterize.py:1055\u001b[0m, in \u001b[0;36mencoded_pluto\u001b[0;34m(X_orig, all_centroids, X_enc, X_bin, B, output, bias, activation, objective, K, lamda)\u001b[0m\n\u001b[1;32m   1053\u001b[0m P_delta_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m*\u001b[39m(P_0_np\u001b[39m.\u001b[39mshape))\n\u001b[1;32m   1054\u001b[0m T_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(T_0_np)\n\u001b[0;32m-> 1055\u001b[0m res \u001b[39m=\u001b[39m minimize(\n\u001b[1;32m   1056\u001b[0m     pluto_obj, T_init, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ml-bfgs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m   1057\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, disp\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   1058\u001b[0m torch_result \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m   1059\u001b[0m \u001b[39mreturn\u001b[39;00m torch_result\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torchmin/minimize.py:91\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, method, max_iter, tol, options, callback, disp, return_all)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_bfgs(fun, x0, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m     90\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_lbfgs(fun, x0, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m     92\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_cg(fun, x0, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torchmin/bfgs.py:381\u001b[0m, in \u001b[0;36m_minimize_lbfgs\u001b[0;34m(fun, x0, lr, history_size, max_iter, line_search, gtol, xtol, normp, callback, disp, return_all)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_minimize_lbfgs\u001b[39m(\n\u001b[1;32m    338\u001b[0m         fun, x0, lr\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, history_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m         line_search\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrong-wolfe\u001b[39m\u001b[39m'\u001b[39m, gtol\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m, xtol\u001b[39m=\u001b[39m\u001b[39m1e-9\u001b[39m,\n\u001b[1;32m    340\u001b[0m         normp\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m), callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, disp\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, return_all\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    341\u001b[0m     \u001b[39m\"\"\"Minimize a multivariate function with L-BFGS\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m        Result of the optimization routine.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_bfgs_core(\n\u001b[1;32m    382\u001b[0m         fun, x0, lr, low_mem\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, history_size\u001b[39m=\u001b[39;49mhistory_size,\n\u001b[1;32m    383\u001b[0m         max_iter\u001b[39m=\u001b[39;49mmax_iter, line_search\u001b[39m=\u001b[39;49mline_search, gtol\u001b[39m=\u001b[39;49mgtol, xtol\u001b[39m=\u001b[39;49mxtol,\n\u001b[1;32m    384\u001b[0m         normp\u001b[39m=\u001b[39;49mnormp, callback\u001b[39m=\u001b[39;49mcallback, disp\u001b[39m=\u001b[39;49mdisp, return_all\u001b[39m=\u001b[39;49mreturn_all)\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torchmin/bfgs.py:214\u001b[0m, in \u001b[0;36m_minimize_bfgs_core\u001b[0;34m(fun, x0, lr, low_mem, history_size, inv_hess, max_iter, line_search, gtol, xtol, normp, callback, disp, return_all)\u001b[0m\n\u001b[1;32m    210\u001b[0m     f_new, g_new, _, _ \u001b[39m=\u001b[39m closure(x_new)\n\u001b[1;32m    211\u001b[0m \u001b[39melif\u001b[39;00m line_search \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstrong-wolfe\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    212\u001b[0m     \u001b[39m#  Determine step size via strong-wolfe line search\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     f_new, g_new, t, ls_evals \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 214\u001b[0m         strong_wolfe(dir_evaluate, x, t, d, f, g, gtd)\n\u001b[1;32m    215\u001b[0m     x_new \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m d\u001b[39m.\u001b[39mmul(t)\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torchmin/line_search.py:183\u001b[0m, in \u001b[0;36mstrong_wolfe\u001b[0;34m(fun, x, t, d, f, g, gtd, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     f, g, t, ls_nevals \u001b[39m=\u001b[39m _strong_wolfe_extra(\n\u001b[1;32m    177\u001b[0m         fun, x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), t, d\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), f, g\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), gtd, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# in theory we shouldn't need to use pytorch's native _strong_wolfe\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39m# since the custom implementation above is equivalent with\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39m# extra_codition=None. But we will keep this in case they make any\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[39m# changes.\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     f, g, t, ls_nevals \u001b[39m=\u001b[39m _strong_wolfe(\n\u001b[1;32m    184\u001b[0m         fun, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), t, d\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), f, g\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), gtd, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    186\u001b[0m \u001b[39m# convert back to torch scalar\u001b[39;00m\n\u001b[1;32m    187\u001b[0m f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(f, dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torch/optim/lbfgs.py:49\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mclone(memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcontiguous_format)\n\u001b[1;32m     48\u001b[0m \u001b[39m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m f_new, g_new \u001b[39m=\u001b[39m obj_func(x, t, d)\n\u001b[1;32m     50\u001b[0m ls_func_evals \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     51\u001b[0m gtd_new \u001b[39m=\u001b[39m g_new\u001b[39m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torchmin/function.py:137\u001b[0m, in \u001b[0;36mScalarFunction.dir_evaluate\u001b[0;34m(self, x, t, d)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m    136\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfun(x)\n\u001b[0;32m--> 137\u001b[0m grad \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39;49mgrad(f, x)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m de_value(f\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(f), grad\u001b[39m=\u001b[39mgrad)\n",
      "File \u001b[0;32m~/.conda/envs/pqhdr/lib/python3.8/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "est3 = mm.estFactory(X_path=data_to_fcpath_train, W_path=weightpath, Y_path=featurepath_train, bias_path=biaspath, dir= dir_train, ncodebooks=ncodebooks, ncentroids=ncentroids, methods=[method])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(est3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(dir_test+'/'+data_to_fcpath_test)\n",
    "w_test = np.load(dir_train+'/'+weightpath)\n",
    "bias = np.load(dir_train+'/'+biaspath)\n",
    "# print(type(est3))\n",
    "y_out_matmul = mm.eval_matmul(est3, x_test, w_test) # MADDNESS乘法的结果\n",
    "# y_out_last = mu.softmax(y_out_matmul + bias.T) # MADDNESS替换后当前层输出，即+bias并激活函数后的结果\n",
    "y_out_last = y_out_matmul# + bias.T # MADDNESS替换后当前层输出，即+bias并不需要激活函数后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.30822068 -0.30822068 -0.30822068 ... -0.18297607  1.3199594\n",
      "  -0.30822068]\n",
      " [-0.4334653  -0.30822068 -0.30822068 ... -0.05773144  1.0694702\n",
      "  -0.30822068]\n",
      " [-0.30822068 -0.30822068 -0.18297607 ... -0.18297607  1.0694702\n",
      "  -0.30822068]\n",
      " ...\n",
      " [-0.30822068 -0.18297607 -0.05773144 ... -0.05773144  0.56849164\n",
      "  -0.18297607]\n",
      " [-0.30822068 -0.30822068 -0.18297607 ... -0.05773144  0.8189809\n",
      "  -0.30822068]\n",
      " [-0.18297607 -0.18297607  0.06751318 ...  0.06751318  0.69373626\n",
      "   0.06751318]]\n",
      "y_out_last.shape:  (1024000, 512)\n",
      "y_out_last_re.shape:  (1000, 32, 32, 512)\n"
     ]
    }
   ],
   "source": [
    "print(y_out_last)\n",
    "print(\"y_out_last.shape: \", y_out_last.shape)\n",
    "y_out_last_re = y_out_last.reshape(test_sam_num, batch_size, -1, y_out_last.shape[-1])\n",
    "print(\"y_out_last_re.shape: \", y_out_last_re.shape)\n",
    "# np.save(\"LDPC_decoder_NET_testdata/\" + snr + \"nomul_matmul_yout_matmul\", y_out_matmul)\n",
    "# np.save(dir_result+'/'+method+'fc1_fb256_cb%i_ct%i.npy' % (ncodebooks, ncentroids), y_out_matmul)\n",
    "np.save(os.path.join(dir_result, '%s%s_trsam%i_tesam%i_fb%i_cb%i_ct%i.npy' % (method, linear_name, train_sam_num, test_sam_num, feedback_bits, ncodebooks, ncentroids)), y_out_last_re)\n",
    "# io.savemat(dir_result+'\\\\fc1_256.mat', {\"NN_output_buffer\": y_out_last})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pqhdr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec04f7dc2b3ae4a422de9aaf96e8c62fe190a2869a08d14112cb2d7713497448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
